{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools \n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.dirname(os.path.realpath('')) + '/PA 2/data_prog2Spring18'\n",
    "train_data_path = data_path + \"/train_data\"\n",
    "test_data_path = data_path + \"/test_data\"\n",
    "train_labels_path = data_path + \"/labels/train_label.txt\"\n",
    "test_labels_path = data_path + \"/labels/test_label.txt\"\n",
    "\n",
    "num_train_data = len(os.listdir(train_data_path))\n",
    "train_data = []\n",
    "for filename in sorted(os.listdir(train_data_path)):\n",
    "    image = mpimg.imread(os.path.join(train_data_path, filename))\n",
    "    train_data.append(image.reshape(784).tolist())\n",
    "\n",
    "num_test_data = len(os.listdir(test_data_path))\n",
    "test_data = []\n",
    "for filename in sorted(os.listdir(test_data_path)):\n",
    "    image = mpimg.imread(os.path.join(test_data_path, filename))\n",
    "    test_data.append(image.reshape(784).tolist())\n",
    "\n",
    "X_train = [[y / 255. for y in x] for x in train_data]\n",
    "X_test = [[y / 255. for y in x] for x in test_data]\n",
    "\n",
    "Y_train = []\n",
    "with open(train_labels_path,'r') as f:\n",
    "    for line in f.readlines():\n",
    "        j = int(line[:-1]) - 1\n",
    "        tmp = [0.] * 10\n",
    "        tmp[j] = 1.\n",
    "        Y_train.append(tmp)\n",
    "    \n",
    "Y_test = []\n",
    "with open(test_labels_path,'r') as f:\n",
    "    for line in f.readlines():\n",
    "        j = int(line[:-1]) - 1\n",
    "        tmp = [0.] * 10\n",
    "        tmp[j] = 1.\n",
    "        Y_test.append(tmp)\n",
    "\n",
    "X_train = np.matrix(X_train) \t# 50000 * 784\n",
    "X_test = np.matrix(X_test)\t\t# 5000 * 784\n",
    "Y_train = np.matrix(Y_train) \t# 50000 * 10\n",
    "Y_test = np.matrix(Y_test)\t \t# 5000 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
    "\n",
    "def cross_entropy_loss(Y_hat, Y):\n",
    "    return -1 / Y.shape[1] * np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "\n",
    "def prediction(y):\n",
    "    return np.argmax(y, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter:   0 loss: 2.30264 training acc: 0.09718 testing acc: 0.0968\n",
      "epoch: 1 iter:   1 loss: 2.30263 training acc: 0.09718 testing acc: 0.0968\n",
      "epoch: 1 iter:   2 loss: 2.30254 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:   3 loss: 2.30266 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:   4 loss: 2.30265 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:   5 loss: 2.30268 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:   6 loss: 2.30264 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:   7 loss: 2.30255 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:   8 loss: 2.30253 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:   9 loss: 2.30250 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:  10 loss: 2.30243 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:  11 loss: 2.30233 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  12 loss: 2.30227 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:  13 loss: 2.30232 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  14 loss: 2.30234 training acc: 0.09684 testing acc: 0.103\n",
      "epoch: 1 iter:  15 loss: 2.30225 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  16 loss: 2.30217 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  17 loss: 2.30214 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  18 loss: 2.30222 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  19 loss: 2.30219 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  20 loss: 2.30213 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  21 loss: 2.30214 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  22 loss: 2.30199 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  23 loss: 2.30196 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  24 loss: 2.30200 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  25 loss: 2.30201 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  26 loss: 2.30200 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  27 loss: 2.30199 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  28 loss: 2.30195 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  29 loss: 2.30188 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  30 loss: 2.30181 training acc: 0.09976 testing acc: 0.0956\n",
      "epoch: 1 iter:  31 loss: 2.30177 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  32 loss: 2.30174 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  33 loss: 2.30174 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  34 loss: 2.30177 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  35 loss: 2.30182 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  36 loss: 2.30176 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  37 loss: 2.30175 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  38 loss: 2.30181 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  39 loss: 2.30180 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  40 loss: 2.30187 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  41 loss: 2.30180 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  42 loss: 2.30176 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  43 loss: 2.30186 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  44 loss: 2.30184 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  45 loss: 2.30187 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  46 loss: 2.30189 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  47 loss: 2.30188 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  48 loss: 2.30187 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  49 loss: 2.30182 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  50 loss: 2.30186 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  51 loss: 2.30184 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  52 loss: 2.30178 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  53 loss: 2.30182 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  54 loss: 2.30178 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  55 loss: 2.30183 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  56 loss: 2.30183 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  57 loss: 2.30190 training acc: 0.10202 testing acc: 0.1042\n",
      "epoch: 1 iter:  58 loss: 2.30179 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  59 loss: 2.30173 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  60 loss: 2.30169 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  61 loss: 2.30169 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  62 loss: 2.30169 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  63 loss: 2.30170 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  64 loss: 2.30172 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  65 loss: 2.30164 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  66 loss: 2.30160 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  67 loss: 2.30167 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  68 loss: 2.30172 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  69 loss: 2.30176 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  70 loss: 2.30172 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  71 loss: 2.30182 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  72 loss: 2.30180 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  73 loss: 2.30183 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  74 loss: 2.30182 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  75 loss: 2.30174 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  76 loss: 2.30180 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  77 loss: 2.30181 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  78 loss: 2.30187 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  79 loss: 2.30183 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  80 loss: 2.30181 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  81 loss: 2.30185 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  82 loss: 2.30181 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  83 loss: 2.30188 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  84 loss: 2.30188 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  85 loss: 2.30181 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  86 loss: 2.30181 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  87 loss: 2.30174 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  88 loss: 2.30168 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  89 loss: 2.30168 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  90 loss: 2.30160 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  91 loss: 2.30164 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  92 loss: 2.30160 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  93 loss: 2.30163 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  94 loss: 2.30159 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  95 loss: 2.30160 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  96 loss: 2.30166 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  97 loss: 2.30163 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  98 loss: 2.30162 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter:  99 loss: 2.30166 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 100 loss: 2.30164 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 101 loss: 2.30161 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 102 loss: 2.30165 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 103 loss: 2.30164 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 104 loss: 2.30159 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 105 loss: 2.30157 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 106 loss: 2.30151 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 107 loss: 2.30151 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 108 loss: 2.30151 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 109 loss: 2.30148 training acc: 0.11356 testing acc: 0.1068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 110 loss: 2.30156 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 111 loss: 2.30151 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 112 loss: 2.30148 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 113 loss: 2.30151 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 114 loss: 2.30148 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 115 loss: 2.30144 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 116 loss: 2.30141 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 117 loss: 2.30140 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 118 loss: 2.30133 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 119 loss: 2.30136 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 120 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 121 loss: 2.30123 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 122 loss: 2.30123 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 123 loss: 2.30134 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 124 loss: 2.30134 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 125 loss: 2.30131 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 126 loss: 2.30129 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 127 loss: 2.30129 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 128 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 129 loss: 2.30131 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 130 loss: 2.30127 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 131 loss: 2.30124 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 132 loss: 2.30123 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 133 loss: 2.30128 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 134 loss: 2.30125 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 135 loss: 2.30133 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 136 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 137 loss: 2.30133 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 138 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 139 loss: 2.30136 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 140 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 141 loss: 2.30137 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 142 loss: 2.30142 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 143 loss: 2.30139 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 144 loss: 2.30138 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 145 loss: 2.30136 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 146 loss: 2.30135 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 147 loss: 2.30139 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 148 loss: 2.30147 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 149 loss: 2.30147 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 150 loss: 2.30146 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 151 loss: 2.30147 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 152 loss: 2.30156 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 153 loss: 2.30154 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 154 loss: 2.30143 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 155 loss: 2.30140 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 156 loss: 2.30142 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 157 loss: 2.30146 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 158 loss: 2.30155 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 159 loss: 2.30161 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 160 loss: 2.30158 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 161 loss: 2.30150 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 162 loss: 2.30156 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 163 loss: 2.30160 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 164 loss: 2.30167 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 165 loss: 2.30175 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 166 loss: 2.30169 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 167 loss: 2.30168 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 168 loss: 2.30166 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 169 loss: 2.30166 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 170 loss: 2.30165 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 171 loss: 2.30164 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 172 loss: 2.30166 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 173 loss: 2.30159 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 174 loss: 2.30160 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 175 loss: 2.30163 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 176 loss: 2.30168 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 177 loss: 2.30175 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 178 loss: 2.30175 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 179 loss: 2.30174 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 180 loss: 2.30177 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 181 loss: 2.30176 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 182 loss: 2.30173 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 183 loss: 2.30177 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 184 loss: 2.30181 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 185 loss: 2.30174 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 186 loss: 2.30173 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 187 loss: 2.30172 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 188 loss: 2.30172 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 189 loss: 2.30163 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 190 loss: 2.30156 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 191 loss: 2.30151 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 192 loss: 2.30156 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 193 loss: 2.30154 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 194 loss: 2.30153 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 195 loss: 2.30147 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 196 loss: 2.30142 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 197 loss: 2.30146 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 198 loss: 2.30137 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 199 loss: 2.30142 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 200 loss: 2.30141 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 201 loss: 2.30142 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 202 loss: 2.30144 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 203 loss: 2.30136 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 204 loss: 2.30129 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 205 loss: 2.30129 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 206 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 207 loss: 2.30124 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 208 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 209 loss: 2.30121 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 210 loss: 2.30122 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 211 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 212 loss: 2.30120 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 213 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 214 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 215 loss: 2.30122 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 216 loss: 2.30122 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 217 loss: 2.30118 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 218 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 219 loss: 2.30118 training acc: 0.11356 testing acc: 0.1068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 220 loss: 2.30117 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 221 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 222 loss: 2.30117 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 223 loss: 2.30112 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 224 loss: 2.30112 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 225 loss: 2.30110 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 226 loss: 2.30116 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 227 loss: 2.30115 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 228 loss: 2.30112 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 229 loss: 2.30112 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 230 loss: 2.30110 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 231 loss: 2.30108 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 232 loss: 2.30106 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 233 loss: 2.30103 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 234 loss: 2.30106 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 235 loss: 2.30104 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 236 loss: 2.30103 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 237 loss: 2.30105 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 238 loss: 2.30107 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 239 loss: 2.30107 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 240 loss: 2.30109 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 241 loss: 2.30111 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 242 loss: 2.30115 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 243 loss: 2.30115 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 244 loss: 2.30116 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 245 loss: 2.30114 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 246 loss: 2.30113 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 247 loss: 2.30113 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 248 loss: 2.30114 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 249 loss: 2.30123 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 250 loss: 2.30128 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 251 loss: 2.30128 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 252 loss: 2.30131 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 253 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 254 loss: 2.30127 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 255 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 256 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 257 loss: 2.30133 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 258 loss: 2.30129 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 259 loss: 2.30120 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 260 loss: 2.30117 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 261 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 262 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 263 loss: 2.30120 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 264 loss: 2.30122 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 265 loss: 2.30123 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 266 loss: 2.30120 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 267 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 268 loss: 2.30118 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 269 loss: 2.30113 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 270 loss: 2.30111 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 271 loss: 2.30109 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 272 loss: 2.30109 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 273 loss: 2.30109 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 274 loss: 2.30105 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 275 loss: 2.30108 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 276 loss: 2.30109 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 277 loss: 2.30118 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 278 loss: 2.30121 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 279 loss: 2.30124 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 280 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 281 loss: 2.30124 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 282 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 283 loss: 2.30129 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 284 loss: 2.30133 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 285 loss: 2.30135 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 286 loss: 2.30131 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 287 loss: 2.30130 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 288 loss: 2.30131 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 289 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 290 loss: 2.30137 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 291 loss: 2.30141 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 292 loss: 2.30136 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 293 loss: 2.30133 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 294 loss: 2.30127 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 295 loss: 2.30127 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 296 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 297 loss: 2.30122 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 298 loss: 2.30123 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 299 loss: 2.30121 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 300 loss: 2.30124 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 301 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 302 loss: 2.30127 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 303 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 304 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 305 loss: 2.30122 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 306 loss: 2.30118 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 307 loss: 2.30122 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 308 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 309 loss: 2.30131 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 310 loss: 2.30137 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 311 loss: 2.30137 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 312 loss: 2.30136 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 313 loss: 2.30136 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 314 loss: 2.30136 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 315 loss: 2.30134 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 316 loss: 2.30141 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 317 loss: 2.30140 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 318 loss: 2.30139 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 319 loss: 2.30144 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 320 loss: 2.30141 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 321 loss: 2.30138 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 322 loss: 2.30122 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 323 loss: 2.30127 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 324 loss: 2.30124 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 325 loss: 2.30123 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 326 loss: 2.30120 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 327 loss: 2.30123 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 328 loss: 2.30127 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 329 loss: 2.30133 training acc: 0.11356 testing acc: 0.1068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 330 loss: 2.30138 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 331 loss: 2.30138 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 332 loss: 2.30134 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 333 loss: 2.30127 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 334 loss: 2.30120 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 335 loss: 2.30127 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 336 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 337 loss: 2.30131 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 338 loss: 2.30135 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 339 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 340 loss: 2.30129 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 341 loss: 2.30128 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 342 loss: 2.30124 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 343 loss: 2.30125 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 344 loss: 2.30126 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 345 loss: 2.30124 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 346 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 347 loss: 2.30132 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 348 loss: 2.30134 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 349 loss: 2.30131 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 350 loss: 2.30129 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 351 loss: 2.30125 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 352 loss: 2.30119 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 353 loss: 2.30122 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 354 loss: 2.30114 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 355 loss: 2.30113 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 356 loss: 2.30114 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 357 loss: 2.30115 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 358 loss: 2.30117 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 359 loss: 2.30116 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 360 loss: 2.30116 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 361 loss: 2.30116 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 362 loss: 2.30112 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 363 loss: 2.30112 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 364 loss: 2.30106 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 365 loss: 2.30109 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 366 loss: 2.30102 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 367 loss: 2.30110 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 368 loss: 2.30109 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 369 loss: 2.30104 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 370 loss: 2.30099 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 371 loss: 2.30100 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 372 loss: 2.30092 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 373 loss: 2.30086 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 374 loss: 2.30076 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 375 loss: 2.30074 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 376 loss: 2.30068 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 377 loss: 2.30070 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 378 loss: 2.30069 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 379 loss: 2.30071 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 380 loss: 2.30064 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 381 loss: 2.30065 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 382 loss: 2.30067 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 383 loss: 2.30064 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 384 loss: 2.30061 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 385 loss: 2.30063 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 386 loss: 2.30066 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 387 loss: 2.30061 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 388 loss: 2.30054 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 389 loss: 2.30052 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 390 loss: 2.30050 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 391 loss: 2.30057 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 392 loss: 2.30057 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 393 loss: 2.30054 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 394 loss: 2.30061 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 395 loss: 2.30056 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 396 loss: 2.30048 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 397 loss: 2.30052 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 398 loss: 2.30057 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 399 loss: 2.30047 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 400 loss: 2.30049 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 401 loss: 2.30044 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 402 loss: 2.30039 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 403 loss: 2.30035 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 404 loss: 2.30033 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 405 loss: 2.30029 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 406 loss: 2.30013 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 407 loss: 2.30012 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 408 loss: 2.30015 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 409 loss: 2.30012 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 410 loss: 2.30000 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 411 loss: 2.29998 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 412 loss: 2.29998 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 413 loss: 2.29996 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 414 loss: 2.29993 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 415 loss: 2.29994 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 416 loss: 2.29988 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 417 loss: 2.29991 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 418 loss: 2.29997 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 419 loss: 2.29994 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 420 loss: 2.29999 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 421 loss: 2.30004 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 422 loss: 2.29990 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 423 loss: 2.29991 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 424 loss: 2.29984 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 425 loss: 2.29971 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 426 loss: 2.29981 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 427 loss: 2.29967 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 428 loss: 2.29965 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 429 loss: 2.29962 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 430 loss: 2.29958 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 431 loss: 2.29955 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 432 loss: 2.29953 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 433 loss: 2.29941 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 434 loss: 2.29939 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 435 loss: 2.29928 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 436 loss: 2.29914 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 437 loss: 2.29913 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 438 loss: 2.29885 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 439 loss: 2.29878 training acc: 0.11362 testing acc: 0.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 440 loss: 2.29869 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 441 loss: 2.29863 training acc: 0.11356 testing acc: 0.1068\n",
      "epoch: 1 iter: 442 loss: 2.29864 training acc: 0.11414 testing acc: 0.1074\n",
      "epoch: 1 iter: 443 loss: 2.29860 training acc: 0.11416 testing acc: 0.1074\n",
      "epoch: 1 iter: 444 loss: 2.29855 training acc: 0.12436 testing acc: 0.1166\n",
      "epoch: 1 iter: 445 loss: 2.29856 training acc: 0.17802 testing acc: 0.1708\n",
      "epoch: 1 iter: 446 loss: 2.29843 training acc: 0.17934 testing acc: 0.172\n",
      "epoch: 1 iter: 447 loss: 2.29820 training acc: 0.11702 testing acc: 0.1094\n",
      "epoch: 1 iter: 448 loss: 2.29814 training acc: 0.12454 testing acc: 0.1166\n",
      "epoch: 1 iter: 449 loss: 2.29799 training acc: 0.11482 testing acc: 0.1076\n",
      "epoch: 1 iter: 450 loss: 2.29785 training acc: 0.11598 testing acc: 0.1086\n",
      "epoch: 1 iter: 451 loss: 2.29771 training acc: 0.12590 testing acc: 0.118\n",
      "epoch: 1 iter: 452 loss: 2.29753 training acc: 0.11936 testing acc: 0.1116\n",
      "epoch: 1 iter: 453 loss: 2.29743 training acc: 0.12464 testing acc: 0.1166\n",
      "epoch: 1 iter: 454 loss: 2.29731 training acc: 0.14814 testing acc: 0.1402\n",
      "epoch: 1 iter: 455 loss: 2.29730 training acc: 0.14754 testing acc: 0.1394\n",
      "epoch: 1 iter: 456 loss: 2.29716 training acc: 0.17022 testing acc: 0.1646\n",
      "epoch: 1 iter: 457 loss: 2.29701 training acc: 0.13602 testing acc: 0.1266\n",
      "epoch: 1 iter: 458 loss: 2.29682 training acc: 0.14678 testing acc: 0.139\n",
      "epoch: 1 iter: 459 loss: 2.29663 training acc: 0.17794 testing acc: 0.1714\n",
      "epoch: 1 iter: 460 loss: 2.29646 training acc: 0.19178 testing acc: 0.1854\n",
      "epoch: 1 iter: 461 loss: 2.29625 training acc: 0.18564 testing acc: 0.1808\n",
      "epoch: 1 iter: 462 loss: 2.29610 training acc: 0.19248 testing acc: 0.1862\n",
      "epoch: 1 iter: 463 loss: 2.29598 training acc: 0.17450 testing acc: 0.1678\n",
      "epoch: 1 iter: 464 loss: 2.29569 training acc: 0.13956 testing acc: 0.1296\n",
      "epoch: 1 iter: 465 loss: 2.29539 training acc: 0.14968 testing acc: 0.1416\n",
      "epoch: 1 iter: 466 loss: 2.29525 training acc: 0.16730 testing acc: 0.1622\n",
      "epoch: 1 iter: 467 loss: 2.29510 training acc: 0.15946 testing acc: 0.1532\n",
      "epoch: 1 iter: 468 loss: 2.29493 training acc: 0.18862 testing acc: 0.181\n",
      "epoch: 1 iter: 469 loss: 2.29479 training acc: 0.19258 testing acc: 0.1846\n",
      "epoch: 1 iter: 470 loss: 2.29464 training acc: 0.18356 testing acc: 0.1764\n",
      "epoch: 1 iter: 471 loss: 2.29426 training acc: 0.14144 testing acc: 0.131\n",
      "epoch: 1 iter: 472 loss: 2.29387 training acc: 0.13624 testing acc: 0.1262\n",
      "epoch: 1 iter: 473 loss: 2.29347 training acc: 0.15044 testing acc: 0.1416\n",
      "epoch: 1 iter: 474 loss: 2.29344 training acc: 0.16834 testing acc: 0.1626\n",
      "epoch: 1 iter: 475 loss: 2.29298 training acc: 0.16566 testing acc: 0.1604\n",
      "epoch: 1 iter: 476 loss: 2.29275 training acc: 0.16642 testing acc: 0.1608\n",
      "epoch: 1 iter: 477 loss: 2.29246 training acc: 0.17954 testing acc: 0.1724\n",
      "epoch: 1 iter: 478 loss: 2.29256 training acc: 0.19382 testing acc: 0.1874\n",
      "epoch: 1 iter: 479 loss: 2.29181 training acc: 0.19424 testing acc: 0.1874\n",
      "epoch: 1 iter: 480 loss: 2.29143 training acc: 0.19310 testing acc: 0.1838\n",
      "epoch: 1 iter: 481 loss: 2.29082 training acc: 0.20180 testing acc: 0.19\n",
      "epoch: 1 iter: 482 loss: 2.29031 training acc: 0.18598 testing acc: 0.1762\n",
      "epoch: 1 iter: 483 loss: 2.28972 training acc: 0.17588 testing acc: 0.166\n",
      "epoch: 1 iter: 484 loss: 2.28900 training acc: 0.18910 testing acc: 0.18\n",
      "epoch: 1 iter: 485 loss: 2.28853 training acc: 0.20468 testing acc: 0.1974\n",
      "epoch: 1 iter: 486 loss: 2.28795 training acc: 0.19522 testing acc: 0.1848\n",
      "epoch: 1 iter: 487 loss: 2.28705 training acc: 0.21216 testing acc: 0.2016\n",
      "epoch: 1 iter: 488 loss: 2.28588 training acc: 0.21236 testing acc: 0.2068\n",
      "epoch: 1 iter: 489 loss: 2.28499 training acc: 0.24164 testing acc: 0.2338\n",
      "epoch: 1 iter: 490 loss: 2.28386 training acc: 0.22896 testing acc: 0.2216\n",
      "epoch: 1 iter: 491 loss: 2.28303 training acc: 0.24300 testing acc: 0.2384\n",
      "epoch: 1 iter: 492 loss: 2.28190 training acc: 0.21426 testing acc: 0.2106\n",
      "epoch: 1 iter: 493 loss: 2.28066 training acc: 0.22812 testing acc: 0.2224\n",
      "epoch: 1 iter: 494 loss: 2.27956 training acc: 0.21060 testing acc: 0.206\n",
      "epoch: 1 iter: 495 loss: 2.27900 training acc: 0.21082 testing acc: 0.206\n",
      "epoch: 1 iter: 496 loss: 2.27775 training acc: 0.21054 testing acc: 0.206\n",
      "epoch: 1 iter: 497 loss: 2.27605 training acc: 0.21016 testing acc: 0.2052\n",
      "epoch: 1 iter: 498 loss: 2.27382 training acc: 0.21458 testing acc: 0.2112\n",
      "epoch: 1 iter: 499 loss: 2.27259 training acc: 0.20970 testing acc: 0.2048\n",
      "epoch: 1 iter: 500 loss: 2.27171 training acc: 0.20986 testing acc: 0.205\n",
      "epoch: 1 iter: 501 loss: 2.27039 training acc: 0.21002 testing acc: 0.205\n",
      "epoch: 1 iter: 502 loss: 2.26920 training acc: 0.21006 testing acc: 0.2052\n",
      "epoch: 1 iter: 503 loss: 2.26758 training acc: 0.21012 testing acc: 0.2054\n",
      "epoch: 1 iter: 504 loss: 2.26609 training acc: 0.20978 testing acc: 0.2048\n",
      "epoch: 1 iter: 505 loss: 2.26341 training acc: 0.20898 testing acc: 0.204\n",
      "epoch: 1 iter: 506 loss: 2.26216 training acc: 0.20926 testing acc: 0.2046\n",
      "epoch: 1 iter: 507 loss: 2.26073 training acc: 0.20868 testing acc: 0.204\n",
      "epoch: 1 iter: 508 loss: 2.25959 training acc: 0.20936 testing acc: 0.2046\n",
      "epoch: 1 iter: 509 loss: 2.25808 training acc: 0.20972 testing acc: 0.2048\n",
      "epoch: 1 iter: 510 loss: 2.25494 training acc: 0.20878 testing acc: 0.204\n",
      "epoch: 1 iter: 511 loss: 2.25171 training acc: 0.20770 testing acc: 0.2028\n",
      "epoch: 1 iter: 512 loss: 2.25046 training acc: 0.20882 testing acc: 0.204\n",
      "epoch: 1 iter: 513 loss: 2.24800 training acc: 0.20916 testing acc: 0.204\n",
      "epoch: 1 iter: 514 loss: 2.24485 training acc: 0.20868 testing acc: 0.2038\n",
      "epoch: 1 iter: 515 loss: 2.24150 training acc: 0.20784 testing acc: 0.2032\n",
      "epoch: 1 iter: 516 loss: 2.24002 training acc: 0.20872 testing acc: 0.2038\n",
      "epoch: 1 iter: 517 loss: 2.23613 training acc: 0.20778 testing acc: 0.2028\n",
      "epoch: 1 iter: 518 loss: 2.23280 training acc: 0.20714 testing acc: 0.2026\n",
      "epoch: 1 iter: 519 loss: 2.23126 training acc: 0.20790 testing acc: 0.2028\n",
      "epoch: 1 iter: 520 loss: 2.22663 training acc: 0.20572 testing acc: 0.2016\n",
      "epoch: 1 iter: 521 loss: 2.22494 training acc: 0.20634 testing acc: 0.202\n",
      "epoch: 1 iter: 522 loss: 2.22207 training acc: 0.20678 testing acc: 0.2022\n",
      "epoch: 1 iter: 523 loss: 2.21790 training acc: 0.20590 testing acc: 0.2016\n",
      "epoch: 1 iter: 524 loss: 2.21404 training acc: 0.20510 testing acc: 0.2012\n",
      "epoch: 1 iter: 525 loss: 2.20916 training acc: 0.20538 testing acc: 0.2012\n",
      "epoch: 1 iter: 526 loss: 2.20771 training acc: 0.20674 testing acc: 0.202\n",
      "epoch: 1 iter: 527 loss: 2.20596 training acc: 0.20814 testing acc: 0.203\n",
      "epoch: 1 iter: 528 loss: 2.20230 training acc: 0.20800 testing acc: 0.203\n",
      "epoch: 1 iter: 529 loss: 2.19836 training acc: 0.20772 testing acc: 0.203\n",
      "epoch: 1 iter: 530 loss: 2.19291 training acc: 0.20656 testing acc: 0.2016\n",
      "epoch: 1 iter: 531 loss: 2.19036 training acc: 0.20832 testing acc: 0.2034\n",
      "epoch: 1 iter: 532 loss: 2.18691 training acc: 0.20828 testing acc: 0.2034\n",
      "epoch: 1 iter: 533 loss: 2.18156 training acc: 0.20698 testing acc: 0.2018\n",
      "epoch: 1 iter: 534 loss: 2.17852 training acc: 0.20726 testing acc: 0.2022\n",
      "epoch: 1 iter: 535 loss: 2.17489 training acc: 0.20772 testing acc: 0.203\n",
      "epoch: 1 iter: 536 loss: 2.17087 training acc: 0.20778 testing acc: 0.203\n",
      "epoch: 1 iter: 537 loss: 2.16758 training acc: 0.20746 testing acc: 0.2024\n",
      "epoch: 1 iter: 538 loss: 2.16371 training acc: 0.20790 testing acc: 0.2028\n",
      "epoch: 1 iter: 539 loss: 2.15999 training acc: 0.20880 testing acc: 0.2036\n",
      "epoch: 1 iter: 540 loss: 2.15578 training acc: 0.20934 testing acc: 0.2044\n",
      "epoch: 1 iter: 541 loss: 2.15213 training acc: 0.20944 testing acc: 0.2044\n",
      "epoch: 1 iter: 542 loss: 2.14823 training acc: 0.20914 testing acc: 0.2038\n",
      "epoch: 1 iter: 543 loss: 2.14440 training acc: 0.20942 testing acc: 0.2044\n",
      "epoch: 1 iter: 544 loss: 2.14171 training acc: 0.20914 testing acc: 0.2036\n",
      "epoch: 1 iter: 545 loss: 2.13731 training acc: 0.20966 testing acc: 0.205\n",
      "epoch: 1 iter: 546 loss: 2.13310 training acc: 0.20940 testing acc: 0.2042\n",
      "epoch: 1 iter: 547 loss: 2.13067 training acc: 0.21374 testing acc: 0.2088\n",
      "epoch: 1 iter: 548 loss: 2.12741 training acc: 0.21156 testing acc: 0.2068\n",
      "epoch: 1 iter: 549 loss: 2.12396 training acc: 0.21166 testing acc: 0.2064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 550 loss: 2.12064 training acc: 0.21216 testing acc: 0.206\n",
      "epoch: 1 iter: 551 loss: 2.11782 training acc: 0.21344 testing acc: 0.2092\n",
      "epoch: 1 iter: 552 loss: 2.11503 training acc: 0.21466 testing acc: 0.2094\n",
      "epoch: 1 iter: 553 loss: 2.11125 training acc: 0.21184 testing acc: 0.2058\n",
      "epoch: 1 iter: 554 loss: 2.10693 training acc: 0.21406 testing acc: 0.2078\n",
      "epoch: 1 iter: 555 loss: 2.10239 training acc: 0.21370 testing acc: 0.2084\n",
      "epoch: 1 iter: 556 loss: 2.09897 training acc: 0.21616 testing acc: 0.2104\n",
      "epoch: 1 iter: 557 loss: 2.09630 training acc: 0.21674 testing acc: 0.2114\n",
      "epoch: 1 iter: 558 loss: 2.09463 training acc: 0.21612 testing acc: 0.2106\n",
      "epoch: 1 iter: 559 loss: 2.09028 training acc: 0.21606 testing acc: 0.2102\n",
      "epoch: 1 iter: 560 loss: 2.08710 training acc: 0.21712 testing acc: 0.212\n",
      "epoch: 1 iter: 561 loss: 2.08323 training acc: 0.21614 testing acc: 0.2106\n",
      "epoch: 1 iter: 562 loss: 2.08085 training acc: 0.21684 testing acc: 0.2106\n",
      "epoch: 1 iter: 563 loss: 2.07911 training acc: 0.21538 testing acc: 0.2104\n",
      "epoch: 1 iter: 564 loss: 2.07655 training acc: 0.21744 testing acc: 0.212\n",
      "epoch: 1 iter: 565 loss: 2.07336 training acc: 0.21902 testing acc: 0.2138\n",
      "epoch: 1 iter: 566 loss: 2.07010 training acc: 0.21962 testing acc: 0.2146\n",
      "epoch: 1 iter: 567 loss: 2.06723 training acc: 0.21864 testing acc: 0.2168\n",
      "epoch: 1 iter: 568 loss: 2.06311 training acc: 0.21756 testing acc: 0.2126\n",
      "epoch: 1 iter: 569 loss: 2.06027 training acc: 0.21994 testing acc: 0.2154\n",
      "epoch: 1 iter: 570 loss: 2.05665 training acc: 0.21816 testing acc: 0.2124\n",
      "epoch: 1 iter: 571 loss: 2.05711 training acc: 0.21572 testing acc: 0.2108\n",
      "epoch: 1 iter: 572 loss: 2.05251 training acc: 0.21656 testing acc: 0.211\n",
      "epoch: 1 iter: 573 loss: 2.04817 training acc: 0.21760 testing acc: 0.2122\n",
      "epoch: 1 iter: 574 loss: 2.04550 training acc: 0.21770 testing acc: 0.212\n",
      "epoch: 1 iter: 575 loss: 2.04420 training acc: 0.21484 testing acc: 0.2108\n",
      "epoch: 1 iter: 576 loss: 2.03893 training acc: 0.21940 testing acc: 0.2138\n",
      "epoch: 1 iter: 577 loss: 2.03587 training acc: 0.22324 testing acc: 0.2186\n",
      "epoch: 1 iter: 578 loss: 2.03231 training acc: 0.22224 testing acc: 0.2172\n",
      "epoch: 1 iter: 579 loss: 2.03544 training acc: 0.21522 testing acc: 0.2108\n",
      "epoch: 1 iter: 580 loss: 2.02701 training acc: 0.21946 testing acc: 0.2158\n",
      "epoch: 1 iter: 581 loss: 2.02433 training acc: 0.22392 testing acc: 0.2188\n",
      "epoch: 1 iter: 582 loss: 2.02274 training acc: 0.22118 testing acc: 0.217\n",
      "epoch: 1 iter: 583 loss: 2.02085 training acc: 0.22096 testing acc: 0.2164\n",
      "epoch: 1 iter: 584 loss: 2.01912 training acc: 0.22464 testing acc: 0.219\n",
      "epoch: 1 iter: 585 loss: 2.01566 training acc: 0.22544 testing acc: 0.2186\n",
      "epoch: 1 iter: 586 loss: 2.01308 training acc: 0.22354 testing acc: 0.218\n",
      "epoch: 1 iter: 587 loss: 2.01018 training acc: 0.23184 testing acc: 0.2284\n",
      "epoch: 1 iter: 588 loss: 2.00873 training acc: 0.23376 testing acc: 0.2302\n",
      "epoch: 1 iter: 589 loss: 2.00478 training acc: 0.22902 testing acc: 0.2234\n",
      "epoch: 1 iter: 590 loss: 2.00643 training acc: 0.22210 testing acc: 0.2162\n",
      "epoch: 1 iter: 591 loss: 2.00212 training acc: 0.22548 testing acc: 0.22\n",
      "epoch: 1 iter: 592 loss: 2.00116 training acc: 0.22150 testing acc: 0.214\n",
      "epoch: 1 iter: 593 loss: 1.99586 training acc: 0.22844 testing acc: 0.2252\n",
      "epoch: 1 iter: 594 loss: 1.99295 training acc: 0.23396 testing acc: 0.2276\n",
      "epoch: 1 iter: 595 loss: 1.99206 training acc: 0.23160 testing acc: 0.2304\n",
      "epoch: 1 iter: 596 loss: 1.98873 training acc: 0.22496 testing acc: 0.222\n",
      "epoch: 1 iter: 597 loss: 1.98717 training acc: 0.22886 testing acc: 0.2232\n",
      "epoch: 1 iter: 598 loss: 1.98761 training acc: 0.23872 testing acc: 0.2376\n",
      "epoch: 1 iter: 599 loss: 1.98419 training acc: 0.23998 testing acc: 0.237\n",
      "epoch: 1 iter: 600 loss: 1.98778 training acc: 0.25638 testing acc: 0.2538\n",
      "epoch: 1 iter: 601 loss: 1.98434 training acc: 0.25602 testing acc: 0.2528\n",
      "epoch: 1 iter: 602 loss: 1.98093 training acc: 0.25240 testing acc: 0.2504\n",
      "epoch: 1 iter: 603 loss: 1.97864 training acc: 0.24654 testing acc: 0.2432\n",
      "epoch: 1 iter: 604 loss: 1.97588 training acc: 0.24936 testing acc: 0.2466\n",
      "epoch: 1 iter: 605 loss: 1.97428 training acc: 0.22972 testing acc: 0.2232\n",
      "epoch: 1 iter: 606 loss: 1.97053 training acc: 0.23078 testing acc: 0.225\n",
      "epoch: 1 iter: 607 loss: 1.96855 training acc: 0.23012 testing acc: 0.2242\n",
      "epoch: 1 iter: 608 loss: 1.96764 training acc: 0.22780 testing acc: 0.2246\n",
      "epoch: 1 iter: 609 loss: 1.96456 training acc: 0.23382 testing acc: 0.2288\n",
      "epoch: 1 iter: 610 loss: 1.96251 training acc: 0.23720 testing acc: 0.2342\n",
      "epoch: 1 iter: 611 loss: 1.96479 training acc: 0.23110 testing acc: 0.2244\n",
      "epoch: 1 iter: 612 loss: 1.96179 training acc: 0.23434 testing acc: 0.2302\n",
      "epoch: 1 iter: 613 loss: 1.96045 training acc: 0.23722 testing acc: 0.233\n",
      "epoch: 1 iter: 614 loss: 1.96757 training acc: 0.22584 testing acc: 0.221\n",
      "epoch: 1 iter: 615 loss: 1.95789 training acc: 0.23820 testing acc: 0.2342\n",
      "epoch: 1 iter: 616 loss: 1.95497 training acc: 0.23744 testing acc: 0.2334\n",
      "epoch: 1 iter: 617 loss: 1.95123 training acc: 0.23824 testing acc: 0.234\n",
      "epoch: 1 iter: 618 loss: 1.94918 training acc: 0.23922 testing acc: 0.2328\n",
      "epoch: 1 iter: 619 loss: 1.94805 training acc: 0.24242 testing acc: 0.239\n",
      "epoch: 1 iter: 620 loss: 1.94791 training acc: 0.23766 testing acc: 0.2342\n",
      "epoch: 1 iter: 621 loss: 1.94662 training acc: 0.23604 testing acc: 0.2314\n",
      "epoch: 1 iter: 622 loss: 1.95074 training acc: 0.24372 testing acc: 0.2436\n",
      "epoch: 1 iter: 623 loss: 1.94424 training acc: 0.23394 testing acc: 0.2294\n",
      "epoch: 1 iter: 624 loss: 1.94418 training acc: 0.24008 testing acc: 0.2394\n",
      "epoch: 1 iter: 625 loss: 1.93930 training acc: 0.22962 testing acc: 0.2242\n",
      "epoch: 1 iter: 626 loss: 1.93787 training acc: 0.23092 testing acc: 0.2256\n",
      "epoch: 1 iter: 627 loss: 1.94585 training acc: 0.22130 testing acc: 0.2158\n",
      "epoch: 1 iter: 628 loss: 1.93896 training acc: 0.22406 testing acc: 0.2174\n",
      "epoch: 1 iter: 629 loss: 1.93280 training acc: 0.22938 testing acc: 0.223\n",
      "epoch: 1 iter: 630 loss: 1.93129 training acc: 0.23976 testing acc: 0.2372\n",
      "epoch: 1 iter: 631 loss: 1.93585 training acc: 0.23012 testing acc: 0.2256\n",
      "epoch: 1 iter: 632 loss: 1.93897 training acc: 0.22300 testing acc: 0.2164\n",
      "epoch: 1 iter: 633 loss: 1.93328 training acc: 0.22688 testing acc: 0.2216\n",
      "epoch: 1 iter: 634 loss: 1.92872 training acc: 0.23798 testing acc: 0.2372\n",
      "epoch: 1 iter: 635 loss: 1.92918 training acc: 0.23986 testing acc: 0.2388\n",
      "epoch: 1 iter: 636 loss: 1.92460 training acc: 0.23512 testing acc: 0.232\n",
      "epoch: 1 iter: 637 loss: 1.92391 training acc: 0.23662 testing acc: 0.2354\n",
      "epoch: 1 iter: 638 loss: 1.92760 training acc: 0.23380 testing acc: 0.228\n",
      "epoch: 1 iter: 639 loss: 1.92442 training acc: 0.23488 testing acc: 0.23\n",
      "epoch: 1 iter: 640 loss: 1.92079 training acc: 0.24982 testing acc: 0.2482\n",
      "epoch: 1 iter: 641 loss: 1.91876 training acc: 0.24712 testing acc: 0.2454\n",
      "epoch: 1 iter: 642 loss: 1.92415 training acc: 0.27152 testing acc: 0.2698\n",
      "epoch: 1 iter: 643 loss: 1.91463 training acc: 0.25530 testing acc: 0.2546\n",
      "epoch: 1 iter: 644 loss: 1.91962 training acc: 0.26992 testing acc: 0.2672\n",
      "epoch: 1 iter: 645 loss: 1.92001 training acc: 0.26924 testing acc: 0.2656\n",
      "epoch: 1 iter: 646 loss: 1.92102 training acc: 0.22904 testing acc: 0.221\n",
      "epoch: 1 iter: 647 loss: 1.91103 training acc: 0.25464 testing acc: 0.2522\n",
      "epoch: 1 iter: 648 loss: 1.91723 training acc: 0.26694 testing acc: 0.2606\n",
      "epoch: 1 iter: 649 loss: 1.91954 training acc: 0.26776 testing acc: 0.2652\n",
      "epoch: 1 iter: 650 loss: 1.91089 training acc: 0.25682 testing acc: 0.2512\n",
      "epoch: 1 iter: 651 loss: 1.91346 training acc: 0.27012 testing acc: 0.2668\n",
      "epoch: 1 iter: 652 loss: 1.91952 training acc: 0.26246 testing acc: 0.259\n",
      "epoch: 1 iter: 653 loss: 1.90753 training acc: 0.25174 testing acc: 0.2472\n",
      "epoch: 1 iter: 654 loss: 1.90755 training acc: 0.23216 testing acc: 0.2244\n",
      "epoch: 1 iter: 655 loss: 1.90790 training acc: 0.25920 testing acc: 0.2594\n",
      "epoch: 1 iter: 656 loss: 1.90113 training acc: 0.24616 testing acc: 0.2438\n",
      "epoch: 1 iter: 657 loss: 1.90385 training acc: 0.25188 testing acc: 0.2486\n",
      "epoch: 1 iter: 658 loss: 1.89847 training acc: 0.24378 testing acc: 0.2412\n",
      "epoch: 1 iter: 659 loss: 1.90152 training acc: 0.25778 testing acc: 0.256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 660 loss: 1.89348 training acc: 0.24474 testing acc: 0.2444\n",
      "epoch: 1 iter: 661 loss: 1.89617 training acc: 0.24114 testing acc: 0.2402\n",
      "epoch: 1 iter: 662 loss: 1.89610 training acc: 0.23994 testing acc: 0.2366\n",
      "epoch: 1 iter: 663 loss: 1.89366 training acc: 0.24342 testing acc: 0.2424\n",
      "epoch: 1 iter: 664 loss: 1.89862 training acc: 0.25424 testing acc: 0.251\n",
      "epoch: 1 iter: 665 loss: 1.89450 training acc: 0.27020 testing acc: 0.2682\n",
      "epoch: 1 iter: 666 loss: 1.89237 training acc: 0.26528 testing acc: 0.264\n",
      "epoch: 1 iter: 667 loss: 1.90696 training acc: 0.27380 testing acc: 0.2686\n",
      "epoch: 1 iter: 668 loss: 1.89127 training acc: 0.26686 testing acc: 0.2648\n",
      "epoch: 1 iter: 669 loss: 1.88534 training acc: 0.25478 testing acc: 0.2562\n",
      "epoch: 1 iter: 670 loss: 1.89923 training acc: 0.23064 testing acc: 0.222\n",
      "epoch: 1 iter: 671 loss: 1.88507 training acc: 0.26326 testing acc: 0.2648\n",
      "epoch: 1 iter: 672 loss: 1.88045 training acc: 0.25080 testing acc: 0.2534\n",
      "epoch: 1 iter: 673 loss: 1.88303 training acc: 0.24158 testing acc: 0.2378\n",
      "epoch: 1 iter: 674 loss: 1.88798 training acc: 0.23286 testing acc: 0.2286\n",
      "epoch: 1 iter: 675 loss: 1.88126 training acc: 0.24694 testing acc: 0.2438\n",
      "epoch: 1 iter: 676 loss: 1.88136 training acc: 0.24722 testing acc: 0.2422\n",
      "epoch: 1 iter: 677 loss: 1.87774 training acc: 0.25360 testing acc: 0.25\n",
      "epoch: 1 iter: 678 loss: 1.88356 training acc: 0.26722 testing acc: 0.2644\n",
      "epoch: 1 iter: 679 loss: 1.87342 training acc: 0.24490 testing acc: 0.244\n",
      "epoch: 1 iter: 680 loss: 1.87248 training acc: 0.25290 testing acc: 0.2528\n",
      "epoch: 1 iter: 681 loss: 1.87275 training acc: 0.25216 testing acc: 0.2526\n",
      "epoch: 1 iter: 682 loss: 1.87680 training acc: 0.26768 testing acc: 0.2696\n",
      "epoch: 1 iter: 683 loss: 1.87510 training acc: 0.26384 testing acc: 0.2656\n",
      "epoch: 1 iter: 684 loss: 1.88758 training acc: 0.27342 testing acc: 0.2726\n",
      "epoch: 1 iter: 685 loss: 1.90303 training acc: 0.26544 testing acc: 0.2584\n",
      "epoch: 1 iter: 686 loss: 1.87384 training acc: 0.26764 testing acc: 0.2648\n",
      "epoch: 1 iter: 687 loss: 1.87619 training acc: 0.27330 testing acc: 0.2726\n",
      "epoch: 1 iter: 688 loss: 1.89341 training acc: 0.25906 testing acc: 0.252\n",
      "epoch: 1 iter: 689 loss: 1.87782 training acc: 0.26002 testing acc: 0.2566\n",
      "epoch: 1 iter: 690 loss: 1.87698 training acc: 0.27120 testing acc: 0.2706\n",
      "epoch: 1 iter: 691 loss: 1.86444 training acc: 0.24906 testing acc: 0.2516\n",
      "epoch: 1 iter: 692 loss: 1.86788 training acc: 0.24282 testing acc: 0.2452\n",
      "epoch: 1 iter: 693 loss: 1.86188 training acc: 0.25230 testing acc: 0.254\n",
      "epoch: 1 iter: 694 loss: 1.86048 training acc: 0.26130 testing acc: 0.2624\n",
      "epoch: 1 iter: 695 loss: 1.86273 training acc: 0.24332 testing acc: 0.2448\n",
      "epoch: 1 iter: 696 loss: 1.85973 training acc: 0.26140 testing acc: 0.2628\n",
      "epoch: 1 iter: 697 loss: 1.86051 training acc: 0.25462 testing acc: 0.2514\n",
      "epoch: 1 iter: 698 loss: 1.85958 training acc: 0.25084 testing acc: 0.2482\n",
      "epoch: 1 iter: 699 loss: 1.85576 training acc: 0.26082 testing acc: 0.2632\n",
      "epoch: 1 iter: 700 loss: 1.86931 training acc: 0.24772 testing acc: 0.244\n",
      "epoch: 1 iter: 701 loss: 1.86442 training acc: 0.27562 testing acc: 0.2794\n",
      "epoch: 1 iter: 702 loss: 1.85706 training acc: 0.27424 testing acc: 0.2734\n",
      "epoch: 1 iter: 703 loss: 1.85905 training acc: 0.27698 testing acc: 0.2748\n",
      "epoch: 1 iter: 704 loss: 1.86462 training acc: 0.27692 testing acc: 0.2768\n",
      "epoch: 1 iter: 705 loss: 1.86522 training acc: 0.24290 testing acc: 0.2428\n",
      "epoch: 1 iter: 706 loss: 1.85406 training acc: 0.24930 testing acc: 0.2464\n",
      "epoch: 1 iter: 707 loss: 1.85156 training acc: 0.25158 testing acc: 0.254\n",
      "epoch: 1 iter: 708 loss: 1.85324 training acc: 0.24948 testing acc: 0.252\n",
      "epoch: 1 iter: 709 loss: 1.85125 training acc: 0.25162 testing acc: 0.2484\n",
      "epoch: 1 iter: 710 loss: 1.85780 training acc: 0.26986 testing acc: 0.2648\n",
      "epoch: 1 iter: 711 loss: 1.85863 training acc: 0.26388 testing acc: 0.2616\n",
      "epoch: 1 iter: 712 loss: 1.86334 training acc: 0.27136 testing acc: 0.268\n",
      "epoch: 1 iter: 713 loss: 1.86727 training acc: 0.27414 testing acc: 0.2728\n",
      "epoch: 1 iter: 714 loss: 1.86394 training acc: 0.26782 testing acc: 0.2624\n",
      "epoch: 1 iter: 715 loss: 1.85818 training acc: 0.27260 testing acc: 0.2686\n",
      "epoch: 1 iter: 716 loss: 1.84945 training acc: 0.26830 testing acc: 0.2664\n",
      "epoch: 1 iter: 717 loss: 1.84845 training acc: 0.26972 testing acc: 0.267\n",
      "epoch: 1 iter: 718 loss: 1.84452 training acc: 0.26128 testing acc: 0.2566\n",
      "epoch: 1 iter: 719 loss: 1.84503 training acc: 0.26096 testing acc: 0.2578\n",
      "epoch: 1 iter: 720 loss: 1.84674 training acc: 0.27466 testing acc: 0.273\n",
      "epoch: 1 iter: 721 loss: 1.84498 training acc: 0.27092 testing acc: 0.2686\n",
      "epoch: 1 iter: 722 loss: 1.85057 training acc: 0.24818 testing acc: 0.2488\n",
      "epoch: 1 iter: 723 loss: 1.84973 training acc: 0.24836 testing acc: 0.2496\n",
      "epoch: 1 iter: 724 loss: 1.84296 training acc: 0.26138 testing acc: 0.2622\n",
      "epoch: 1 iter: 725 loss: 1.84436 training acc: 0.27206 testing acc: 0.275\n",
      "epoch: 1 iter: 726 loss: 1.84298 training acc: 0.26932 testing acc: 0.27\n",
      "epoch: 1 iter: 727 loss: 1.83977 training acc: 0.26086 testing acc: 0.2598\n",
      "epoch: 1 iter: 728 loss: 1.84012 training acc: 0.25514 testing acc: 0.2558\n",
      "epoch: 1 iter: 729 loss: 1.83902 training acc: 0.26354 testing acc: 0.2622\n",
      "epoch: 1 iter: 730 loss: 1.84423 training acc: 0.27496 testing acc: 0.274\n",
      "epoch: 1 iter: 731 loss: 1.83793 training acc: 0.25796 testing acc: 0.2606\n",
      "epoch: 1 iter: 732 loss: 1.83782 training acc: 0.25550 testing acc: 0.2538\n",
      "epoch: 1 iter: 733 loss: 1.84573 training acc: 0.25138 testing acc: 0.2522\n",
      "epoch: 1 iter: 734 loss: 1.87043 training acc: 0.24238 testing acc: 0.2404\n",
      "epoch: 1 iter: 735 loss: 1.83537 training acc: 0.25382 testing acc: 0.2526\n",
      "epoch: 1 iter: 736 loss: 1.84166 training acc: 0.27804 testing acc: 0.2762\n",
      "epoch: 1 iter: 737 loss: 1.84231 training acc: 0.27964 testing acc: 0.2766\n",
      "epoch: 1 iter: 738 loss: 1.83484 training acc: 0.27904 testing acc: 0.276\n",
      "epoch: 1 iter: 739 loss: 1.83413 training acc: 0.27364 testing acc: 0.275\n",
      "epoch: 1 iter: 740 loss: 1.83202 training acc: 0.27142 testing acc: 0.271\n",
      "epoch: 1 iter: 741 loss: 1.85230 training acc: 0.26972 testing acc: 0.2642\n",
      "epoch: 1 iter: 742 loss: 1.82757 training acc: 0.26028 testing acc: 0.26\n",
      "epoch: 1 iter: 743 loss: 1.82556 training acc: 0.26222 testing acc: 0.26\n",
      "epoch: 1 iter: 744 loss: 1.82919 training acc: 0.27332 testing acc: 0.2724\n",
      "epoch: 1 iter: 745 loss: 1.82854 training acc: 0.26362 testing acc: 0.2592\n",
      "epoch: 1 iter: 746 loss: 1.82723 training acc: 0.27132 testing acc: 0.2692\n",
      "epoch: 1 iter: 747 loss: 1.83069 training acc: 0.26806 testing acc: 0.266\n",
      "epoch: 1 iter: 748 loss: 1.82276 training acc: 0.27094 testing acc: 0.2702\n",
      "epoch: 1 iter: 749 loss: 1.82268 training acc: 0.27716 testing acc: 0.2732\n",
      "epoch: 1 iter: 750 loss: 1.82187 training acc: 0.27332 testing acc: 0.2678\n",
      "epoch: 1 iter: 751 loss: 1.82551 training acc: 0.26540 testing acc: 0.2612\n",
      "epoch: 1 iter: 752 loss: 1.81840 training acc: 0.27548 testing acc: 0.2792\n",
      "epoch: 1 iter: 753 loss: 1.81786 training acc: 0.27540 testing acc: 0.277\n",
      "epoch: 1 iter: 754 loss: 1.81955 training acc: 0.28012 testing acc: 0.2774\n",
      "epoch: 1 iter: 755 loss: 1.82047 training acc: 0.27858 testing acc: 0.2742\n",
      "epoch: 1 iter: 756 loss: 1.81674 training acc: 0.27994 testing acc: 0.2826\n",
      "epoch: 1 iter: 757 loss: 1.81683 training acc: 0.28348 testing acc: 0.283\n",
      "epoch: 1 iter: 758 loss: 1.81346 training acc: 0.26704 testing acc: 0.2672\n",
      "epoch: 1 iter: 759 loss: 1.81671 training acc: 0.27722 testing acc: 0.278\n",
      "epoch: 1 iter: 760 loss: 1.81480 training acc: 0.27208 testing acc: 0.27\n",
      "epoch: 1 iter: 761 loss: 1.81571 training acc: 0.27512 testing acc: 0.2752\n",
      "epoch: 1 iter: 762 loss: 1.81358 training acc: 0.25902 testing acc: 0.2594\n",
      "epoch: 1 iter: 763 loss: 1.82640 training acc: 0.25200 testing acc: 0.2516\n",
      "epoch: 1 iter: 764 loss: 1.81313 training acc: 0.28830 testing acc: 0.2872\n",
      "epoch: 1 iter: 765 loss: 1.81362 training acc: 0.28614 testing acc: 0.2886\n",
      "epoch: 1 iter: 766 loss: 1.82006 training acc: 0.27346 testing acc: 0.2714\n",
      "epoch: 1 iter: 767 loss: 1.82827 training acc: 0.28106 testing acc: 0.2808\n",
      "epoch: 1 iter: 768 loss: 1.82705 training acc: 0.28720 testing acc: 0.2852\n",
      "epoch: 1 iter: 769 loss: 1.81943 training acc: 0.27704 testing acc: 0.2818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 770 loss: 1.82401 training acc: 0.26856 testing acc: 0.2696\n",
      "epoch: 1 iter: 771 loss: 1.81171 training acc: 0.27814 testing acc: 0.2772\n",
      "epoch: 1 iter: 772 loss: 1.81295 training acc: 0.27620 testing acc: 0.2806\n",
      "epoch: 1 iter: 773 loss: 1.80595 training acc: 0.27876 testing acc: 0.279\n",
      "epoch: 1 iter: 774 loss: 1.81227 training acc: 0.27268 testing acc: 0.2754\n",
      "epoch: 1 iter: 775 loss: 1.80559 training acc: 0.27480 testing acc: 0.2748\n",
      "epoch: 1 iter: 776 loss: 1.80908 training acc: 0.27920 testing acc: 0.2772\n",
      "epoch: 1 iter: 777 loss: 1.80856 training acc: 0.25680 testing acc: 0.256\n",
      "epoch: 1 iter: 778 loss: 1.80557 training acc: 0.28542 testing acc: 0.2838\n",
      "epoch: 1 iter: 779 loss: 1.80410 training acc: 0.27870 testing acc: 0.2782\n",
      "epoch: 1 iter: 780 loss: 1.83890 training acc: 0.23546 testing acc: 0.2328\n",
      "epoch: 1 iter: 781 loss: 1.83455 training acc: 0.23510 testing acc: 0.2308\n",
      "epoch: 1 iter: 782 loss: 1.83369 training acc: 0.22476 testing acc: 0.2222\n",
      "epoch: 1 iter: 783 loss: 1.81738 training acc: 0.27800 testing acc: 0.273\n",
      "epoch: 1 iter: 784 loss: 1.79909 training acc: 0.26996 testing acc: 0.2674\n",
      "epoch: 1 iter: 785 loss: 1.80015 training acc: 0.27978 testing acc: 0.28\n",
      "epoch: 1 iter: 786 loss: 1.79835 training acc: 0.27766 testing acc: 0.2762\n",
      "epoch: 1 iter: 787 loss: 1.80094 training acc: 0.26320 testing acc: 0.2606\n",
      "epoch: 1 iter: 788 loss: 1.80175 training acc: 0.26078 testing acc: 0.2598\n",
      "epoch: 1 iter: 789 loss: 1.80157 training acc: 0.27730 testing acc: 0.2738\n",
      "epoch: 1 iter: 790 loss: 1.80317 training acc: 0.26512 testing acc: 0.2628\n",
      "epoch: 1 iter: 791 loss: 1.80696 training acc: 0.27178 testing acc: 0.2698\n",
      "epoch: 1 iter: 792 loss: 1.81785 training acc: 0.26692 testing acc: 0.2628\n",
      "epoch: 1 iter: 793 loss: 1.81123 training acc: 0.26398 testing acc: 0.2596\n",
      "epoch: 1 iter: 794 loss: 1.80340 training acc: 0.28118 testing acc: 0.2814\n",
      "epoch: 1 iter: 795 loss: 1.79322 training acc: 0.27284 testing acc: 0.2704\n",
      "epoch: 1 iter: 796 loss: 1.79588 training acc: 0.26312 testing acc: 0.2606\n",
      "epoch: 1 iter: 797 loss: 1.79330 training acc: 0.27476 testing acc: 0.2764\n",
      "epoch: 1 iter: 798 loss: 1.79458 training acc: 0.26916 testing acc: 0.2698\n",
      "epoch: 1 iter: 799 loss: 1.79162 training acc: 0.28266 testing acc: 0.281\n",
      "epoch: 1 iter: 800 loss: 1.78808 training acc: 0.27602 testing acc: 0.2748\n",
      "epoch: 1 iter: 801 loss: 1.79279 training acc: 0.26448 testing acc: 0.2626\n",
      "epoch: 1 iter: 802 loss: 1.78921 training acc: 0.27528 testing acc: 0.275\n",
      "epoch: 1 iter: 803 loss: 1.78929 training acc: 0.26376 testing acc: 0.2632\n",
      "epoch: 1 iter: 804 loss: 1.79148 training acc: 0.26152 testing acc: 0.261\n",
      "epoch: 1 iter: 805 loss: 1.78983 training acc: 0.27006 testing acc: 0.2682\n",
      "epoch: 1 iter: 806 loss: 1.78782 training acc: 0.27730 testing acc: 0.2774\n",
      "epoch: 1 iter: 807 loss: 1.79245 training acc: 0.28080 testing acc: 0.2774\n",
      "epoch: 1 iter: 808 loss: 1.79612 training acc: 0.27870 testing acc: 0.2804\n",
      "epoch: 1 iter: 809 loss: 1.79033 training acc: 0.26754 testing acc: 0.2642\n",
      "epoch: 1 iter: 810 loss: 1.78541 training acc: 0.27556 testing acc: 0.2746\n",
      "epoch: 1 iter: 811 loss: 1.78508 training acc: 0.27280 testing acc: 0.272\n",
      "epoch: 1 iter: 812 loss: 1.79047 training acc: 0.25478 testing acc: 0.2558\n",
      "epoch: 1 iter: 813 loss: 1.79609 training acc: 0.25950 testing acc: 0.2554\n",
      "epoch: 1 iter: 814 loss: 1.78365 training acc: 0.26850 testing acc: 0.2694\n",
      "epoch: 1 iter: 815 loss: 1.78106 training acc: 0.27416 testing acc: 0.2734\n",
      "epoch: 1 iter: 816 loss: 1.78769 training acc: 0.28048 testing acc: 0.2778\n",
      "epoch: 1 iter: 817 loss: 1.78936 training acc: 0.27696 testing acc: 0.2762\n",
      "epoch: 1 iter: 818 loss: 1.78997 training acc: 0.28178 testing acc: 0.2788\n",
      "epoch: 1 iter: 819 loss: 1.79254 training acc: 0.27674 testing acc: 0.2752\n",
      "epoch: 1 iter: 820 loss: 1.79891 training acc: 0.25786 testing acc: 0.258\n",
      "epoch: 1 iter: 821 loss: 1.78269 training acc: 0.28528 testing acc: 0.2832\n",
      "epoch: 1 iter: 822 loss: 1.77985 training acc: 0.28306 testing acc: 0.282\n",
      "epoch: 1 iter: 823 loss: 1.78448 training acc: 0.27300 testing acc: 0.2716\n",
      "epoch: 1 iter: 824 loss: 1.77946 training acc: 0.28228 testing acc: 0.2818\n",
      "epoch: 1 iter: 825 loss: 1.78330 training acc: 0.28474 testing acc: 0.2844\n",
      "epoch: 1 iter: 826 loss: 1.78716 training acc: 0.28746 testing acc: 0.2864\n",
      "epoch: 1 iter: 827 loss: 1.78650 training acc: 0.25784 testing acc: 0.2582\n",
      "epoch: 1 iter: 828 loss: 1.78049 training acc: 0.26516 testing acc: 0.261\n",
      "epoch: 1 iter: 829 loss: 1.79321 training acc: 0.28488 testing acc: 0.2818\n",
      "epoch: 1 iter: 830 loss: 1.78709 training acc: 0.28148 testing acc: 0.2806\n",
      "epoch: 1 iter: 831 loss: 1.77989 training acc: 0.25822 testing acc: 0.2604\n",
      "epoch: 1 iter: 832 loss: 1.77586 training acc: 0.27428 testing acc: 0.2738\n",
      "epoch: 1 iter: 833 loss: 1.77791 training acc: 0.27866 testing acc: 0.278\n",
      "epoch: 1 iter: 834 loss: 1.77946 training acc: 0.25984 testing acc: 0.2622\n",
      "epoch: 1 iter: 835 loss: 1.78185 training acc: 0.27282 testing acc: 0.2714\n",
      "epoch: 1 iter: 836 loss: 1.77829 training acc: 0.26686 testing acc: 0.267\n",
      "epoch: 1 iter: 837 loss: 1.77754 training acc: 0.27528 testing acc: 0.274\n",
      "epoch: 1 iter: 838 loss: 1.77500 training acc: 0.27238 testing acc: 0.2678\n",
      "epoch: 1 iter: 839 loss: 1.78764 training acc: 0.25590 testing acc: 0.2542\n",
      "epoch: 1 iter: 840 loss: 1.77987 training acc: 0.26310 testing acc: 0.2648\n",
      "epoch: 1 iter: 841 loss: 1.76981 training acc: 0.27692 testing acc: 0.2796\n",
      "epoch: 1 iter: 842 loss: 1.78166 training acc: 0.28416 testing acc: 0.2834\n",
      "epoch: 1 iter: 843 loss: 1.78407 training acc: 0.26656 testing acc: 0.2648\n",
      "epoch: 1 iter: 844 loss: 1.77092 training acc: 0.28352 testing acc: 0.2792\n",
      "epoch: 1 iter: 845 loss: 1.77157 training acc: 0.27916 testing acc: 0.2746\n",
      "epoch: 1 iter: 846 loss: 1.77126 training acc: 0.26650 testing acc: 0.2694\n",
      "epoch: 1 iter: 847 loss: 1.76704 training acc: 0.27968 testing acc: 0.2752\n",
      "epoch: 1 iter: 848 loss: 1.77952 training acc: 0.28094 testing acc: 0.2788\n",
      "epoch: 1 iter: 849 loss: 1.78280 training acc: 0.27924 testing acc: 0.2758\n",
      "epoch: 1 iter: 850 loss: 1.76481 training acc: 0.27356 testing acc: 0.2718\n",
      "epoch: 1 iter: 851 loss: 1.77190 training acc: 0.28436 testing acc: 0.284\n",
      "epoch: 1 iter: 852 loss: 1.76433 training acc: 0.27876 testing acc: 0.2788\n",
      "epoch: 1 iter: 853 loss: 1.76464 training acc: 0.26902 testing acc: 0.2714\n",
      "epoch: 1 iter: 854 loss: 1.76451 training acc: 0.27854 testing acc: 0.277\n",
      "epoch: 1 iter: 855 loss: 1.76234 training acc: 0.28064 testing acc: 0.2786\n",
      "epoch: 1 iter: 856 loss: 1.76324 training acc: 0.29214 testing acc: 0.2908\n",
      "epoch: 1 iter: 857 loss: 1.76537 training acc: 0.28124 testing acc: 0.2796\n",
      "epoch: 1 iter: 858 loss: 1.76626 training acc: 0.27566 testing acc: 0.2802\n",
      "epoch: 1 iter: 859 loss: 1.75860 training acc: 0.27236 testing acc: 0.273\n",
      "epoch: 1 iter: 860 loss: 1.76360 training acc: 0.26996 testing acc: 0.2734\n",
      "epoch: 1 iter: 861 loss: 1.76326 training acc: 0.28050 testing acc: 0.2766\n",
      "epoch: 1 iter: 862 loss: 1.76580 training acc: 0.27780 testing acc: 0.2764\n",
      "epoch: 1 iter: 863 loss: 1.76828 training acc: 0.28212 testing acc: 0.2772\n",
      "epoch: 1 iter: 864 loss: 1.76211 training acc: 0.27584 testing acc: 0.2734\n",
      "epoch: 1 iter: 865 loss: 1.76960 training acc: 0.27404 testing acc: 0.271\n",
      "epoch: 1 iter: 866 loss: 1.76117 training acc: 0.27224 testing acc: 0.2696\n",
      "epoch: 1 iter: 867 loss: 1.76204 training acc: 0.27840 testing acc: 0.2744\n",
      "epoch: 1 iter: 868 loss: 1.77318 training acc: 0.28106 testing acc: 0.2804\n",
      "epoch: 1 iter: 869 loss: 1.76003 training acc: 0.28006 testing acc: 0.2786\n",
      "epoch: 1 iter: 870 loss: 1.77433 training acc: 0.26146 testing acc: 0.263\n",
      "epoch: 1 iter: 871 loss: 1.76085 training acc: 0.28798 testing acc: 0.2866\n",
      "epoch: 1 iter: 872 loss: 1.75416 training acc: 0.28414 testing acc: 0.2794\n",
      "epoch: 1 iter: 873 loss: 1.76749 training acc: 0.29662 testing acc: 0.2966\n",
      "epoch: 1 iter: 874 loss: 1.75879 training acc: 0.27408 testing acc: 0.2748\n",
      "epoch: 1 iter: 875 loss: 1.75319 training acc: 0.28280 testing acc: 0.2822\n",
      "epoch: 1 iter: 876 loss: 1.76232 training acc: 0.29286 testing acc: 0.2922\n",
      "epoch: 1 iter: 877 loss: 1.74949 training acc: 0.28540 testing acc: 0.2906\n",
      "epoch: 1 iter: 878 loss: 1.75129 training acc: 0.29174 testing acc: 0.2934\n",
      "epoch: 1 iter: 879 loss: 1.75427 training acc: 0.29810 testing acc: 0.2996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 880 loss: 1.75958 training acc: 0.29344 testing acc: 0.2946\n",
      "epoch: 1 iter: 881 loss: 1.77770 training acc: 0.29122 testing acc: 0.294\n",
      "epoch: 1 iter: 882 loss: 1.75471 training acc: 0.29384 testing acc: 0.2942\n",
      "epoch: 1 iter: 883 loss: 1.74922 training acc: 0.27708 testing acc: 0.2796\n",
      "epoch: 1 iter: 884 loss: 1.75287 training acc: 0.29828 testing acc: 0.3016\n",
      "epoch: 1 iter: 885 loss: 1.74636 training acc: 0.29398 testing acc: 0.295\n",
      "epoch: 1 iter: 886 loss: 1.74686 training acc: 0.28922 testing acc: 0.286\n",
      "epoch: 1 iter: 887 loss: 1.75572 training acc: 0.29048 testing acc: 0.2936\n",
      "epoch: 1 iter: 888 loss: 1.75185 training acc: 0.29340 testing acc: 0.2946\n",
      "epoch: 1 iter: 889 loss: 1.75744 training acc: 0.26986 testing acc: 0.276\n",
      "epoch: 1 iter: 890 loss: 1.75461 training acc: 0.27786 testing acc: 0.2772\n",
      "epoch: 1 iter: 891 loss: 1.76997 training acc: 0.26660 testing acc: 0.2684\n",
      "epoch: 1 iter: 892 loss: 1.76304 training acc: 0.26710 testing acc: 0.2696\n",
      "epoch: 1 iter: 893 loss: 1.75845 training acc: 0.29746 testing acc: 0.2972\n",
      "epoch: 1 iter: 894 loss: 1.74847 training acc: 0.29396 testing acc: 0.2922\n",
      "epoch: 1 iter: 895 loss: 1.75128 training acc: 0.29528 testing acc: 0.2952\n",
      "epoch: 1 iter: 896 loss: 1.76148 training acc: 0.29624 testing acc: 0.2984\n",
      "epoch: 1 iter: 897 loss: 1.74464 training acc: 0.28952 testing acc: 0.2878\n",
      "epoch: 1 iter: 898 loss: 1.74398 training acc: 0.29274 testing acc: 0.2914\n",
      "epoch: 1 iter: 899 loss: 1.74105 training acc: 0.28120 testing acc: 0.2858\n",
      "epoch: 1 iter: 900 loss: 1.75556 training acc: 0.28662 testing acc: 0.2848\n",
      "epoch: 1 iter: 901 loss: 1.74714 training acc: 0.28568 testing acc: 0.286\n",
      "epoch: 1 iter: 902 loss: 1.74892 training acc: 0.28996 testing acc: 0.286\n",
      "epoch: 1 iter: 903 loss: 1.74064 training acc: 0.28004 testing acc: 0.2776\n",
      "epoch: 1 iter: 904 loss: 1.74630 training acc: 0.28228 testing acc: 0.281\n",
      "epoch: 1 iter: 905 loss: 1.74359 training acc: 0.27170 testing acc: 0.2696\n",
      "epoch: 1 iter: 906 loss: 1.73997 training acc: 0.27834 testing acc: 0.2796\n",
      "epoch: 1 iter: 907 loss: 1.75264 training acc: 0.27330 testing acc: 0.2714\n",
      "epoch: 1 iter: 908 loss: 1.74258 training acc: 0.28702 testing acc: 0.2912\n",
      "epoch: 1 iter: 909 loss: 1.74683 training acc: 0.30188 testing acc: 0.3016\n",
      "epoch: 1 iter: 910 loss: 1.75220 training acc: 0.29070 testing acc: 0.292\n",
      "epoch: 1 iter: 911 loss: 1.76971 training acc: 0.29566 testing acc: 0.294\n",
      "epoch: 1 iter: 912 loss: 1.74564 training acc: 0.29452 testing acc: 0.2962\n",
      "epoch: 1 iter: 913 loss: 1.74504 training acc: 0.29650 testing acc: 0.2978\n",
      "epoch: 1 iter: 914 loss: 1.74838 training acc: 0.30262 testing acc: 0.302\n",
      "epoch: 1 iter: 915 loss: 1.74273 training acc: 0.29572 testing acc: 0.2946\n",
      "epoch: 1 iter: 916 loss: 1.73841 training acc: 0.29650 testing acc: 0.2992\n",
      "epoch: 1 iter: 917 loss: 1.75617 training acc: 0.28644 testing acc: 0.2876\n",
      "epoch: 1 iter: 918 loss: 1.74856 training acc: 0.28670 testing acc: 0.2884\n",
      "epoch: 1 iter: 919 loss: 1.73677 training acc: 0.28104 testing acc: 0.2826\n",
      "epoch: 1 iter: 920 loss: 1.73871 training acc: 0.29318 testing acc: 0.293\n",
      "epoch: 1 iter: 921 loss: 1.73663 training acc: 0.28962 testing acc: 0.2912\n",
      "epoch: 1 iter: 922 loss: 1.73869 training acc: 0.28106 testing acc: 0.2814\n",
      "epoch: 1 iter: 923 loss: 1.76505 training acc: 0.26322 testing acc: 0.2642\n",
      "epoch: 1 iter: 924 loss: 1.74007 training acc: 0.29338 testing acc: 0.2916\n",
      "epoch: 1 iter: 925 loss: 1.76416 training acc: 0.26662 testing acc: 0.2684\n",
      "epoch: 1 iter: 926 loss: 1.76783 training acc: 0.26346 testing acc: 0.262\n",
      "epoch: 1 iter: 927 loss: 1.73781 training acc: 0.28940 testing acc: 0.2918\n",
      "epoch: 1 iter: 928 loss: 1.73453 training acc: 0.29308 testing acc: 0.298\n",
      "epoch: 1 iter: 929 loss: 1.74913 training acc: 0.29874 testing acc: 0.296\n",
      "epoch: 1 iter: 930 loss: 1.74139 training acc: 0.29964 testing acc: 0.2968\n",
      "epoch: 1 iter: 931 loss: 1.76848 training acc: 0.27088 testing acc: 0.2672\n",
      "epoch: 1 iter: 932 loss: 1.74021 training acc: 0.28934 testing acc: 0.291\n",
      "epoch: 1 iter: 933 loss: 1.74712 training acc: 0.29706 testing acc: 0.2956\n",
      "epoch: 1 iter: 934 loss: 1.73579 training acc: 0.28238 testing acc: 0.2856\n",
      "epoch: 1 iter: 935 loss: 1.73514 training acc: 0.29112 testing acc: 0.2924\n",
      "epoch: 1 iter: 936 loss: 1.74915 training acc: 0.29102 testing acc: 0.2868\n",
      "epoch: 1 iter: 937 loss: 1.73813 training acc: 0.29322 testing acc: 0.2906\n",
      "epoch: 1 iter: 938 loss: 1.73877 training acc: 0.29104 testing acc: 0.2906\n",
      "epoch: 1 iter: 939 loss: 1.72909 training acc: 0.29260 testing acc: 0.2932\n",
      "epoch: 1 iter: 940 loss: 1.73825 training acc: 0.29484 testing acc: 0.2946\n",
      "epoch: 1 iter: 941 loss: 1.72729 training acc: 0.28972 testing acc: 0.2908\n",
      "epoch: 1 iter: 942 loss: 1.72624 training acc: 0.28796 testing acc: 0.2904\n",
      "epoch: 1 iter: 943 loss: 1.73216 training acc: 0.29286 testing acc: 0.2918\n",
      "epoch: 1 iter: 944 loss: 1.72647 training acc: 0.28648 testing acc: 0.283\n",
      "epoch: 1 iter: 945 loss: 1.72821 training acc: 0.27820 testing acc: 0.2792\n",
      "epoch: 1 iter: 946 loss: 1.72337 training acc: 0.28276 testing acc: 0.2796\n",
      "epoch: 1 iter: 947 loss: 1.72816 training acc: 0.28692 testing acc: 0.2868\n",
      "epoch: 1 iter: 948 loss: 1.72644 training acc: 0.27820 testing acc: 0.2792\n",
      "epoch: 1 iter: 949 loss: 1.74832 training acc: 0.27626 testing acc: 0.2714\n",
      "epoch: 1 iter: 950 loss: 1.72971 training acc: 0.28318 testing acc: 0.2794\n",
      "epoch: 1 iter: 951 loss: 1.72511 training acc: 0.27800 testing acc: 0.2738\n",
      "epoch: 1 iter: 952 loss: 1.72380 training acc: 0.28656 testing acc: 0.283\n",
      "epoch: 1 iter: 953 loss: 1.72040 training acc: 0.28352 testing acc: 0.2794\n",
      "epoch: 1 iter: 954 loss: 1.73357 training acc: 0.26860 testing acc: 0.2698\n",
      "epoch: 1 iter: 955 loss: 1.72599 training acc: 0.28380 testing acc: 0.2794\n",
      "epoch: 1 iter: 956 loss: 1.73295 training acc: 0.27166 testing acc: 0.269\n",
      "epoch: 1 iter: 957 loss: 1.73092 training acc: 0.28816 testing acc: 0.2888\n",
      "epoch: 1 iter: 958 loss: 1.72883 training acc: 0.29024 testing acc: 0.2878\n",
      "epoch: 1 iter: 959 loss: 1.72414 training acc: 0.28864 testing acc: 0.2876\n",
      "epoch: 1 iter: 960 loss: 1.75134 training acc: 0.29320 testing acc: 0.2924\n",
      "epoch: 1 iter: 961 loss: 1.72039 training acc: 0.28818 testing acc: 0.287\n",
      "epoch: 1 iter: 962 loss: 1.72664 training acc: 0.28656 testing acc: 0.2882\n",
      "epoch: 1 iter: 963 loss: 1.73037 training acc: 0.29484 testing acc: 0.2928\n",
      "epoch: 1 iter: 964 loss: 1.72214 training acc: 0.27920 testing acc: 0.278\n",
      "epoch: 1 iter: 965 loss: 1.73526 training acc: 0.29030 testing acc: 0.2888\n",
      "epoch: 1 iter: 966 loss: 1.72094 training acc: 0.28942 testing acc: 0.2874\n",
      "epoch: 1 iter: 967 loss: 1.72365 training acc: 0.28400 testing acc: 0.283\n",
      "epoch: 1 iter: 968 loss: 1.73004 training acc: 0.29572 testing acc: 0.2944\n",
      "epoch: 1 iter: 969 loss: 1.72136 training acc: 0.28792 testing acc: 0.2826\n",
      "epoch: 1 iter: 970 loss: 1.72828 training acc: 0.28284 testing acc: 0.2802\n",
      "epoch: 1 iter: 971 loss: 1.72445 training acc: 0.27106 testing acc: 0.27\n",
      "epoch: 1 iter: 972 loss: 1.72247 training acc: 0.26846 testing acc: 0.2712\n",
      "epoch: 1 iter: 973 loss: 1.72220 training acc: 0.28094 testing acc: 0.2802\n",
      "epoch: 1 iter: 974 loss: 1.72039 training acc: 0.28240 testing acc: 0.2812\n",
      "epoch: 1 iter: 975 loss: 1.73024 training acc: 0.28764 testing acc: 0.2866\n",
      "epoch: 1 iter: 976 loss: 1.71497 training acc: 0.27478 testing acc: 0.2776\n",
      "epoch: 1 iter: 977 loss: 1.71484 training acc: 0.28050 testing acc: 0.2806\n",
      "epoch: 1 iter: 978 loss: 1.73598 training acc: 0.28086 testing acc: 0.2772\n",
      "epoch: 1 iter: 979 loss: 1.71934 training acc: 0.28186 testing acc: 0.2794\n",
      "epoch: 1 iter: 980 loss: 1.71525 training acc: 0.27650 testing acc: 0.278\n",
      "epoch: 1 iter: 981 loss: 1.71445 training acc: 0.28246 testing acc: 0.2868\n",
      "epoch: 1 iter: 982 loss: 1.71823 training acc: 0.28708 testing acc: 0.285\n",
      "epoch: 1 iter: 983 loss: 1.71264 training acc: 0.27532 testing acc: 0.2796\n",
      "epoch: 1 iter: 984 loss: 1.71644 training acc: 0.28416 testing acc: 0.2866\n",
      "epoch: 1 iter: 985 loss: 1.71832 training acc: 0.28914 testing acc: 0.2886\n",
      "epoch: 1 iter: 986 loss: 1.71331 training acc: 0.29104 testing acc: 0.2916\n",
      "epoch: 1 iter: 987 loss: 1.71067 training acc: 0.28434 testing acc: 0.2854\n",
      "epoch: 1 iter: 988 loss: 1.70843 training acc: 0.29814 testing acc: 0.3014\n",
      "epoch: 1 iter: 989 loss: 1.71017 training acc: 0.29410 testing acc: 0.2938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 990 loss: 1.70590 training acc: 0.28216 testing acc: 0.2874\n",
      "epoch: 1 iter: 991 loss: 1.70939 training acc: 0.29216 testing acc: 0.2928\n",
      "epoch: 1 iter: 992 loss: 1.71427 training acc: 0.29188 testing acc: 0.29\n",
      "epoch: 1 iter: 993 loss: 1.71640 training acc: 0.29780 testing acc: 0.296\n",
      "epoch: 1 iter: 994 loss: 1.71438 training acc: 0.29726 testing acc: 0.2956\n",
      "epoch: 1 iter: 995 loss: 1.73888 training acc: 0.27648 testing acc: 0.2764\n",
      "epoch: 1 iter: 996 loss: 1.71596 training acc: 0.29692 testing acc: 0.2956\n",
      "epoch: 1 iter: 997 loss: 1.70823 training acc: 0.30148 testing acc: 0.2996\n",
      "epoch: 1 iter: 998 loss: 1.71539 training acc: 0.29932 testing acc: 0.3012\n",
      "epoch: 1 iter: 999 loss: 1.70853 training acc: 0.29972 testing acc: 0.297\n",
      "epoch: 2 iter:   0 loss: 1.72729 training acc: 0.28738 testing acc: 0.2914\n",
      "epoch: 2 iter:   1 loss: 1.71087 training acc: 0.30042 testing acc: 0.3048\n",
      "epoch: 2 iter:   2 loss: 1.70742 training acc: 0.30194 testing acc: 0.303\n",
      "epoch: 2 iter:   3 loss: 1.71372 training acc: 0.29580 testing acc: 0.298\n",
      "epoch: 2 iter:   4 loss: 1.71007 training acc: 0.29858 testing acc: 0.2982\n",
      "epoch: 2 iter:   5 loss: 1.71320 training acc: 0.30900 testing acc: 0.3076\n",
      "epoch: 2 iter:   6 loss: 1.70619 training acc: 0.30426 testing acc: 0.3072\n",
      "epoch: 2 iter:   7 loss: 1.70350 training acc: 0.30452 testing acc: 0.3052\n",
      "epoch: 2 iter:   8 loss: 1.70226 training acc: 0.30592 testing acc: 0.3064\n",
      "epoch: 2 iter:   9 loss: 1.70702 training acc: 0.29990 testing acc: 0.3024\n",
      "epoch: 2 iter:  10 loss: 1.70257 training acc: 0.29890 testing acc: 0.295\n",
      "epoch: 2 iter:  11 loss: 1.70407 training acc: 0.30392 testing acc: 0.3032\n",
      "epoch: 2 iter:  12 loss: 1.70230 training acc: 0.29890 testing acc: 0.3002\n",
      "epoch: 2 iter:  13 loss: 1.69986 training acc: 0.30376 testing acc: 0.3036\n",
      "epoch: 2 iter:  14 loss: 1.69863 training acc: 0.29548 testing acc: 0.2954\n",
      "epoch: 2 iter:  15 loss: 1.71452 training acc: 0.29736 testing acc: 0.2954\n",
      "epoch: 2 iter:  16 loss: 1.70572 training acc: 0.28146 testing acc: 0.2848\n",
      "epoch: 2 iter:  17 loss: 1.75211 training acc: 0.26686 testing acc: 0.271\n",
      "epoch: 2 iter:  18 loss: 1.71166 training acc: 0.29508 testing acc: 0.2956\n",
      "epoch: 2 iter:  19 loss: 1.70415 training acc: 0.29900 testing acc: 0.2944\n",
      "epoch: 2 iter:  20 loss: 1.71672 training acc: 0.29002 testing acc: 0.2896\n",
      "epoch: 2 iter:  21 loss: 1.70576 training acc: 0.29514 testing acc: 0.2924\n",
      "epoch: 2 iter:  22 loss: 1.70179 training acc: 0.29382 testing acc: 0.2902\n",
      "epoch: 2 iter:  23 loss: 1.70040 training acc: 0.29520 testing acc: 0.2926\n",
      "epoch: 2 iter:  24 loss: 1.70316 training acc: 0.29478 testing acc: 0.2902\n",
      "epoch: 2 iter:  25 loss: 1.71453 training acc: 0.29112 testing acc: 0.2864\n",
      "epoch: 2 iter:  26 loss: 1.70323 training acc: 0.29590 testing acc: 0.2912\n",
      "epoch: 2 iter:  27 loss: 1.69890 training acc: 0.29386 testing acc: 0.2892\n",
      "epoch: 2 iter:  28 loss: 1.69369 training acc: 0.29614 testing acc: 0.2956\n",
      "epoch: 2 iter:  29 loss: 1.69666 training acc: 0.29180 testing acc: 0.2908\n",
      "epoch: 2 iter:  30 loss: 1.69946 training acc: 0.28950 testing acc: 0.2872\n",
      "epoch: 2 iter:  31 loss: 1.71398 training acc: 0.27326 testing acc: 0.273\n",
      "epoch: 2 iter:  32 loss: 1.69558 training acc: 0.28770 testing acc: 0.2852\n",
      "epoch: 2 iter:  33 loss: 1.69483 training acc: 0.30180 testing acc: 0.3036\n",
      "epoch: 2 iter:  34 loss: 1.69526 training acc: 0.29640 testing acc: 0.2908\n",
      "epoch: 2 iter:  35 loss: 1.69092 training acc: 0.30398 testing acc: 0.3036\n",
      "epoch: 2 iter:  36 loss: 1.69358 training acc: 0.29748 testing acc: 0.2994\n",
      "epoch: 2 iter:  37 loss: 1.69965 training acc: 0.29744 testing acc: 0.298\n",
      "epoch: 2 iter:  38 loss: 1.69081 training acc: 0.29848 testing acc: 0.2952\n",
      "epoch: 2 iter:  39 loss: 1.70196 training acc: 0.30036 testing acc: 0.3004\n",
      "epoch: 2 iter:  40 loss: 1.69501 training acc: 0.29624 testing acc: 0.2966\n",
      "epoch: 2 iter:  41 loss: 1.68945 training acc: 0.29912 testing acc: 0.3022\n",
      "epoch: 2 iter:  42 loss: 1.69661 training acc: 0.29860 testing acc: 0.2972\n",
      "epoch: 2 iter:  43 loss: 1.69428 training acc: 0.29246 testing acc: 0.2888\n",
      "epoch: 2 iter:  44 loss: 1.69250 training acc: 0.29290 testing acc: 0.2904\n",
      "epoch: 2 iter:  45 loss: 1.69497 training acc: 0.29128 testing acc: 0.2936\n",
      "epoch: 2 iter:  46 loss: 1.69058 training acc: 0.29382 testing acc: 0.293\n",
      "epoch: 2 iter:  47 loss: 1.69729 training acc: 0.28580 testing acc: 0.286\n",
      "epoch: 2 iter:  48 loss: 1.68643 training acc: 0.30704 testing acc: 0.3082\n",
      "epoch: 2 iter:  49 loss: 1.69911 training acc: 0.28936 testing acc: 0.2914\n",
      "epoch: 2 iter:  50 loss: 1.69388 training acc: 0.29234 testing acc: 0.2908\n",
      "epoch: 2 iter:  51 loss: 1.69415 training acc: 0.29862 testing acc: 0.2974\n",
      "epoch: 2 iter:  52 loss: 1.69488 training acc: 0.29950 testing acc: 0.2974\n",
      "epoch: 2 iter:  53 loss: 1.69386 training acc: 0.29740 testing acc: 0.2966\n",
      "epoch: 2 iter:  54 loss: 1.69674 training acc: 0.30838 testing acc: 0.3072\n",
      "epoch: 2 iter:  55 loss: 1.69255 training acc: 0.29856 testing acc: 0.3026\n",
      "epoch: 2 iter:  56 loss: 1.69226 training acc: 0.30010 testing acc: 0.3038\n",
      "epoch: 2 iter:  57 loss: 1.69109 training acc: 0.29804 testing acc: 0.2954\n",
      "epoch: 2 iter:  58 loss: 1.70372 training acc: 0.28822 testing acc: 0.2836\n",
      "epoch: 2 iter:  59 loss: 1.72262 training acc: 0.26276 testing acc: 0.2586\n",
      "epoch: 2 iter:  60 loss: 1.68583 training acc: 0.29214 testing acc: 0.2912\n",
      "epoch: 2 iter:  61 loss: 1.68670 training acc: 0.29182 testing acc: 0.2894\n",
      "epoch: 2 iter:  62 loss: 1.70115 training acc: 0.29446 testing acc: 0.2938\n",
      "epoch: 2 iter:  63 loss: 1.69385 training acc: 0.28380 testing acc: 0.2854\n",
      "epoch: 2 iter:  64 loss: 1.68721 training acc: 0.29464 testing acc: 0.3008\n",
      "epoch: 2 iter:  65 loss: 1.68475 training acc: 0.29104 testing acc: 0.2924\n",
      "epoch: 2 iter:  66 loss: 1.68841 training acc: 0.29012 testing acc: 0.2898\n",
      "epoch: 2 iter:  67 loss: 1.68510 training acc: 0.30020 testing acc: 0.3008\n",
      "epoch: 2 iter:  68 loss: 1.68672 training acc: 0.29548 testing acc: 0.3012\n",
      "epoch: 2 iter:  69 loss: 1.69163 training acc: 0.29120 testing acc: 0.2906\n",
      "epoch: 2 iter:  70 loss: 1.68939 training acc: 0.30352 testing acc: 0.3002\n",
      "epoch: 2 iter:  71 loss: 1.68859 training acc: 0.29962 testing acc: 0.2984\n",
      "epoch: 2 iter:  72 loss: 1.68431 training acc: 0.29506 testing acc: 0.2952\n",
      "epoch: 2 iter:  73 loss: 1.68077 training acc: 0.28674 testing acc: 0.286\n",
      "epoch: 2 iter:  74 loss: 1.68985 training acc: 0.28048 testing acc: 0.2816\n",
      "epoch: 2 iter:  75 loss: 1.67969 training acc: 0.29614 testing acc: 0.297\n",
      "epoch: 2 iter:  76 loss: 1.68228 training acc: 0.31114 testing acc: 0.3084\n",
      "epoch: 2 iter:  77 loss: 1.71084 training acc: 0.28366 testing acc: 0.2868\n",
      "epoch: 2 iter:  78 loss: 1.68228 training acc: 0.29924 testing acc: 0.3012\n",
      "epoch: 2 iter:  79 loss: 1.67877 training acc: 0.29934 testing acc: 0.3022\n",
      "epoch: 2 iter:  80 loss: 1.67669 training acc: 0.29732 testing acc: 0.2994\n",
      "epoch: 2 iter:  81 loss: 1.68996 training acc: 0.28602 testing acc: 0.2856\n",
      "epoch: 2 iter:  82 loss: 1.68511 training acc: 0.29540 testing acc: 0.299\n",
      "epoch: 2 iter:  83 loss: 1.67678 training acc: 0.30074 testing acc: 0.3004\n",
      "epoch: 2 iter:  84 loss: 1.67718 training acc: 0.30552 testing acc: 0.3048\n",
      "epoch: 2 iter:  85 loss: 1.68737 training acc: 0.29956 testing acc: 0.2994\n",
      "epoch: 2 iter:  86 loss: 1.67870 training acc: 0.28672 testing acc: 0.2906\n",
      "epoch: 2 iter:  87 loss: 1.67483 training acc: 0.30038 testing acc: 0.298\n",
      "epoch: 2 iter:  88 loss: 1.67989 training acc: 0.28866 testing acc: 0.2886\n",
      "epoch: 2 iter:  89 loss: 1.67761 training acc: 0.30076 testing acc: 0.2996\n",
      "epoch: 2 iter:  90 loss: 1.67514 training acc: 0.29930 testing acc: 0.2994\n",
      "epoch: 2 iter:  91 loss: 1.67594 training acc: 0.30134 testing acc: 0.302\n",
      "epoch: 2 iter:  92 loss: 1.68592 training acc: 0.29204 testing acc: 0.297\n",
      "epoch: 2 iter:  93 loss: 1.68199 training acc: 0.29736 testing acc: 0.2952\n",
      "epoch: 2 iter:  94 loss: 1.67368 training acc: 0.30472 testing acc: 0.3042\n",
      "epoch: 2 iter:  95 loss: 1.67252 training acc: 0.30996 testing acc: 0.3086\n",
      "epoch: 2 iter:  96 loss: 1.67427 training acc: 0.29872 testing acc: 0.3004\n",
      "epoch: 2 iter:  97 loss: 1.68906 training acc: 0.29934 testing acc: 0.2988\n",
      "epoch: 2 iter:  98 loss: 1.68172 training acc: 0.28986 testing acc: 0.2956\n",
      "epoch: 2 iter:  99 loss: 1.67711 training acc: 0.29572 testing acc: 0.3014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 iter: 100 loss: 1.67408 training acc: 0.30560 testing acc: 0.307\n",
      "epoch: 2 iter: 101 loss: 1.67803 training acc: 0.29764 testing acc: 0.2982\n",
      "epoch: 2 iter: 102 loss: 1.67340 training acc: 0.30254 testing acc: 0.303\n",
      "epoch: 2 iter: 103 loss: 1.66972 training acc: 0.29520 testing acc: 0.2994\n",
      "epoch: 2 iter: 104 loss: 1.68886 training acc: 0.28288 testing acc: 0.2826\n",
      "epoch: 2 iter: 105 loss: 1.67987 training acc: 0.28416 testing acc: 0.2848\n",
      "epoch: 2 iter: 106 loss: 1.67338 training acc: 0.30154 testing acc: 0.3034\n",
      "epoch: 2 iter: 107 loss: 1.67167 training acc: 0.30314 testing acc: 0.3052\n",
      "epoch: 2 iter: 108 loss: 1.68377 training acc: 0.29450 testing acc: 0.2928\n",
      "epoch: 2 iter: 109 loss: 1.67189 training acc: 0.30660 testing acc: 0.3024\n",
      "epoch: 2 iter: 110 loss: 1.66954 training acc: 0.30198 testing acc: 0.3004\n",
      "epoch: 2 iter: 111 loss: 1.67248 training acc: 0.30438 testing acc: 0.3058\n",
      "epoch: 2 iter: 112 loss: 1.68786 training acc: 0.30416 testing acc: 0.3086\n",
      "epoch: 2 iter: 113 loss: 1.69185 training acc: 0.29276 testing acc: 0.293\n",
      "epoch: 2 iter: 114 loss: 1.67168 training acc: 0.30312 testing acc: 0.3056\n",
      "epoch: 2 iter: 115 loss: 1.67272 training acc: 0.30816 testing acc: 0.3108\n",
      "epoch: 2 iter: 116 loss: 1.67069 training acc: 0.30122 testing acc: 0.2992\n",
      "epoch: 2 iter: 117 loss: 1.68486 training acc: 0.29854 testing acc: 0.2976\n",
      "epoch: 2 iter: 118 loss: 1.68352 training acc: 0.29808 testing acc: 0.2968\n",
      "epoch: 2 iter: 119 loss: 1.66601 training acc: 0.30152 testing acc: 0.3014\n",
      "epoch: 2 iter: 120 loss: 1.68405 training acc: 0.29582 testing acc: 0.2952\n",
      "epoch: 2 iter: 121 loss: 1.66846 training acc: 0.29730 testing acc: 0.2944\n",
      "epoch: 2 iter: 122 loss: 1.66633 training acc: 0.29198 testing acc: 0.2882\n",
      "epoch: 2 iter: 123 loss: 1.66679 training acc: 0.29668 testing acc: 0.2946\n",
      "epoch: 2 iter: 124 loss: 1.66531 training acc: 0.29202 testing acc: 0.2906\n",
      "epoch: 2 iter: 125 loss: 1.66545 training acc: 0.30606 testing acc: 0.2984\n",
      "epoch: 2 iter: 126 loss: 1.66608 training acc: 0.29770 testing acc: 0.296\n",
      "epoch: 2 iter: 127 loss: 1.66617 training acc: 0.29282 testing acc: 0.2918\n",
      "epoch: 2 iter: 128 loss: 1.69586 training acc: 0.29564 testing acc: 0.2924\n",
      "epoch: 2 iter: 129 loss: 1.68695 training acc: 0.30360 testing acc: 0.3016\n",
      "epoch: 2 iter: 130 loss: 1.66412 training acc: 0.30262 testing acc: 0.3008\n",
      "epoch: 2 iter: 131 loss: 1.66900 training acc: 0.29680 testing acc: 0.2928\n",
      "epoch: 2 iter: 132 loss: 1.67381 training acc: 0.29138 testing acc: 0.291\n",
      "epoch: 2 iter: 133 loss: 1.73897 training acc: 0.27820 testing acc: 0.2766\n",
      "epoch: 2 iter: 134 loss: 1.67322 training acc: 0.30372 testing acc: 0.3006\n",
      "epoch: 2 iter: 135 loss: 1.66524 training acc: 0.30784 testing acc: 0.3064\n",
      "epoch: 2 iter: 136 loss: 1.66950 training acc: 0.31018 testing acc: 0.3134\n",
      "epoch: 2 iter: 137 loss: 1.66439 training acc: 0.31000 testing acc: 0.309\n",
      "epoch: 2 iter: 138 loss: 1.66195 training acc: 0.30650 testing acc: 0.3068\n",
      "epoch: 2 iter: 139 loss: 1.66121 training acc: 0.30616 testing acc: 0.3062\n",
      "epoch: 2 iter: 140 loss: 1.67945 training acc: 0.29644 testing acc: 0.2978\n",
      "epoch: 2 iter: 141 loss: 1.66524 training acc: 0.31286 testing acc: 0.3136\n",
      "epoch: 2 iter: 142 loss: 1.66361 training acc: 0.31672 testing acc: 0.3186\n",
      "epoch: 2 iter: 143 loss: 1.66891 training acc: 0.30880 testing acc: 0.3098\n",
      "epoch: 2 iter: 144 loss: 1.66225 training acc: 0.30168 testing acc: 0.305\n",
      "epoch: 2 iter: 145 loss: 1.67750 training acc: 0.30460 testing acc: 0.304\n",
      "epoch: 2 iter: 146 loss: 1.66358 training acc: 0.31708 testing acc: 0.3162\n",
      "epoch: 2 iter: 147 loss: 1.66279 training acc: 0.31586 testing acc: 0.3176\n",
      "epoch: 2 iter: 148 loss: 1.67497 training acc: 0.30264 testing acc: 0.3022\n",
      "epoch: 2 iter: 149 loss: 1.66030 training acc: 0.30792 testing acc: 0.307\n",
      "epoch: 2 iter: 150 loss: 1.66930 training acc: 0.29746 testing acc: 0.2994\n",
      "epoch: 2 iter: 151 loss: 1.68850 training acc: 0.28714 testing acc: 0.2876\n",
      "epoch: 2 iter: 152 loss: 1.66000 training acc: 0.30378 testing acc: 0.3046\n",
      "epoch: 2 iter: 153 loss: 1.66111 training acc: 0.30598 testing acc: 0.3106\n",
      "epoch: 2 iter: 154 loss: 1.65742 training acc: 0.30274 testing acc: 0.304\n",
      "epoch: 2 iter: 155 loss: 1.65483 training acc: 0.31094 testing acc: 0.3062\n",
      "epoch: 2 iter: 156 loss: 1.66520 training acc: 0.30312 testing acc: 0.301\n",
      "epoch: 2 iter: 157 loss: 1.69942 training acc: 0.29914 testing acc: 0.3024\n",
      "epoch: 2 iter: 158 loss: 1.67672 training acc: 0.30872 testing acc: 0.3088\n",
      "epoch: 2 iter: 159 loss: 1.65495 training acc: 0.30404 testing acc: 0.3052\n",
      "epoch: 2 iter: 160 loss: 1.65591 training acc: 0.30100 testing acc: 0.3022\n",
      "epoch: 2 iter: 161 loss: 1.66565 training acc: 0.30888 testing acc: 0.303\n",
      "epoch: 2 iter: 162 loss: 1.67734 training acc: 0.30722 testing acc: 0.3022\n",
      "epoch: 2 iter: 163 loss: 1.65832 training acc: 0.29666 testing acc: 0.2936\n",
      "epoch: 2 iter: 164 loss: 1.66146 training acc: 0.29514 testing acc: 0.2924\n",
      "epoch: 2 iter: 165 loss: 1.65550 training acc: 0.29490 testing acc: 0.292\n",
      "epoch: 2 iter: 166 loss: 1.65285 training acc: 0.29700 testing acc: 0.2936\n",
      "epoch: 2 iter: 167 loss: 1.67135 training acc: 0.29656 testing acc: 0.2958\n",
      "epoch: 2 iter: 168 loss: 1.65390 training acc: 0.29810 testing acc: 0.2992\n",
      "epoch: 2 iter: 169 loss: 1.65649 training acc: 0.30156 testing acc: 0.2978\n",
      "epoch: 2 iter: 170 loss: 1.65688 training acc: 0.30708 testing acc: 0.3096\n",
      "epoch: 2 iter: 171 loss: 1.66413 training acc: 0.29692 testing acc: 0.2988\n",
      "epoch: 2 iter: 172 loss: 1.68898 training acc: 0.30788 testing acc: 0.3048\n",
      "epoch: 2 iter: 173 loss: 1.66872 training acc: 0.30528 testing acc: 0.3018\n",
      "epoch: 2 iter: 174 loss: 1.65679 training acc: 0.30244 testing acc: 0.2998\n",
      "epoch: 2 iter: 175 loss: 1.65528 training acc: 0.30880 testing acc: 0.3058\n",
      "epoch: 2 iter: 176 loss: 1.65832 training acc: 0.30334 testing acc: 0.3074\n",
      "epoch: 2 iter: 177 loss: 1.65354 training acc: 0.29956 testing acc: 0.296\n",
      "epoch: 2 iter: 178 loss: 1.66150 training acc: 0.29208 testing acc: 0.2892\n",
      "epoch: 2 iter: 179 loss: 1.66011 training acc: 0.29924 testing acc: 0.2964\n",
      "epoch: 2 iter: 180 loss: 1.65656 training acc: 0.30092 testing acc: 0.3\n",
      "epoch: 2 iter: 181 loss: 1.65521 training acc: 0.30576 testing acc: 0.3062\n",
      "epoch: 2 iter: 182 loss: 1.64758 training acc: 0.30140 testing acc: 0.3002\n",
      "epoch: 2 iter: 183 loss: 1.65189 training acc: 0.30596 testing acc: 0.3102\n",
      "epoch: 2 iter: 184 loss: 1.65100 training acc: 0.30126 testing acc: 0.303\n",
      "epoch: 2 iter: 185 loss: 1.65103 training acc: 0.30258 testing acc: 0.3026\n",
      "epoch: 2 iter: 186 loss: 1.66330 training acc: 0.30174 testing acc: 0.303\n",
      "epoch: 2 iter: 187 loss: 1.66241 training acc: 0.29938 testing acc: 0.2992\n",
      "epoch: 2 iter: 188 loss: 1.65657 training acc: 0.29854 testing acc: 0.3004\n",
      "epoch: 2 iter: 189 loss: 1.66499 training acc: 0.29164 testing acc: 0.292\n",
      "epoch: 2 iter: 190 loss: 1.65458 training acc: 0.30978 testing acc: 0.3096\n",
      "epoch: 2 iter: 191 loss: 1.66166 training acc: 0.29418 testing acc: 0.296\n",
      "epoch: 2 iter: 192 loss: 1.64854 training acc: 0.30830 testing acc: 0.3078\n",
      "epoch: 2 iter: 193 loss: 1.64733 training acc: 0.31192 testing acc: 0.3132\n",
      "epoch: 2 iter: 194 loss: 1.64409 training acc: 0.31378 testing acc: 0.3156\n",
      "epoch: 2 iter: 195 loss: 1.65302 training acc: 0.30816 testing acc: 0.3072\n",
      "epoch: 2 iter: 196 loss: 1.64509 training acc: 0.30812 testing acc: 0.3092\n",
      "epoch: 2 iter: 197 loss: 1.64528 training acc: 0.30666 testing acc: 0.3114\n",
      "epoch: 2 iter: 198 loss: 1.65766 training acc: 0.30838 testing acc: 0.307\n",
      "epoch: 2 iter: 199 loss: 1.64641 training acc: 0.30296 testing acc: 0.304\n",
      "epoch: 2 iter: 200 loss: 1.64915 training acc: 0.30098 testing acc: 0.3046\n",
      "epoch: 2 iter: 201 loss: 1.65245 training acc: 0.30710 testing acc: 0.3104\n",
      "epoch: 2 iter: 202 loss: 1.64075 training acc: 0.30678 testing acc: 0.31\n",
      "epoch: 2 iter: 203 loss: 1.64902 training acc: 0.30152 testing acc: 0.304\n",
      "epoch: 2 iter: 204 loss: 1.64099 training acc: 0.30388 testing acc: 0.3064\n",
      "epoch: 2 iter: 205 loss: 1.64545 training acc: 0.30826 testing acc: 0.3094\n",
      "epoch: 2 iter: 206 loss: 1.65488 training acc: 0.29636 testing acc: 0.2934\n",
      "epoch: 2 iter: 207 loss: 1.64867 training acc: 0.30790 testing acc: 0.3088\n",
      "epoch: 2 iter: 208 loss: 1.64276 training acc: 0.30720 testing acc: 0.3098\n",
      "epoch: 2 iter: 209 loss: 1.64149 training acc: 0.31502 testing acc: 0.3122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 iter: 210 loss: 1.63980 training acc: 0.32014 testing acc: 0.3202\n",
      "epoch: 2 iter: 211 loss: 1.66838 training acc: 0.30268 testing acc: 0.3012\n",
      "epoch: 2 iter: 212 loss: 1.66086 training acc: 0.30838 testing acc: 0.3134\n",
      "epoch: 2 iter: 213 loss: 1.64331 training acc: 0.31420 testing acc: 0.3162\n",
      "epoch: 2 iter: 214 loss: 1.64407 training acc: 0.31662 testing acc: 0.3144\n",
      "epoch: 2 iter: 215 loss: 1.64869 training acc: 0.32046 testing acc: 0.3208\n",
      "epoch: 2 iter: 216 loss: 1.66879 training acc: 0.31180 testing acc: 0.3124\n",
      "epoch: 2 iter: 217 loss: 1.65936 training acc: 0.31296 testing acc: 0.3174\n",
      "epoch: 2 iter: 218 loss: 1.64444 training acc: 0.31284 testing acc: 0.3166\n",
      "epoch: 2 iter: 219 loss: 1.64977 training acc: 0.30760 testing acc: 0.31\n",
      "epoch: 2 iter: 220 loss: 1.64443 training acc: 0.30344 testing acc: 0.3026\n",
      "epoch: 2 iter: 221 loss: 1.65893 training acc: 0.29648 testing acc: 0.2966\n",
      "epoch: 2 iter: 222 loss: 1.65015 training acc: 0.31204 testing acc: 0.3076\n",
      "epoch: 2 iter: 223 loss: 1.65182 training acc: 0.31238 testing acc: 0.3092\n",
      "epoch: 2 iter: 224 loss: 1.64242 training acc: 0.31846 testing acc: 0.3146\n",
      "epoch: 2 iter: 225 loss: 1.64670 training acc: 0.30964 testing acc: 0.303\n",
      "epoch: 2 iter: 226 loss: 1.70707 training acc: 0.29048 testing acc: 0.2906\n",
      "epoch: 2 iter: 227 loss: 1.65225 training acc: 0.31566 testing acc: 0.3126\n",
      "epoch: 2 iter: 228 loss: 1.63495 training acc: 0.31702 testing acc: 0.3144\n",
      "epoch: 2 iter: 229 loss: 1.63391 training acc: 0.31776 testing acc: 0.3162\n",
      "epoch: 2 iter: 230 loss: 1.65214 training acc: 0.31010 testing acc: 0.3112\n",
      "epoch: 2 iter: 231 loss: 1.63973 training acc: 0.31880 testing acc: 0.3184\n",
      "epoch: 2 iter: 232 loss: 1.64089 training acc: 0.30990 testing acc: 0.3112\n",
      "epoch: 2 iter: 233 loss: 1.63519 training acc: 0.31218 testing acc: 0.3116\n",
      "epoch: 2 iter: 234 loss: 1.63355 training acc: 0.31732 testing acc: 0.321\n",
      "epoch: 2 iter: 235 loss: 1.63380 training acc: 0.30870 testing acc: 0.3064\n",
      "epoch: 2 iter: 236 loss: 1.63414 training acc: 0.30822 testing acc: 0.3076\n",
      "epoch: 2 iter: 237 loss: 1.63325 training acc: 0.31534 testing acc: 0.3146\n",
      "epoch: 2 iter: 238 loss: 1.63790 training acc: 0.31832 testing acc: 0.3174\n",
      "epoch: 2 iter: 239 loss: 1.64175 training acc: 0.30816 testing acc: 0.3064\n",
      "epoch: 2 iter: 240 loss: 1.64101 training acc: 0.31236 testing acc: 0.3104\n",
      "epoch: 2 iter: 241 loss: 1.63451 training acc: 0.31004 testing acc: 0.3088\n",
      "epoch: 2 iter: 242 loss: 1.63177 training acc: 0.31986 testing acc: 0.3204\n",
      "epoch: 2 iter: 243 loss: 1.63398 training acc: 0.31082 testing acc: 0.3086\n",
      "epoch: 2 iter: 244 loss: 1.64507 training acc: 0.30710 testing acc: 0.312\n",
      "epoch: 2 iter: 245 loss: 1.63294 training acc: 0.31240 testing acc: 0.3106\n",
      "epoch: 2 iter: 246 loss: 1.63587 training acc: 0.30744 testing acc: 0.3078\n",
      "epoch: 2 iter: 247 loss: 1.63711 training acc: 0.30708 testing acc: 0.3138\n",
      "epoch: 2 iter: 248 loss: 1.63531 training acc: 0.32116 testing acc: 0.3232\n",
      "epoch: 2 iter: 249 loss: 1.64540 training acc: 0.31462 testing acc: 0.3134\n",
      "epoch: 2 iter: 250 loss: 1.65705 training acc: 0.31138 testing acc: 0.3096\n",
      "epoch: 2 iter: 251 loss: 1.63248 training acc: 0.31630 testing acc: 0.3146\n",
      "epoch: 2 iter: 252 loss: 1.64250 training acc: 0.31536 testing acc: 0.3142\n",
      "epoch: 2 iter: 253 loss: 1.63228 training acc: 0.31796 testing acc: 0.321\n",
      "epoch: 2 iter: 254 loss: 1.63061 training acc: 0.31448 testing acc: 0.3178\n",
      "epoch: 2 iter: 255 loss: 1.64481 training acc: 0.30552 testing acc: 0.312\n",
      "epoch: 2 iter: 256 loss: 1.63138 training acc: 0.31606 testing acc: 0.3212\n",
      "epoch: 2 iter: 257 loss: 1.63468 training acc: 0.31474 testing acc: 0.3204\n",
      "epoch: 2 iter: 258 loss: 1.63745 training acc: 0.31756 testing acc: 0.3194\n",
      "epoch: 2 iter: 259 loss: 1.62989 training acc: 0.32088 testing acc: 0.3176\n",
      "epoch: 2 iter: 260 loss: 1.64668 training acc: 0.30790 testing acc: 0.3006\n",
      "epoch: 2 iter: 261 loss: 1.65572 training acc: 0.30876 testing acc: 0.3084\n",
      "epoch: 2 iter: 262 loss: 1.62785 training acc: 0.31928 testing acc: 0.3182\n",
      "epoch: 2 iter: 263 loss: 1.64071 training acc: 0.30388 testing acc: 0.3062\n",
      "epoch: 2 iter: 264 loss: 1.63270 training acc: 0.30530 testing acc: 0.3068\n",
      "epoch: 2 iter: 265 loss: 1.63253 training acc: 0.30470 testing acc: 0.305\n",
      "epoch: 2 iter: 266 loss: 1.62442 training acc: 0.31252 testing acc: 0.3154\n",
      "epoch: 2 iter: 267 loss: 1.62681 training acc: 0.30420 testing acc: 0.307\n",
      "epoch: 2 iter: 268 loss: 1.63668 training acc: 0.30168 testing acc: 0.3046\n",
      "epoch: 2 iter: 269 loss: 1.62796 training acc: 0.32132 testing acc: 0.3214\n",
      "epoch: 2 iter: 270 loss: 1.62648 training acc: 0.31540 testing acc: 0.3166\n",
      "epoch: 2 iter: 271 loss: 1.63201 training acc: 0.31410 testing acc: 0.3138\n",
      "epoch: 2 iter: 272 loss: 1.62406 training acc: 0.31340 testing acc: 0.3126\n",
      "epoch: 2 iter: 273 loss: 1.62314 training acc: 0.32228 testing acc: 0.3224\n",
      "epoch: 2 iter: 274 loss: 1.62588 training acc: 0.31900 testing acc: 0.3218\n",
      "epoch: 2 iter: 275 loss: 1.62486 training acc: 0.32012 testing acc: 0.3224\n",
      "epoch: 2 iter: 276 loss: 1.62179 training acc: 0.31970 testing acc: 0.323\n",
      "epoch: 2 iter: 277 loss: 1.62028 training acc: 0.31158 testing acc: 0.3122\n",
      "epoch: 2 iter: 278 loss: 1.62575 training acc: 0.30714 testing acc: 0.3102\n",
      "epoch: 2 iter: 279 loss: 1.62499 training acc: 0.31858 testing acc: 0.3198\n",
      "epoch: 2 iter: 280 loss: 1.62245 training acc: 0.32374 testing acc: 0.3266\n",
      "epoch: 2 iter: 281 loss: 1.62440 training acc: 0.30750 testing acc: 0.309\n",
      "epoch: 2 iter: 282 loss: 1.61935 training acc: 0.31016 testing acc: 0.3102\n",
      "epoch: 2 iter: 283 loss: 1.61743 training acc: 0.31134 testing acc: 0.3134\n",
      "epoch: 2 iter: 284 loss: 1.62068 training acc: 0.30686 testing acc: 0.3104\n",
      "epoch: 2 iter: 285 loss: 1.62831 training acc: 0.30056 testing acc: 0.2976\n",
      "epoch: 2 iter: 286 loss: 1.62669 training acc: 0.30572 testing acc: 0.3042\n",
      "epoch: 2 iter: 287 loss: 1.62397 training acc: 0.30650 testing acc: 0.3064\n",
      "epoch: 2 iter: 288 loss: 1.61887 training acc: 0.30890 testing acc: 0.3094\n",
      "epoch: 2 iter: 289 loss: 1.61589 training acc: 0.30870 testing acc: 0.3076\n",
      "epoch: 2 iter: 290 loss: 1.63716 training acc: 0.29716 testing acc: 0.2978\n",
      "epoch: 2 iter: 291 loss: 1.61700 training acc: 0.30228 testing acc: 0.3022\n",
      "epoch: 2 iter: 292 loss: 1.61903 training acc: 0.30896 testing acc: 0.3092\n",
      "epoch: 2 iter: 293 loss: 1.61504 training acc: 0.31214 testing acc: 0.3112\n",
      "epoch: 2 iter: 294 loss: 1.63884 training acc: 0.29542 testing acc: 0.3\n",
      "epoch: 2 iter: 295 loss: 1.64957 training acc: 0.29358 testing acc: 0.2928\n",
      "epoch: 2 iter: 296 loss: 1.62370 training acc: 0.30750 testing acc: 0.3066\n",
      "epoch: 2 iter: 297 loss: 1.62484 training acc: 0.32126 testing acc: 0.3204\n",
      "epoch: 2 iter: 298 loss: 1.61628 training acc: 0.32044 testing acc: 0.3198\n",
      "epoch: 2 iter: 299 loss: 1.61683 training acc: 0.31766 testing acc: 0.3206\n",
      "epoch: 2 iter: 300 loss: 1.61728 training acc: 0.31108 testing acc: 0.3122\n",
      "epoch: 2 iter: 301 loss: 1.61967 training acc: 0.30928 testing acc: 0.3102\n",
      "epoch: 2 iter: 302 loss: 1.62014 training acc: 0.30126 testing acc: 0.2986\n",
      "epoch: 2 iter: 303 loss: 1.62063 training acc: 0.30432 testing acc: 0.3\n",
      "epoch: 2 iter: 304 loss: 1.61272 training acc: 0.30338 testing acc: 0.2996\n",
      "epoch: 2 iter: 305 loss: 1.61607 training acc: 0.29992 testing acc: 0.2968\n",
      "epoch: 2 iter: 306 loss: 1.61401 training acc: 0.30910 testing acc: 0.3114\n",
      "epoch: 2 iter: 307 loss: 1.61643 training acc: 0.30132 testing acc: 0.2984\n",
      "epoch: 2 iter: 308 loss: 1.61863 training acc: 0.30700 testing acc: 0.3042\n",
      "epoch: 2 iter: 309 loss: 1.61160 training acc: 0.31880 testing acc: 0.3202\n",
      "epoch: 2 iter: 310 loss: 1.61195 training acc: 0.30794 testing acc: 0.3052\n",
      "epoch: 2 iter: 311 loss: 1.61909 training acc: 0.30578 testing acc: 0.3012\n",
      "epoch: 2 iter: 312 loss: 1.62131 training acc: 0.30872 testing acc: 0.3066\n",
      "epoch: 2 iter: 313 loss: 1.61444 training acc: 0.30890 testing acc: 0.3048\n",
      "epoch: 2 iter: 314 loss: 1.61017 training acc: 0.31244 testing acc: 0.3112\n",
      "epoch: 2 iter: 315 loss: 1.61620 training acc: 0.31606 testing acc: 0.3158\n",
      "epoch: 2 iter: 316 loss: 1.61346 training acc: 0.30102 testing acc: 0.296\n",
      "epoch: 2 iter: 317 loss: 1.62634 training acc: 0.29856 testing acc: 0.2938\n",
      "epoch: 2 iter: 318 loss: 1.61036 training acc: 0.30708 testing acc: 0.3094\n",
      "epoch: 2 iter: 319 loss: 1.61256 training acc: 0.30438 testing acc: 0.3028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 iter: 320 loss: 1.61259 training acc: 0.31470 testing acc: 0.3108\n",
      "epoch: 2 iter: 321 loss: 1.62019 training acc: 0.30342 testing acc: 0.3048\n",
      "epoch: 2 iter: 322 loss: 1.61768 training acc: 0.30408 testing acc: 0.3054\n",
      "epoch: 2 iter: 323 loss: 1.61282 training acc: 0.31348 testing acc: 0.316\n",
      "epoch: 2 iter: 324 loss: 1.60548 training acc: 0.32500 testing acc: 0.3244\n",
      "epoch: 2 iter: 325 loss: 1.60547 training acc: 0.30854 testing acc: 0.3062\n",
      "epoch: 2 iter: 326 loss: 1.61279 training acc: 0.31404 testing acc: 0.3154\n",
      "epoch: 2 iter: 327 loss: 1.59745 training acc: 0.31794 testing acc: 0.3192\n",
      "epoch: 2 iter: 328 loss: 1.60165 training acc: 0.31450 testing acc: 0.3174\n",
      "epoch: 2 iter: 329 loss: 1.61475 training acc: 0.30316 testing acc: 0.3034\n",
      "epoch: 2 iter: 330 loss: 1.62240 training acc: 0.31132 testing acc: 0.3126\n",
      "epoch: 2 iter: 331 loss: 1.60138 training acc: 0.31066 testing acc: 0.3106\n",
      "epoch: 2 iter: 332 loss: 1.61166 training acc: 0.31150 testing acc: 0.3096\n",
      "epoch: 2 iter: 333 loss: 1.63220 training acc: 0.30262 testing acc: 0.3\n",
      "epoch: 2 iter: 334 loss: 1.61794 training acc: 0.32484 testing acc: 0.3298\n",
      "epoch: 2 iter: 335 loss: 1.60588 training acc: 0.32486 testing acc: 0.327\n",
      "epoch: 2 iter: 336 loss: 1.62196 training acc: 0.31372 testing acc: 0.3182\n",
      "epoch: 2 iter: 337 loss: 1.60537 training acc: 0.32132 testing acc: 0.3232\n",
      "epoch: 2 iter: 338 loss: 1.60356 training acc: 0.32318 testing acc: 0.3244\n",
      "epoch: 2 iter: 339 loss: 1.59586 training acc: 0.34514 testing acc: 0.3496\n",
      "epoch: 2 iter: 340 loss: 1.59445 training acc: 0.33090 testing acc: 0.3336\n",
      "epoch: 2 iter: 341 loss: 1.59649 training acc: 0.34284 testing acc: 0.3438\n",
      "epoch: 2 iter: 342 loss: 1.59535 training acc: 0.32540 testing acc: 0.3274\n",
      "epoch: 2 iter: 343 loss: 1.59889 training acc: 0.32668 testing acc: 0.3294\n",
      "epoch: 2 iter: 344 loss: 1.60252 training acc: 0.31684 testing acc: 0.317\n",
      "epoch: 2 iter: 345 loss: 1.59176 training acc: 0.31798 testing acc: 0.3182\n",
      "epoch: 2 iter: 346 loss: 1.59309 training acc: 0.35414 testing acc: 0.3552\n",
      "epoch: 2 iter: 347 loss: 1.59360 training acc: 0.35234 testing acc: 0.3548\n",
      "epoch: 2 iter: 348 loss: 1.60305 training acc: 0.33168 testing acc: 0.3316\n",
      "epoch: 2 iter: 349 loss: 1.60411 training acc: 0.34878 testing acc: 0.3528\n",
      "epoch: 2 iter: 350 loss: 1.59749 training acc: 0.35402 testing acc: 0.3588\n",
      "epoch: 2 iter: 351 loss: 1.58885 training acc: 0.33694 testing acc: 0.336\n",
      "epoch: 2 iter: 352 loss: 1.58695 training acc: 0.31962 testing acc: 0.3186\n",
      "epoch: 2 iter: 353 loss: 1.58580 training acc: 0.32522 testing acc: 0.3244\n",
      "epoch: 2 iter: 354 loss: 1.58434 training acc: 0.31518 testing acc: 0.3138\n",
      "epoch: 2 iter: 355 loss: 1.59238 training acc: 0.32426 testing acc: 0.3244\n",
      "epoch: 2 iter: 356 loss: 1.58714 training acc: 0.30862 testing acc: 0.3056\n",
      "epoch: 2 iter: 357 loss: 1.58265 training acc: 0.31392 testing acc: 0.312\n",
      "epoch: 2 iter: 358 loss: 1.58671 training acc: 0.32318 testing acc: 0.3224\n",
      "epoch: 2 iter: 359 loss: 1.58489 training acc: 0.32316 testing acc: 0.3208\n",
      "epoch: 2 iter: 360 loss: 1.58964 training acc: 0.30948 testing acc: 0.3066\n",
      "epoch: 2 iter: 361 loss: 1.58293 training acc: 0.31266 testing acc: 0.308\n",
      "epoch: 2 iter: 362 loss: 1.58027 training acc: 0.31814 testing acc: 0.3162\n",
      "epoch: 2 iter: 363 loss: 1.59362 training acc: 0.30812 testing acc: 0.3096\n",
      "epoch: 2 iter: 364 loss: 1.57738 training acc: 0.31634 testing acc: 0.3186\n",
      "epoch: 2 iter: 365 loss: 1.58539 training acc: 0.32248 testing acc: 0.322\n",
      "epoch: 2 iter: 366 loss: 1.58613 training acc: 0.30532 testing acc: 0.3016\n",
      "epoch: 2 iter: 367 loss: 1.58524 training acc: 0.31174 testing acc: 0.3068\n",
      "epoch: 2 iter: 368 loss: 1.57519 training acc: 0.31666 testing acc: 0.315\n",
      "epoch: 2 iter: 369 loss: 1.57163 training acc: 0.31712 testing acc: 0.3162\n",
      "epoch: 2 iter: 370 loss: 1.59264 training acc: 0.30622 testing acc: 0.3082\n",
      "epoch: 2 iter: 371 loss: 1.58216 training acc: 0.30968 testing acc: 0.3108\n",
      "epoch: 2 iter: 372 loss: 1.58012 training acc: 0.31788 testing acc: 0.3186\n",
      "epoch: 2 iter: 373 loss: 1.57730 training acc: 0.33506 testing acc: 0.3374\n",
      "epoch: 2 iter: 374 loss: 1.57275 training acc: 0.33796 testing acc: 0.34\n",
      "epoch: 2 iter: 375 loss: 1.57761 training acc: 0.32306 testing acc: 0.3238\n",
      "epoch: 2 iter: 376 loss: 1.56991 training acc: 0.33608 testing acc: 0.336\n",
      "epoch: 2 iter: 377 loss: 1.57876 training acc: 0.31758 testing acc: 0.3186\n",
      "epoch: 2 iter: 378 loss: 1.57770 training acc: 0.31524 testing acc: 0.3126\n",
      "epoch: 2 iter: 379 loss: 1.56474 training acc: 0.34360 testing acc: 0.3442\n",
      "epoch: 2 iter: 380 loss: 1.56544 training acc: 0.33604 testing acc: 0.3388\n",
      "epoch: 2 iter: 381 loss: 1.56521 training acc: 0.33592 testing acc: 0.3386\n",
      "epoch: 2 iter: 382 loss: 1.56426 training acc: 0.33616 testing acc: 0.3358\n",
      "epoch: 2 iter: 383 loss: 1.56563 training acc: 0.36070 testing acc: 0.3618\n",
      "epoch: 2 iter: 384 loss: 1.56540 training acc: 0.36082 testing acc: 0.365\n",
      "epoch: 2 iter: 385 loss: 1.55803 training acc: 0.35318 testing acc: 0.3542\n",
      "epoch: 2 iter: 386 loss: 1.56396 training acc: 0.32576 testing acc: 0.3236\n",
      "epoch: 2 iter: 387 loss: 1.56112 training acc: 0.34416 testing acc: 0.3472\n",
      "epoch: 2 iter: 388 loss: 1.56303 training acc: 0.34212 testing acc: 0.3432\n",
      "epoch: 2 iter: 389 loss: 1.56637 training acc: 0.32412 testing acc: 0.3232\n",
      "epoch: 2 iter: 390 loss: 1.56158 training acc: 0.33670 testing acc: 0.3328\n",
      "epoch: 2 iter: 391 loss: 1.55901 training acc: 0.32772 testing acc: 0.326\n",
      "epoch: 2 iter: 392 loss: 1.55730 training acc: 0.31722 testing acc: 0.3158\n",
      "epoch: 2 iter: 393 loss: 1.55664 training acc: 0.35742 testing acc: 0.3544\n",
      "epoch: 2 iter: 394 loss: 1.56889 training acc: 0.35340 testing acc: 0.3512\n",
      "epoch: 2 iter: 395 loss: 1.56369 training acc: 0.35084 testing acc: 0.3444\n",
      "epoch: 2 iter: 396 loss: 1.55626 training acc: 0.32760 testing acc: 0.3252\n",
      "epoch: 2 iter: 397 loss: 1.55550 training acc: 0.33404 testing acc: 0.3342\n",
      "epoch: 2 iter: 398 loss: 1.55502 training acc: 0.33862 testing acc: 0.3374\n",
      "epoch: 2 iter: 399 loss: 1.55533 training acc: 0.36564 testing acc: 0.3682\n",
      "epoch: 2 iter: 400 loss: 1.56414 training acc: 0.33126 testing acc: 0.3258\n",
      "epoch: 2 iter: 401 loss: 1.55276 training acc: 0.34948 testing acc: 0.3486\n",
      "epoch: 2 iter: 402 loss: 1.54854 training acc: 0.34632 testing acc: 0.3458\n",
      "epoch: 2 iter: 403 loss: 1.54632 training acc: 0.34594 testing acc: 0.3474\n",
      "epoch: 2 iter: 404 loss: 1.54584 training acc: 0.34164 testing acc: 0.3426\n",
      "epoch: 2 iter: 405 loss: 1.55184 training acc: 0.35472 testing acc: 0.354\n",
      "epoch: 2 iter: 406 loss: 1.55463 training acc: 0.34834 testing acc: 0.3486\n",
      "epoch: 2 iter: 407 loss: 1.54668 training acc: 0.33690 testing acc: 0.3362\n",
      "epoch: 2 iter: 408 loss: 1.58227 training acc: 0.32304 testing acc: 0.3224\n",
      "epoch: 2 iter: 409 loss: 1.55679 training acc: 0.33002 testing acc: 0.3272\n",
      "epoch: 2 iter: 410 loss: 1.55433 training acc: 0.32990 testing acc: 0.3286\n",
      "epoch: 2 iter: 411 loss: 1.54988 training acc: 0.33346 testing acc: 0.3322\n",
      "epoch: 2 iter: 412 loss: 1.55340 training acc: 0.34684 testing acc: 0.3448\n",
      "epoch: 2 iter: 413 loss: 1.53812 training acc: 0.34660 testing acc: 0.3442\n",
      "epoch: 2 iter: 414 loss: 1.54196 training acc: 0.35416 testing acc: 0.351\n",
      "epoch: 2 iter: 415 loss: 1.53599 training acc: 0.35772 testing acc: 0.3542\n",
      "epoch: 2 iter: 416 loss: 1.53842 training acc: 0.35248 testing acc: 0.35\n",
      "epoch: 2 iter: 417 loss: 1.54419 training acc: 0.37638 testing acc: 0.3806\n",
      "epoch: 2 iter: 418 loss: 1.54391 training acc: 0.35882 testing acc: 0.3586\n",
      "epoch: 2 iter: 419 loss: 1.53267 training acc: 0.37092 testing acc: 0.365\n",
      "epoch: 2 iter: 420 loss: 1.52792 training acc: 0.36586 testing acc: 0.363\n",
      "epoch: 2 iter: 421 loss: 1.53085 training acc: 0.33962 testing acc: 0.3384\n",
      "epoch: 2 iter: 422 loss: 1.53388 training acc: 0.34366 testing acc: 0.3412\n",
      "epoch: 2 iter: 423 loss: 1.53503 training acc: 0.33696 testing acc: 0.3358\n",
      "epoch: 2 iter: 424 loss: 1.52968 training acc: 0.34308 testing acc: 0.34\n",
      "epoch: 2 iter: 425 loss: 1.52503 training acc: 0.34966 testing acc: 0.3466\n",
      "epoch: 2 iter: 426 loss: 1.52082 training acc: 0.35972 testing acc: 0.3586\n",
      "epoch: 2 iter: 427 loss: 1.52630 training acc: 0.35210 testing acc: 0.3492\n",
      "epoch: 2 iter: 428 loss: 1.53758 training acc: 0.36270 testing acc: 0.366\n",
      "epoch: 2 iter: 429 loss: 1.52877 training acc: 0.37640 testing acc: 0.3772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 iter: 430 loss: 1.53098 training acc: 0.37748 testing acc: 0.3814\n",
      "epoch: 2 iter: 431 loss: 1.52276 training acc: 0.38450 testing acc: 0.3874\n",
      "epoch: 2 iter: 432 loss: 1.51567 training acc: 0.40032 testing acc: 0.4052\n",
      "epoch: 2 iter: 433 loss: 1.51452 training acc: 0.38628 testing acc: 0.3926\n",
      "epoch: 2 iter: 434 loss: 1.51784 training acc: 0.34696 testing acc: 0.3422\n",
      "epoch: 2 iter: 435 loss: 1.51474 training acc: 0.35454 testing acc: 0.3514\n",
      "epoch: 2 iter: 436 loss: 1.50977 training acc: 0.35784 testing acc: 0.358\n",
      "epoch: 2 iter: 437 loss: 1.51478 training acc: 0.36298 testing acc: 0.3566\n",
      "epoch: 2 iter: 438 loss: 1.51120 training acc: 0.39322 testing acc: 0.3928\n",
      "epoch: 2 iter: 439 loss: 1.50730 training acc: 0.36522 testing acc: 0.364\n",
      "epoch: 2 iter: 440 loss: 1.51036 training acc: 0.35532 testing acc: 0.3586\n",
      "epoch: 2 iter: 441 loss: 1.50138 training acc: 0.37490 testing acc: 0.3748\n",
      "epoch: 2 iter: 442 loss: 1.49855 training acc: 0.36684 testing acc: 0.3668\n",
      "epoch: 2 iter: 443 loss: 1.50198 training acc: 0.35484 testing acc: 0.3532\n",
      "epoch: 2 iter: 444 loss: 1.51444 training acc: 0.35388 testing acc: 0.352\n",
      "epoch: 2 iter: 445 loss: 1.51012 training acc: 0.36402 testing acc: 0.3656\n",
      "epoch: 2 iter: 446 loss: 1.50892 training acc: 0.37544 testing acc: 0.3736\n",
      "epoch: 2 iter: 447 loss: 1.51473 training acc: 0.37884 testing acc: 0.3792\n",
      "epoch: 2 iter: 448 loss: 1.52312 training acc: 0.37868 testing acc: 0.3786\n",
      "epoch: 2 iter: 449 loss: 1.49703 training acc: 0.37880 testing acc: 0.3796\n",
      "epoch: 2 iter: 450 loss: 1.50010 training acc: 0.38222 testing acc: 0.3822\n",
      "epoch: 2 iter: 451 loss: 1.50077 training acc: 0.38988 testing acc: 0.3886\n",
      "epoch: 2 iter: 452 loss: 1.49073 training acc: 0.37734 testing acc: 0.3764\n",
      "epoch: 2 iter: 453 loss: 1.48942 training acc: 0.37906 testing acc: 0.383\n",
      "epoch: 2 iter: 454 loss: 1.48749 training acc: 0.36934 testing acc: 0.3744\n",
      "epoch: 2 iter: 455 loss: 1.47992 training acc: 0.39892 testing acc: 0.3968\n",
      "epoch: 2 iter: 456 loss: 1.49717 training acc: 0.38548 testing acc: 0.3818\n",
      "epoch: 2 iter: 457 loss: 1.48356 training acc: 0.40162 testing acc: 0.4002\n",
      "epoch: 2 iter: 458 loss: 1.47633 training acc: 0.39264 testing acc: 0.3918\n",
      "epoch: 2 iter: 459 loss: 1.48056 training acc: 0.39592 testing acc: 0.397\n",
      "epoch: 2 iter: 460 loss: 1.47490 training acc: 0.39458 testing acc: 0.3936\n",
      "epoch: 2 iter: 461 loss: 1.48064 training acc: 0.39768 testing acc: 0.3986\n",
      "epoch: 2 iter: 462 loss: 1.47935 training acc: 0.40212 testing acc: 0.4062\n",
      "epoch: 2 iter: 463 loss: 1.48119 training acc: 0.36554 testing acc: 0.3694\n",
      "epoch: 2 iter: 464 loss: 1.48869 training acc: 0.38226 testing acc: 0.3864\n",
      "epoch: 2 iter: 465 loss: 1.46325 training acc: 0.39214 testing acc: 0.3966\n",
      "epoch: 2 iter: 466 loss: 1.46639 training acc: 0.38536 testing acc: 0.3862\n",
      "epoch: 2 iter: 467 loss: 1.46272 training acc: 0.39994 testing acc: 0.4046\n",
      "epoch: 2 iter: 468 loss: 1.46592 training acc: 0.37732 testing acc: 0.3806\n",
      "epoch: 2 iter: 469 loss: 1.46310 training acc: 0.38340 testing acc: 0.3848\n",
      "epoch: 2 iter: 470 loss: 1.46400 training acc: 0.38242 testing acc: 0.3826\n",
      "epoch: 2 iter: 471 loss: 1.46494 training acc: 0.37852 testing acc: 0.3848\n",
      "epoch: 2 iter: 472 loss: 1.47408 training acc: 0.37574 testing acc: 0.38\n",
      "epoch: 2 iter: 473 loss: 1.45731 training acc: 0.39786 testing acc: 0.4\n",
      "epoch: 2 iter: 474 loss: 1.44978 training acc: 0.39322 testing acc: 0.393\n",
      "epoch: 2 iter: 475 loss: 1.45387 training acc: 0.39160 testing acc: 0.3898\n",
      "epoch: 2 iter: 476 loss: 1.44542 training acc: 0.39788 testing acc: 0.3992\n",
      "epoch: 2 iter: 477 loss: 1.44228 training acc: 0.40510 testing acc: 0.4078\n",
      "epoch: 2 iter: 478 loss: 1.45835 training acc: 0.39658 testing acc: 0.3978\n",
      "epoch: 2 iter: 479 loss: 1.44107 training acc: 0.40504 testing acc: 0.4066\n",
      "epoch: 2 iter: 480 loss: 1.44123 training acc: 0.39656 testing acc: 0.396\n",
      "epoch: 2 iter: 481 loss: 1.44636 training acc: 0.39502 testing acc: 0.3948\n",
      "epoch: 2 iter: 482 loss: 1.43900 training acc: 0.41688 testing acc: 0.4124\n",
      "epoch: 2 iter: 483 loss: 1.43762 training acc: 0.41070 testing acc: 0.4082\n",
      "epoch: 2 iter: 484 loss: 1.43576 training acc: 0.39648 testing acc: 0.4012\n",
      "epoch: 2 iter: 485 loss: 1.43652 training acc: 0.40730 testing acc: 0.4066\n",
      "epoch: 2 iter: 486 loss: 1.44317 training acc: 0.40502 testing acc: 0.4052\n",
      "epoch: 2 iter: 487 loss: 1.43754 training acc: 0.41118 testing acc: 0.4116\n",
      "epoch: 2 iter: 488 loss: 1.43090 training acc: 0.40582 testing acc: 0.406\n",
      "epoch: 2 iter: 489 loss: 1.42602 training acc: 0.40908 testing acc: 0.407\n",
      "epoch: 2 iter: 490 loss: 1.42402 training acc: 0.40416 testing acc: 0.4062\n",
      "epoch: 2 iter: 491 loss: 1.42607 training acc: 0.40032 testing acc: 0.4034\n",
      "epoch: 2 iter: 492 loss: 1.43104 training acc: 0.39660 testing acc: 0.3976\n",
      "epoch: 2 iter: 493 loss: 1.42843 training acc: 0.41336 testing acc: 0.417\n",
      "epoch: 2 iter: 494 loss: 1.41920 training acc: 0.42024 testing acc: 0.4276\n",
      "epoch: 2 iter: 495 loss: 1.41958 training acc: 0.41380 testing acc: 0.4174\n",
      "epoch: 2 iter: 496 loss: 1.42475 training acc: 0.39620 testing acc: 0.3996\n",
      "epoch: 2 iter: 497 loss: 1.41746 training acc: 0.42668 testing acc: 0.43\n",
      "epoch: 2 iter: 498 loss: 1.42153 training acc: 0.41194 testing acc: 0.4142\n",
      "epoch: 2 iter: 499 loss: 1.41967 training acc: 0.41606 testing acc: 0.4148\n",
      "epoch: 2 iter: 500 loss: 1.41565 training acc: 0.41292 testing acc: 0.4132\n",
      "epoch: 2 iter: 501 loss: 1.40856 training acc: 0.41494 testing acc: 0.4158\n",
      "epoch: 2 iter: 502 loss: 1.41311 training acc: 0.40616 testing acc: 0.4052\n",
      "epoch: 2 iter: 503 loss: 1.41028 training acc: 0.40714 testing acc: 0.4078\n",
      "epoch: 2 iter: 504 loss: 1.40345 training acc: 0.40270 testing acc: 0.3988\n",
      "epoch: 2 iter: 505 loss: 1.40937 training acc: 0.41282 testing acc: 0.4124\n",
      "epoch: 2 iter: 506 loss: 1.41200 training acc: 0.41396 testing acc: 0.4168\n",
      "epoch: 2 iter: 507 loss: 1.40103 training acc: 0.40578 testing acc: 0.4042\n",
      "epoch: 2 iter: 508 loss: 1.39977 training acc: 0.41208 testing acc: 0.4134\n",
      "epoch: 2 iter: 509 loss: 1.40235 training acc: 0.40776 testing acc: 0.4076\n",
      "epoch: 2 iter: 510 loss: 1.42431 training acc: 0.38998 testing acc: 0.3912\n",
      "epoch: 2 iter: 511 loss: 1.40548 training acc: 0.40544 testing acc: 0.4072\n",
      "epoch: 2 iter: 512 loss: 1.40759 training acc: 0.39684 testing acc: 0.3982\n",
      "epoch: 2 iter: 513 loss: 1.40082 training acc: 0.40790 testing acc: 0.4092\n",
      "epoch: 2 iter: 514 loss: 1.39408 training acc: 0.40750 testing acc: 0.4098\n",
      "epoch: 2 iter: 515 loss: 1.39557 training acc: 0.40264 testing acc: 0.4078\n",
      "epoch: 2 iter: 516 loss: 1.39884 training acc: 0.39982 testing acc: 0.4\n",
      "epoch: 2 iter: 517 loss: 1.39793 training acc: 0.39346 testing acc: 0.3942\n",
      "epoch: 2 iter: 518 loss: 1.39339 training acc: 0.40686 testing acc: 0.4094\n",
      "epoch: 2 iter: 519 loss: 1.39659 training acc: 0.41640 testing acc: 0.416\n",
      "epoch: 2 iter: 520 loss: 1.39492 training acc: 0.41878 testing acc: 0.4186\n",
      "epoch: 2 iter: 521 loss: 1.38961 training acc: 0.42840 testing acc: 0.4276\n",
      "epoch: 2 iter: 522 loss: 1.39030 training acc: 0.43184 testing acc: 0.4322\n",
      "epoch: 2 iter: 523 loss: 1.38770 training acc: 0.41888 testing acc: 0.4172\n",
      "epoch: 2 iter: 524 loss: 1.38706 training acc: 0.43670 testing acc: 0.4356\n",
      "epoch: 2 iter: 525 loss: 1.38886 training acc: 0.41460 testing acc: 0.4084\n",
      "epoch: 2 iter: 526 loss: 1.38140 training acc: 0.41204 testing acc: 0.4082\n",
      "epoch: 2 iter: 527 loss: 1.38369 training acc: 0.41570 testing acc: 0.4152\n",
      "epoch: 2 iter: 528 loss: 1.38357 training acc: 0.41470 testing acc: 0.413\n",
      "epoch: 2 iter: 529 loss: 1.37834 training acc: 0.41198 testing acc: 0.41\n",
      "epoch: 2 iter: 530 loss: 1.38409 training acc: 0.40502 testing acc: 0.4034\n",
      "epoch: 2 iter: 531 loss: 1.38474 training acc: 0.41430 testing acc: 0.4126\n",
      "epoch: 2 iter: 532 loss: 1.37326 training acc: 0.41404 testing acc: 0.4112\n",
      "epoch: 2 iter: 533 loss: 1.38910 training acc: 0.41020 testing acc: 0.4058\n",
      "epoch: 2 iter: 534 loss: 1.37804 training acc: 0.41452 testing acc: 0.4166\n",
      "epoch: 2 iter: 535 loss: 1.37249 training acc: 0.41526 testing acc: 0.4152\n",
      "epoch: 2 iter: 536 loss: 1.37901 training acc: 0.43826 testing acc: 0.442\n",
      "epoch: 2 iter: 537 loss: 1.37507 training acc: 0.42416 testing acc: 0.4236\n",
      "epoch: 2 iter: 538 loss: 1.37783 training acc: 0.42494 testing acc: 0.4252\n",
      "epoch: 2 iter: 539 loss: 1.37460 training acc: 0.42766 testing acc: 0.4336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 iter: 540 loss: 1.38557 training acc: 0.41134 testing acc: 0.4174\n",
      "epoch: 2 iter: 541 loss: 1.37623 training acc: 0.40774 testing acc: 0.409\n",
      "epoch: 2 iter: 542 loss: 1.37656 training acc: 0.41720 testing acc: 0.4144\n",
      "epoch: 2 iter: 543 loss: 1.37390 training acc: 0.41910 testing acc: 0.4164\n",
      "epoch: 2 iter: 544 loss: 1.36858 training acc: 0.42898 testing acc: 0.4332\n",
      "epoch: 2 iter: 545 loss: 1.37168 training acc: 0.41198 testing acc: 0.4074\n",
      "epoch: 2 iter: 546 loss: 1.39134 training acc: 0.42594 testing acc: 0.4212\n",
      "epoch: 2 iter: 547 loss: 1.37026 training acc: 0.42946 testing acc: 0.4266\n",
      "epoch: 2 iter: 548 loss: 1.36858 training acc: 0.43066 testing acc: 0.4302\n",
      "epoch: 2 iter: 549 loss: 1.37131 training acc: 0.43238 testing acc: 0.4316\n",
      "epoch: 2 iter: 550 loss: 1.37613 training acc: 0.43262 testing acc: 0.4324\n",
      "epoch: 2 iter: 551 loss: 1.36551 training acc: 0.43546 testing acc: 0.4314\n",
      "epoch: 2 iter: 552 loss: 1.36747 training acc: 0.43088 testing acc: 0.4266\n",
      "epoch: 2 iter: 553 loss: 1.36728 training acc: 0.42932 testing acc: 0.4226\n",
      "epoch: 2 iter: 554 loss: 1.36458 training acc: 0.43146 testing acc: 0.4296\n",
      "epoch: 2 iter: 555 loss: 1.36095 training acc: 0.42022 testing acc: 0.4188\n",
      "epoch: 2 iter: 556 loss: 1.36648 training acc: 0.41650 testing acc: 0.4182\n",
      "epoch: 2 iter: 557 loss: 1.35766 training acc: 0.41868 testing acc: 0.4192\n",
      "epoch: 2 iter: 558 loss: 1.35752 training acc: 0.43152 testing acc: 0.4298\n",
      "epoch: 2 iter: 559 loss: 1.35489 training acc: 0.43276 testing acc: 0.43\n",
      "epoch: 2 iter: 560 loss: 1.36528 training acc: 0.42882 testing acc: 0.4306\n",
      "epoch: 2 iter: 561 loss: 1.35864 training acc: 0.43722 testing acc: 0.4356\n",
      "epoch: 2 iter: 562 loss: 1.38496 training acc: 0.43096 testing acc: 0.4336\n",
      "epoch: 2 iter: 563 loss: 1.35660 training acc: 0.43686 testing acc: 0.4312\n",
      "epoch: 2 iter: 564 loss: 1.35511 training acc: 0.44170 testing acc: 0.4368\n",
      "epoch: 2 iter: 565 loss: 1.36457 training acc: 0.43328 testing acc: 0.426\n",
      "epoch: 2 iter: 566 loss: 1.36598 training acc: 0.42724 testing acc: 0.417\n",
      "epoch: 2 iter: 567 loss: 1.35722 training acc: 0.44546 testing acc: 0.4406\n",
      "epoch: 2 iter: 568 loss: 1.35911 training acc: 0.42752 testing acc: 0.4208\n",
      "epoch: 2 iter: 569 loss: 1.36531 training acc: 0.42000 testing acc: 0.4192\n",
      "epoch: 2 iter: 570 loss: 1.35448 training acc: 0.43426 testing acc: 0.4262\n",
      "epoch: 2 iter: 571 loss: 1.35319 training acc: 0.42686 testing acc: 0.424\n",
      "epoch: 2 iter: 572 loss: 1.35113 training acc: 0.45886 testing acc: 0.4586\n",
      "epoch: 2 iter: 573 loss: 1.35860 training acc: 0.42422 testing acc: 0.4274\n",
      "epoch: 2 iter: 574 loss: 1.35145 training acc: 0.42480 testing acc: 0.4258\n",
      "epoch: 2 iter: 575 loss: 1.35496 training acc: 0.41546 testing acc: 0.4202\n",
      "epoch: 2 iter: 576 loss: 1.35831 training acc: 0.43500 testing acc: 0.4316\n",
      "epoch: 2 iter: 577 loss: 1.34999 training acc: 0.44358 testing acc: 0.4446\n",
      "epoch: 2 iter: 578 loss: 1.34805 training acc: 0.43190 testing acc: 0.4302\n",
      "epoch: 2 iter: 579 loss: 1.36021 training acc: 0.42930 testing acc: 0.4328\n",
      "epoch: 2 iter: 580 loss: 1.35126 training acc: 0.43018 testing acc: 0.4316\n",
      "epoch: 2 iter: 581 loss: 1.35147 training acc: 0.42476 testing acc: 0.4296\n",
      "epoch: 2 iter: 582 loss: 1.34416 training acc: 0.43144 testing acc: 0.4322\n",
      "epoch: 2 iter: 583 loss: 1.34386 training acc: 0.43234 testing acc: 0.4278\n",
      "epoch: 2 iter: 584 loss: 1.34674 training acc: 0.44268 testing acc: 0.4368\n",
      "epoch: 2 iter: 585 loss: 1.34071 training acc: 0.44518 testing acc: 0.4426\n",
      "epoch: 2 iter: 586 loss: 1.34405 training acc: 0.43960 testing acc: 0.4352\n",
      "epoch: 2 iter: 587 loss: 1.33872 training acc: 0.46282 testing acc: 0.4554\n",
      "epoch: 2 iter: 588 loss: 1.34776 training acc: 0.44014 testing acc: 0.4352\n",
      "epoch: 2 iter: 589 loss: 1.35292 training acc: 0.45152 testing acc: 0.4432\n",
      "epoch: 2 iter: 590 loss: 1.34612 training acc: 0.43204 testing acc: 0.4296\n",
      "epoch: 2 iter: 591 loss: 1.35451 training acc: 0.43028 testing acc: 0.4304\n",
      "epoch: 2 iter: 592 loss: 1.34559 training acc: 0.43432 testing acc: 0.4384\n",
      "epoch: 2 iter: 593 loss: 1.34505 training acc: 0.43572 testing acc: 0.4354\n",
      "epoch: 2 iter: 594 loss: 1.34108 training acc: 0.44672 testing acc: 0.451\n",
      "epoch: 2 iter: 595 loss: 1.34999 training acc: 0.41146 testing acc: 0.4146\n",
      "epoch: 2 iter: 596 loss: 1.34190 training acc: 0.42366 testing acc: 0.4196\n",
      "epoch: 2 iter: 597 loss: 1.34006 training acc: 0.42476 testing acc: 0.4192\n",
      "epoch: 2 iter: 598 loss: 1.34488 training acc: 0.41888 testing acc: 0.4146\n",
      "epoch: 2 iter: 599 loss: 1.34011 training acc: 0.43276 testing acc: 0.4334\n",
      "epoch: 2 iter: 600 loss: 1.33322 training acc: 0.45772 testing acc: 0.4538\n",
      "epoch: 2 iter: 601 loss: 1.33267 training acc: 0.43606 testing acc: 0.4332\n",
      "epoch: 2 iter: 602 loss: 1.33382 training acc: 0.43678 testing acc: 0.4338\n",
      "epoch: 2 iter: 603 loss: 1.34115 training acc: 0.44598 testing acc: 0.4386\n",
      "epoch: 2 iter: 604 loss: 1.34054 training acc: 0.44660 testing acc: 0.4376\n",
      "epoch: 2 iter: 605 loss: 1.34395 training acc: 0.43720 testing acc: 0.427\n",
      "epoch: 2 iter: 606 loss: 1.35793 training acc: 0.42102 testing acc: 0.4182\n",
      "epoch: 2 iter: 607 loss: 1.33355 training acc: 0.44760 testing acc: 0.4434\n",
      "epoch: 2 iter: 608 loss: 1.32865 training acc: 0.43280 testing acc: 0.4302\n",
      "epoch: 2 iter: 609 loss: 1.32894 training acc: 0.42844 testing acc: 0.4208\n",
      "epoch: 2 iter: 610 loss: 1.33514 training acc: 0.43674 testing acc: 0.4322\n",
      "epoch: 2 iter: 611 loss: 1.33067 training acc: 0.46330 testing acc: 0.464\n",
      "epoch: 2 iter: 612 loss: 1.33001 training acc: 0.44374 testing acc: 0.4388\n",
      "epoch: 2 iter: 613 loss: 1.32662 training acc: 0.43990 testing acc: 0.432\n",
      "epoch: 2 iter: 614 loss: 1.32289 training acc: 0.44538 testing acc: 0.4382\n",
      "epoch: 2 iter: 615 loss: 1.33223 training acc: 0.43000 testing acc: 0.4248\n",
      "epoch: 2 iter: 616 loss: 1.34807 training acc: 0.43652 testing acc: 0.4302\n",
      "epoch: 2 iter: 617 loss: 1.33827 training acc: 0.42924 testing acc: 0.423\n",
      "epoch: 2 iter: 618 loss: 1.32935 training acc: 0.43588 testing acc: 0.4304\n",
      "epoch: 2 iter: 619 loss: 1.32493 training acc: 0.44004 testing acc: 0.4392\n",
      "epoch: 2 iter: 620 loss: 1.33579 training acc: 0.43222 testing acc: 0.4284\n",
      "epoch: 2 iter: 621 loss: 1.33000 training acc: 0.43886 testing acc: 0.4322\n",
      "epoch: 2 iter: 622 loss: 1.32705 training acc: 0.44046 testing acc: 0.4408\n",
      "epoch: 2 iter: 623 loss: 1.33123 training acc: 0.44374 testing acc: 0.4436\n",
      "epoch: 2 iter: 624 loss: 1.32287 training acc: 0.44442 testing acc: 0.4446\n",
      "epoch: 2 iter: 625 loss: 1.32128 training acc: 0.45690 testing acc: 0.4578\n",
      "epoch: 2 iter: 626 loss: 1.32284 training acc: 0.45766 testing acc: 0.4504\n",
      "epoch: 2 iter: 627 loss: 1.33140 training acc: 0.45640 testing acc: 0.4518\n",
      "epoch: 2 iter: 628 loss: 1.33176 training acc: 0.45790 testing acc: 0.4516\n",
      "epoch: 2 iter: 629 loss: 1.33590 training acc: 0.45120 testing acc: 0.4508\n",
      "epoch: 2 iter: 630 loss: 1.34021 training acc: 0.44500 testing acc: 0.4388\n",
      "epoch: 2 iter: 631 loss: 1.32332 training acc: 0.44384 testing acc: 0.4402\n",
      "epoch: 2 iter: 632 loss: 1.31868 training acc: 0.44860 testing acc: 0.448\n",
      "epoch: 2 iter: 633 loss: 1.32257 training acc: 0.45338 testing acc: 0.4474\n",
      "epoch: 2 iter: 634 loss: 1.31715 training acc: 0.46386 testing acc: 0.4626\n",
      "epoch: 2 iter: 635 loss: 1.32426 training acc: 0.45222 testing acc: 0.46\n",
      "epoch: 2 iter: 636 loss: 1.31922 training acc: 0.46040 testing acc: 0.4574\n",
      "epoch: 2 iter: 637 loss: 1.35602 training acc: 0.45548 testing acc: 0.4548\n",
      "epoch: 2 iter: 638 loss: 1.32778 training acc: 0.45506 testing acc: 0.4552\n",
      "epoch: 2 iter: 639 loss: 1.32584 training acc: 0.44590 testing acc: 0.445\n",
      "epoch: 2 iter: 640 loss: 1.32001 training acc: 0.43838 testing acc: 0.441\n",
      "epoch: 2 iter: 641 loss: 1.31300 training acc: 0.44916 testing acc: 0.4494\n",
      "epoch: 2 iter: 642 loss: 1.32399 training acc: 0.43956 testing acc: 0.4376\n",
      "epoch: 2 iter: 643 loss: 1.31262 training acc: 0.44716 testing acc: 0.446\n",
      "epoch: 2 iter: 644 loss: 1.34618 training acc: 0.43542 testing acc: 0.4354\n",
      "epoch: 2 iter: 645 loss: 1.31586 training acc: 0.43188 testing acc: 0.4346\n",
      "epoch: 2 iter: 646 loss: 1.31017 training acc: 0.43748 testing acc: 0.4408\n",
      "epoch: 2 iter: 647 loss: 1.30830 training acc: 0.45668 testing acc: 0.4626\n",
      "epoch: 2 iter: 648 loss: 1.31838 training acc: 0.45124 testing acc: 0.4514\n",
      "epoch: 2 iter: 649 loss: 1.30983 training acc: 0.44518 testing acc: 0.4464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 iter: 650 loss: 1.32128 training acc: 0.44208 testing acc: 0.4428\n",
      "epoch: 2 iter: 651 loss: 1.32146 training acc: 0.44264 testing acc: 0.436\n",
      "epoch: 2 iter: 652 loss: 1.32746 training acc: 0.47136 testing acc: 0.4702\n",
      "epoch: 2 iter: 653 loss: 1.31199 training acc: 0.46402 testing acc: 0.4594\n",
      "epoch: 2 iter: 654 loss: 1.32241 training acc: 0.44532 testing acc: 0.4462\n",
      "epoch: 2 iter: 655 loss: 1.31436 training acc: 0.46970 testing acc: 0.4668\n",
      "epoch: 2 iter: 656 loss: 1.31881 training acc: 0.46074 testing acc: 0.4554\n",
      "epoch: 2 iter: 657 loss: 1.32595 training acc: 0.43764 testing acc: 0.4408\n",
      "epoch: 2 iter: 658 loss: 1.30307 training acc: 0.46226 testing acc: 0.4548\n",
      "epoch: 2 iter: 659 loss: 1.30170 training acc: 0.44884 testing acc: 0.4524\n",
      "epoch: 2 iter: 660 loss: 1.30077 training acc: 0.45612 testing acc: 0.4586\n",
      "epoch: 2 iter: 661 loss: 1.30453 training acc: 0.46822 testing acc: 0.4708\n",
      "epoch: 2 iter: 662 loss: 1.31321 training acc: 0.44250 testing acc: 0.4398\n",
      "epoch: 2 iter: 663 loss: 1.32937 training acc: 0.44568 testing acc: 0.443\n",
      "epoch: 2 iter: 664 loss: 1.30649 training acc: 0.46062 testing acc: 0.4608\n",
      "epoch: 2 iter: 665 loss: 1.30226 training acc: 0.46204 testing acc: 0.4632\n",
      "epoch: 2 iter: 666 loss: 1.31474 training acc: 0.45640 testing acc: 0.4556\n",
      "epoch: 2 iter: 667 loss: 1.32108 training acc: 0.44854 testing acc: 0.452\n",
      "epoch: 2 iter: 668 loss: 1.32018 training acc: 0.44172 testing acc: 0.4408\n",
      "epoch: 2 iter: 669 loss: 1.29919 training acc: 0.47034 testing acc: 0.469\n",
      "epoch: 2 iter: 670 loss: 1.29436 training acc: 0.46922 testing acc: 0.466\n",
      "epoch: 2 iter: 671 loss: 1.30686 training acc: 0.44902 testing acc: 0.4434\n",
      "epoch: 2 iter: 672 loss: 1.30238 training acc: 0.45676 testing acc: 0.4524\n",
      "epoch: 2 iter: 673 loss: 1.30057 training acc: 0.45202 testing acc: 0.4444\n",
      "epoch: 2 iter: 674 loss: 1.29778 training acc: 0.46296 testing acc: 0.4614\n",
      "epoch: 2 iter: 675 loss: 1.30711 training acc: 0.46622 testing acc: 0.4696\n",
      "epoch: 2 iter: 676 loss: 1.29410 training acc: 0.46722 testing acc: 0.4642\n",
      "epoch: 2 iter: 677 loss: 1.30261 training acc: 0.45328 testing acc: 0.449\n",
      "epoch: 2 iter: 678 loss: 1.29024 training acc: 0.47476 testing acc: 0.4674\n",
      "epoch: 2 iter: 679 loss: 1.28676 training acc: 0.47252 testing acc: 0.4656\n",
      "epoch: 2 iter: 680 loss: 1.29539 training acc: 0.46804 testing acc: 0.4624\n",
      "epoch: 2 iter: 681 loss: 1.28779 training acc: 0.46808 testing acc: 0.4644\n",
      "epoch: 2 iter: 682 loss: 1.29205 training acc: 0.45438 testing acc: 0.4472\n",
      "epoch: 2 iter: 683 loss: 1.28427 training acc: 0.46930 testing acc: 0.4612\n",
      "epoch: 2 iter: 684 loss: 1.28715 training acc: 0.46758 testing acc: 0.4608\n",
      "epoch: 2 iter: 685 loss: 1.29659 training acc: 0.44172 testing acc: 0.4372\n",
      "epoch: 2 iter: 686 loss: 1.29611 training acc: 0.46552 testing acc: 0.46\n",
      "epoch: 2 iter: 687 loss: 1.28644 training acc: 0.44890 testing acc: 0.446\n",
      "epoch: 2 iter: 688 loss: 1.30401 training acc: 0.45426 testing acc: 0.4488\n",
      "epoch: 2 iter: 689 loss: 1.29093 training acc: 0.46590 testing acc: 0.4652\n",
      "epoch: 2 iter: 690 loss: 1.28815 training acc: 0.44762 testing acc: 0.4414\n",
      "epoch: 2 iter: 691 loss: 1.30569 training acc: 0.46398 testing acc: 0.4568\n",
      "epoch: 2 iter: 692 loss: 1.28252 training acc: 0.49888 testing acc: 0.5024\n",
      "epoch: 2 iter: 693 loss: 1.27837 training acc: 0.49304 testing acc: 0.4942\n",
      "epoch: 2 iter: 694 loss: 1.29344 training acc: 0.48304 testing acc: 0.4776\n",
      "epoch: 2 iter: 695 loss: 1.27780 training acc: 0.47676 testing acc: 0.4724\n",
      "epoch: 2 iter: 696 loss: 1.28926 training acc: 0.48376 testing acc: 0.4832\n",
      "epoch: 2 iter: 697 loss: 1.29181 training acc: 0.47876 testing acc: 0.4818\n",
      "epoch: 2 iter: 698 loss: 1.28319 training acc: 0.48982 testing acc: 0.4816\n",
      "epoch: 2 iter: 699 loss: 1.28587 training acc: 0.47330 testing acc: 0.4634\n",
      "epoch: 2 iter: 700 loss: 1.30320 training acc: 0.46576 testing acc: 0.4572\n",
      "epoch: 2 iter: 701 loss: 1.27851 training acc: 0.47922 testing acc: 0.4678\n",
      "epoch: 2 iter: 702 loss: 1.27172 training acc: 0.49664 testing acc: 0.4908\n",
      "epoch: 2 iter: 703 loss: 1.27306 training acc: 0.48334 testing acc: 0.478\n",
      "epoch: 2 iter: 704 loss: 1.29650 training acc: 0.45540 testing acc: 0.4496\n",
      "epoch: 2 iter: 705 loss: 1.28296 training acc: 0.46452 testing acc: 0.4606\n",
      "epoch: 2 iter: 706 loss: 1.28207 training acc: 0.45716 testing acc: 0.451\n",
      "epoch: 2 iter: 707 loss: 1.30243 training acc: 0.46134 testing acc: 0.4538\n",
      "epoch: 2 iter: 708 loss: 1.27811 training acc: 0.46462 testing acc: 0.4612\n",
      "epoch: 2 iter: 709 loss: 1.28116 training acc: 0.47234 testing acc: 0.463\n",
      "epoch: 2 iter: 710 loss: 1.27569 training acc: 0.47080 testing acc: 0.465\n",
      "epoch: 2 iter: 711 loss: 1.28729 training acc: 0.48096 testing acc: 0.4738\n",
      "epoch: 2 iter: 712 loss: 1.26223 training acc: 0.48302 testing acc: 0.4768\n",
      "epoch: 2 iter: 713 loss: 1.28278 training acc: 0.50394 testing acc: 0.504\n",
      "epoch: 2 iter: 714 loss: 1.28233 training acc: 0.49924 testing acc: 0.494\n",
      "epoch: 2 iter: 715 loss: 1.26343 training acc: 0.48244 testing acc: 0.4826\n",
      "epoch: 2 iter: 716 loss: 1.27207 training acc: 0.47170 testing acc: 0.471\n",
      "epoch: 2 iter: 717 loss: 1.26060 training acc: 0.47412 testing acc: 0.4682\n",
      "epoch: 2 iter: 718 loss: 1.26437 training acc: 0.48858 testing acc: 0.4826\n",
      "epoch: 2 iter: 719 loss: 1.25760 training acc: 0.49708 testing acc: 0.4896\n",
      "epoch: 2 iter: 720 loss: 1.25485 training acc: 0.49666 testing acc: 0.4924\n",
      "epoch: 2 iter: 721 loss: 1.26269 training acc: 0.48412 testing acc: 0.4738\n",
      "epoch: 2 iter: 722 loss: 1.26129 training acc: 0.50364 testing acc: 0.4984\n",
      "epoch: 2 iter: 723 loss: 1.25705 training acc: 0.48586 testing acc: 0.4802\n",
      "epoch: 2 iter: 724 loss: 1.25964 training acc: 0.47480 testing acc: 0.4712\n",
      "epoch: 2 iter: 725 loss: 1.26285 training acc: 0.46526 testing acc: 0.4634\n",
      "epoch: 2 iter: 726 loss: 1.25942 training acc: 0.50724 testing acc: 0.5088\n",
      "epoch: 2 iter: 727 loss: 1.25466 training acc: 0.48422 testing acc: 0.4786\n",
      "epoch: 2 iter: 728 loss: 1.25724 training acc: 0.47092 testing acc: 0.4636\n",
      "epoch: 2 iter: 729 loss: 1.25674 training acc: 0.46106 testing acc: 0.4526\n",
      "epoch: 2 iter: 730 loss: 1.26297 training acc: 0.47594 testing acc: 0.4602\n",
      "epoch: 2 iter: 731 loss: 1.25219 training acc: 0.50904 testing acc: 0.5018\n",
      "epoch: 2 iter: 732 loss: 1.25932 training acc: 0.49516 testing acc: 0.4886\n",
      "epoch: 2 iter: 733 loss: 1.26854 training acc: 0.49050 testing acc: 0.4906\n",
      "epoch: 2 iter: 734 loss: 1.25414 training acc: 0.48566 testing acc: 0.4774\n",
      "epoch: 2 iter: 735 loss: 1.24559 training acc: 0.49712 testing acc: 0.4866\n",
      "epoch: 2 iter: 736 loss: 1.25516 training acc: 0.48370 testing acc: 0.4722\n",
      "epoch: 2 iter: 737 loss: 1.24278 training acc: 0.52194 testing acc: 0.511\n",
      "epoch: 2 iter: 738 loss: 1.24329 training acc: 0.49586 testing acc: 0.4826\n",
      "epoch: 2 iter: 739 loss: 1.23687 training acc: 0.52816 testing acc: 0.5276\n",
      "epoch: 2 iter: 740 loss: 1.26131 training acc: 0.49208 testing acc: 0.4858\n",
      "epoch: 2 iter: 741 loss: 1.24893 training acc: 0.50336 testing acc: 0.4994\n",
      "epoch: 2 iter: 742 loss: 1.24885 training acc: 0.50154 testing acc: 0.489\n",
      "epoch: 2 iter: 743 loss: 1.26794 training acc: 0.47544 testing acc: 0.4692\n",
      "epoch: 2 iter: 744 loss: 1.23833 training acc: 0.49308 testing acc: 0.4846\n",
      "epoch: 2 iter: 745 loss: 1.23600 training acc: 0.49088 testing acc: 0.4816\n",
      "epoch: 2 iter: 746 loss: 1.22547 training acc: 0.52836 testing acc: 0.5228\n",
      "epoch: 2 iter: 747 loss: 1.23371 training acc: 0.52690 testing acc: 0.5298\n",
      "epoch: 2 iter: 748 loss: 1.22459 training acc: 0.50108 testing acc: 0.4916\n",
      "epoch: 2 iter: 749 loss: 1.22296 training acc: 0.51666 testing acc: 0.5038\n",
      "epoch: 2 iter: 750 loss: 1.23567 training acc: 0.47780 testing acc: 0.4704\n",
      "epoch: 2 iter: 751 loss: 1.22021 training acc: 0.51300 testing acc: 0.5026\n",
      "epoch: 2 iter: 752 loss: 1.21521 training acc: 0.52942 testing acc: 0.5232\n",
      "epoch: 2 iter: 753 loss: 1.22023 training acc: 0.49778 testing acc: 0.4926\n",
      "epoch: 2 iter: 754 loss: 1.22650 training acc: 0.50478 testing acc: 0.4972\n",
      "epoch: 2 iter: 755 loss: 1.22386 training acc: 0.50372 testing acc: 0.4978\n",
      "epoch: 2 iter: 756 loss: 1.21452 training acc: 0.51822 testing acc: 0.5088\n",
      "epoch: 2 iter: 757 loss: 1.21750 training acc: 0.52610 testing acc: 0.5264\n",
      "epoch: 2 iter: 758 loss: 1.23487 training acc: 0.51602 testing acc: 0.5204\n",
      "epoch: 2 iter: 759 loss: 1.21759 training acc: 0.51952 testing acc: 0.5182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 iter: 760 loss: 1.20805 training acc: 0.50764 testing acc: 0.5016\n",
      "epoch: 2 iter: 761 loss: 1.20739 training acc: 0.54302 testing acc: 0.5346\n",
      "epoch: 2 iter: 762 loss: 1.20476 training acc: 0.54464 testing acc: 0.5384\n",
      "epoch: 2 iter: 763 loss: 1.19816 training acc: 0.54460 testing acc: 0.534\n",
      "epoch: 2 iter: 764 loss: 1.22166 training acc: 0.52006 testing acc: 0.5254\n",
      "epoch: 2 iter: 765 loss: 1.19898 training acc: 0.54810 testing acc: 0.5398\n",
      "epoch: 2 iter: 766 loss: 1.19784 training acc: 0.54948 testing acc: 0.5382\n",
      "epoch: 2 iter: 767 loss: 1.19833 training acc: 0.54110 testing acc: 0.5334\n",
      "epoch: 2 iter: 768 loss: 1.19487 training acc: 0.52070 testing acc: 0.5066\n",
      "epoch: 2 iter: 769 loss: 1.19130 training acc: 0.53566 testing acc: 0.5288\n",
      "epoch: 2 iter: 770 loss: 1.19674 training acc: 0.54416 testing acc: 0.5406\n",
      "epoch: 2 iter: 771 loss: 1.20701 training acc: 0.54270 testing acc: 0.534\n",
      "epoch: 2 iter: 772 loss: 1.18614 training acc: 0.54174 testing acc: 0.54\n",
      "epoch: 2 iter: 773 loss: 1.18362 training acc: 0.54910 testing acc: 0.5408\n",
      "epoch: 2 iter: 774 loss: 1.19286 training acc: 0.53980 testing acc: 0.5326\n",
      "epoch: 2 iter: 775 loss: 1.18624 training acc: 0.53610 testing acc: 0.5286\n",
      "epoch: 2 iter: 776 loss: 1.17771 training acc: 0.53312 testing acc: 0.5312\n",
      "epoch: 2 iter: 777 loss: 1.17583 training acc: 0.56986 testing acc: 0.57\n",
      "epoch: 2 iter: 778 loss: 1.18097 training acc: 0.54908 testing acc: 0.5442\n",
      "epoch: 2 iter: 779 loss: 1.18942 training acc: 0.52134 testing acc: 0.5108\n",
      "epoch: 2 iter: 780 loss: 1.17971 training acc: 0.55800 testing acc: 0.557\n",
      "epoch: 2 iter: 781 loss: 1.17628 training acc: 0.56906 testing acc: 0.565\n",
      "epoch: 2 iter: 782 loss: 1.16574 training acc: 0.58396 testing acc: 0.5778\n",
      "epoch: 2 iter: 783 loss: 1.16337 training acc: 0.57856 testing acc: 0.5732\n",
      "epoch: 2 iter: 784 loss: 1.17325 training acc: 0.52464 testing acc: 0.5148\n",
      "epoch: 2 iter: 785 loss: 1.18584 training acc: 0.56254 testing acc: 0.5548\n",
      "epoch: 2 iter: 786 loss: 1.18004 training acc: 0.55934 testing acc: 0.5476\n",
      "epoch: 2 iter: 787 loss: 1.23112 training acc: 0.54590 testing acc: 0.5382\n",
      "epoch: 2 iter: 788 loss: 1.16011 training acc: 0.59266 testing acc: 0.5844\n",
      "epoch: 2 iter: 789 loss: 1.16230 training acc: 0.59410 testing acc: 0.5858\n",
      "epoch: 2 iter: 790 loss: 1.24925 training acc: 0.51382 testing acc: 0.5056\n",
      "epoch: 2 iter: 791 loss: 1.14905 training acc: 0.58090 testing acc: 0.5782\n",
      "epoch: 2 iter: 792 loss: 1.14621 training acc: 0.60318 testing acc: 0.6004\n",
      "epoch: 2 iter: 793 loss: 1.17729 training acc: 0.57838 testing acc: 0.575\n",
      "epoch: 2 iter: 794 loss: 1.14729 training acc: 0.60624 testing acc: 0.6006\n",
      "epoch: 2 iter: 795 loss: 1.14101 training acc: 0.61154 testing acc: 0.6048\n",
      "epoch: 2 iter: 796 loss: 1.13941 training acc: 0.59576 testing acc: 0.5912\n",
      "epoch: 2 iter: 797 loss: 1.15670 training acc: 0.59904 testing acc: 0.5982\n",
      "epoch: 2 iter: 798 loss: 1.15551 training acc: 0.61200 testing acc: 0.6162\n",
      "epoch: 2 iter: 799 loss: 1.14382 training acc: 0.60828 testing acc: 0.6062\n",
      "epoch: 2 iter: 800 loss: 1.13915 training acc: 0.58318 testing acc: 0.5758\n",
      "epoch: 2 iter: 801 loss: 1.13333 training acc: 0.59820 testing acc: 0.5906\n",
      "epoch: 2 iter: 802 loss: 1.13603 training acc: 0.59116 testing acc: 0.5856\n",
      "epoch: 2 iter: 803 loss: 1.12566 training acc: 0.60740 testing acc: 0.6048\n",
      "epoch: 2 iter: 804 loss: 1.12077 training acc: 0.61068 testing acc: 0.6058\n",
      "epoch: 2 iter: 805 loss: 1.12472 training acc: 0.60468 testing acc: 0.5992\n",
      "epoch: 2 iter: 806 loss: 1.11357 training acc: 0.60656 testing acc: 0.6014\n",
      "epoch: 2 iter: 807 loss: 1.10764 training acc: 0.62178 testing acc: 0.615\n",
      "epoch: 2 iter: 808 loss: 1.11147 training acc: 0.62312 testing acc: 0.62\n",
      "epoch: 2 iter: 809 loss: 1.10904 training acc: 0.61138 testing acc: 0.6064\n",
      "epoch: 2 iter: 810 loss: 1.10897 training acc: 0.62238 testing acc: 0.6252\n",
      "epoch: 2 iter: 811 loss: 1.12080 training acc: 0.60758 testing acc: 0.6086\n",
      "epoch: 2 iter: 812 loss: 1.10170 training acc: 0.61782 testing acc: 0.6122\n",
      "epoch: 2 iter: 813 loss: 1.10501 training acc: 0.62410 testing acc: 0.6314\n",
      "epoch: 2 iter: 814 loss: 1.10399 training acc: 0.62552 testing acc: 0.6324\n",
      "epoch: 2 iter: 815 loss: 1.10474 training acc: 0.61666 testing acc: 0.6158\n",
      "epoch: 2 iter: 816 loss: 1.09920 training acc: 0.60712 testing acc: 0.6036\n",
      "epoch: 2 iter: 817 loss: 1.09182 training acc: 0.60900 testing acc: 0.6092\n",
      "epoch: 2 iter: 818 loss: 1.09450 training acc: 0.60996 testing acc: 0.604\n",
      "epoch: 2 iter: 819 loss: 1.12195 training acc: 0.58732 testing acc: 0.584\n",
      "epoch: 2 iter: 820 loss: 1.08817 training acc: 0.60902 testing acc: 0.605\n",
      "epoch: 2 iter: 821 loss: 1.10147 training acc: 0.60536 testing acc: 0.5998\n",
      "epoch: 2 iter: 822 loss: 1.08834 training acc: 0.60770 testing acc: 0.5982\n",
      "epoch: 2 iter: 823 loss: 1.08318 training acc: 0.60720 testing acc: 0.5944\n",
      "epoch: 2 iter: 824 loss: 1.08401 training acc: 0.60506 testing acc: 0.5976\n",
      "epoch: 2 iter: 825 loss: 1.07562 training acc: 0.60250 testing acc: 0.5984\n",
      "epoch: 2 iter: 826 loss: 1.08715 training acc: 0.61646 testing acc: 0.6168\n",
      "epoch: 2 iter: 827 loss: 1.07217 training acc: 0.61632 testing acc: 0.6134\n",
      "epoch: 2 iter: 828 loss: 1.07039 training acc: 0.62244 testing acc: 0.6188\n",
      "epoch: 2 iter: 829 loss: 1.07572 training acc: 0.60332 testing acc: 0.598\n",
      "epoch: 2 iter: 830 loss: 1.06639 training acc: 0.60924 testing acc: 0.605\n",
      "epoch: 2 iter: 831 loss: 1.05282 training acc: 0.61192 testing acc: 0.6072\n",
      "epoch: 2 iter: 832 loss: 1.05236 training acc: 0.61994 testing acc: 0.6128\n",
      "epoch: 2 iter: 833 loss: 1.06031 training acc: 0.61082 testing acc: 0.6046\n",
      "epoch: 2 iter: 834 loss: 1.07239 training acc: 0.61642 testing acc: 0.614\n",
      "epoch: 2 iter: 835 loss: 1.06302 training acc: 0.60244 testing acc: 0.5956\n",
      "epoch: 2 iter: 836 loss: 1.04770 training acc: 0.60848 testing acc: 0.6042\n",
      "epoch: 2 iter: 837 loss: 1.04318 training acc: 0.63112 testing acc: 0.6328\n",
      "epoch: 2 iter: 838 loss: 1.05438 training acc: 0.62398 testing acc: 0.626\n",
      "epoch: 2 iter: 839 loss: 1.03783 training acc: 0.63022 testing acc: 0.6332\n",
      "epoch: 2 iter: 840 loss: 1.03476 training acc: 0.62522 testing acc: 0.6276\n",
      "epoch: 2 iter: 841 loss: 1.03078 training acc: 0.61448 testing acc: 0.6132\n",
      "epoch: 2 iter: 842 loss: 1.03844 training acc: 0.60400 testing acc: 0.6006\n",
      "epoch: 2 iter: 843 loss: 1.02657 training acc: 0.61966 testing acc: 0.6186\n",
      "epoch: 2 iter: 844 loss: 1.03433 training acc: 0.61060 testing acc: 0.6002\n",
      "epoch: 2 iter: 845 loss: 1.02929 training acc: 0.62852 testing acc: 0.6238\n",
      "epoch: 2 iter: 846 loss: 1.03952 training acc: 0.62234 testing acc: 0.6226\n",
      "epoch: 2 iter: 847 loss: 1.04882 training acc: 0.62374 testing acc: 0.623\n",
      "epoch: 2 iter: 848 loss: 1.02955 training acc: 0.62648 testing acc: 0.6256\n",
      "epoch: 2 iter: 849 loss: 1.02155 training acc: 0.62634 testing acc: 0.623\n",
      "epoch: 2 iter: 850 loss: 1.02344 training acc: 0.61070 testing acc: 0.6034\n",
      "epoch: 2 iter: 851 loss: 1.02012 training acc: 0.60554 testing acc: 0.5948\n",
      "epoch: 2 iter: 852 loss: 1.03411 training acc: 0.60742 testing acc: 0.598\n",
      "epoch: 2 iter: 853 loss: 1.01761 training acc: 0.62364 testing acc: 0.618\n",
      "epoch: 2 iter: 854 loss: 1.02030 training acc: 0.62720 testing acc: 0.6216\n",
      "epoch: 2 iter: 855 loss: 1.01339 training acc: 0.62982 testing acc: 0.626\n",
      "epoch: 2 iter: 856 loss: 1.00649 training acc: 0.62630 testing acc: 0.6186\n",
      "epoch: 2 iter: 857 loss: 1.01759 training acc: 0.61736 testing acc: 0.6128\n",
      "epoch: 2 iter: 858 loss: 0.99803 training acc: 0.62808 testing acc: 0.6198\n",
      "epoch: 2 iter: 859 loss: 1.00510 training acc: 0.60880 testing acc: 0.601\n",
      "epoch: 2 iter: 860 loss: 1.03325 training acc: 0.59928 testing acc: 0.5964\n",
      "epoch: 2 iter: 861 loss: 1.00604 training acc: 0.60836 testing acc: 0.6038\n",
      "epoch: 2 iter: 862 loss: 0.99341 training acc: 0.61350 testing acc: 0.6096\n",
      "epoch: 2 iter: 863 loss: 1.00592 training acc: 0.60858 testing acc: 0.601\n",
      "epoch: 2 iter: 864 loss: 0.99530 training acc: 0.61106 testing acc: 0.6048\n",
      "epoch: 2 iter: 865 loss: 0.99060 training acc: 0.61296 testing acc: 0.6098\n",
      "epoch: 2 iter: 866 loss: 0.98696 training acc: 0.62028 testing acc: 0.6156\n",
      "epoch: 2 iter: 867 loss: 0.98880 training acc: 0.62002 testing acc: 0.6112\n",
      "epoch: 2 iter: 868 loss: 0.98973 training acc: 0.61396 testing acc: 0.6128\n",
      "epoch: 2 iter: 869 loss: 0.98812 training acc: 0.61402 testing acc: 0.613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 iter: 870 loss: 0.98439 training acc: 0.62202 testing acc: 0.6228\n",
      "epoch: 2 iter: 871 loss: 0.98532 training acc: 0.63020 testing acc: 0.6316\n",
      "epoch: 2 iter: 872 loss: 0.99014 training acc: 0.62796 testing acc: 0.6294\n",
      "epoch: 2 iter: 873 loss: 1.00093 training acc: 0.62492 testing acc: 0.6212\n",
      "epoch: 2 iter: 874 loss: 0.97486 training acc: 0.62550 testing acc: 0.624\n",
      "epoch: 2 iter: 875 loss: 0.98165 training acc: 0.62278 testing acc: 0.6208\n",
      "epoch: 2 iter: 876 loss: 0.97169 training acc: 0.63432 testing acc: 0.6334\n",
      "epoch: 2 iter: 877 loss: 0.96579 training acc: 0.63344 testing acc: 0.6346\n",
      "epoch: 2 iter: 878 loss: 0.96283 training acc: 0.62292 testing acc: 0.6168\n",
      "epoch: 2 iter: 879 loss: 0.97672 training acc: 0.61422 testing acc: 0.6054\n",
      "epoch: 2 iter: 880 loss: 0.95479 training acc: 0.63334 testing acc: 0.6336\n",
      "epoch: 2 iter: 881 loss: 0.95856 training acc: 0.62926 testing acc: 0.6282\n",
      "epoch: 2 iter: 882 loss: 0.96713 training acc: 0.63656 testing acc: 0.6362\n",
      "epoch: 2 iter: 883 loss: 0.97914 training acc: 0.62456 testing acc: 0.6188\n",
      "epoch: 2 iter: 884 loss: 0.95473 training acc: 0.62160 testing acc: 0.6182\n",
      "epoch: 2 iter: 885 loss: 0.95428 training acc: 0.63222 testing acc: 0.6328\n",
      "epoch: 2 iter: 886 loss: 0.95280 training acc: 0.64474 testing acc: 0.6462\n",
      "epoch: 2 iter: 887 loss: 0.94893 training acc: 0.63942 testing acc: 0.6394\n",
      "epoch: 2 iter: 888 loss: 0.96488 training acc: 0.62404 testing acc: 0.6124\n",
      "epoch: 2 iter: 889 loss: 0.95914 training acc: 0.61930 testing acc: 0.6168\n",
      "epoch: 2 iter: 890 loss: 0.94293 training acc: 0.62868 testing acc: 0.6198\n",
      "epoch: 2 iter: 891 loss: 0.94037 training acc: 0.63240 testing acc: 0.6224\n",
      "epoch: 2 iter: 892 loss: 0.99004 training acc: 0.63250 testing acc: 0.632\n",
      "epoch: 2 iter: 893 loss: 0.93545 training acc: 0.64698 testing acc: 0.6414\n",
      "epoch: 2 iter: 894 loss: 0.93787 training acc: 0.64122 testing acc: 0.635\n",
      "epoch: 2 iter: 895 loss: 0.94600 training acc: 0.63982 testing acc: 0.637\n",
      "epoch: 2 iter: 896 loss: 0.94820 training acc: 0.63508 testing acc: 0.6298\n",
      "epoch: 2 iter: 897 loss: 0.92874 training acc: 0.64162 testing acc: 0.6396\n",
      "epoch: 2 iter: 898 loss: 0.92887 training acc: 0.64004 testing acc: 0.6366\n",
      "epoch: 2 iter: 899 loss: 0.92779 training acc: 0.63986 testing acc: 0.6346\n",
      "epoch: 2 iter: 900 loss: 0.92348 training acc: 0.63302 testing acc: 0.6262\n",
      "epoch: 2 iter: 901 loss: 0.92294 training acc: 0.63490 testing acc: 0.6276\n",
      "epoch: 2 iter: 902 loss: 0.95544 training acc: 0.62942 testing acc: 0.629\n",
      "epoch: 2 iter: 903 loss: 0.93239 training acc: 0.63450 testing acc: 0.633\n",
      "epoch: 2 iter: 904 loss: 0.91758 training acc: 0.64186 testing acc: 0.6408\n",
      "epoch: 2 iter: 905 loss: 0.91377 training acc: 0.64924 testing acc: 0.6466\n",
      "epoch: 2 iter: 906 loss: 0.90697 training acc: 0.65034 testing acc: 0.6448\n",
      "epoch: 2 iter: 907 loss: 0.91290 training acc: 0.63770 testing acc: 0.631\n",
      "epoch: 2 iter: 908 loss: 0.90972 training acc: 0.65490 testing acc: 0.6508\n",
      "epoch: 2 iter: 909 loss: 0.91221 training acc: 0.65898 testing acc: 0.662\n",
      "epoch: 2 iter: 910 loss: 0.91374 training acc: 0.66500 testing acc: 0.6638\n",
      "epoch: 2 iter: 911 loss: 0.92563 training acc: 0.64832 testing acc: 0.6528\n",
      "epoch: 2 iter: 912 loss: 0.92832 training acc: 0.63390 testing acc: 0.6354\n",
      "epoch: 2 iter: 913 loss: 0.92200 training acc: 0.64020 testing acc: 0.6402\n",
      "epoch: 2 iter: 914 loss: 0.90764 training acc: 0.65314 testing acc: 0.6524\n",
      "epoch: 2 iter: 915 loss: 0.90661 training acc: 0.64744 testing acc: 0.644\n",
      "epoch: 2 iter: 916 loss: 0.90609 training acc: 0.64296 testing acc: 0.637\n",
      "epoch: 2 iter: 917 loss: 0.91659 training acc: 0.64722 testing acc: 0.643\n",
      "epoch: 2 iter: 918 loss: 0.91668 training acc: 0.65230 testing acc: 0.6458\n",
      "epoch: 2 iter: 919 loss: 0.90423 training acc: 0.65868 testing acc: 0.6576\n",
      "epoch: 2 iter: 920 loss: 0.91431 training acc: 0.64496 testing acc: 0.6404\n",
      "epoch: 2 iter: 921 loss: 0.88652 training acc: 0.65828 testing acc: 0.6608\n",
      "epoch: 2 iter: 922 loss: 0.89156 training acc: 0.66432 testing acc: 0.6692\n",
      "epoch: 2 iter: 923 loss: 0.89490 training acc: 0.65892 testing acc: 0.664\n",
      "epoch: 2 iter: 924 loss: 0.89901 training acc: 0.65318 testing acc: 0.6552\n",
      "epoch: 2 iter: 925 loss: 0.89119 training acc: 0.66656 testing acc: 0.6674\n",
      "epoch: 2 iter: 926 loss: 0.89834 training acc: 0.64716 testing acc: 0.6414\n",
      "epoch: 2 iter: 927 loss: 0.90599 training acc: 0.65094 testing acc: 0.6504\n",
      "epoch: 2 iter: 928 loss: 0.90730 training acc: 0.65856 testing acc: 0.6552\n",
      "epoch: 2 iter: 929 loss: 0.88946 training acc: 0.67672 testing acc: 0.6772\n",
      "epoch: 2 iter: 930 loss: 0.87720 training acc: 0.67366 testing acc: 0.6758\n",
      "epoch: 2 iter: 931 loss: 0.87442 training acc: 0.67936 testing acc: 0.6804\n",
      "epoch: 2 iter: 932 loss: 0.88210 training acc: 0.67226 testing acc: 0.6774\n",
      "epoch: 2 iter: 933 loss: 0.87054 training acc: 0.66742 testing acc: 0.672\n",
      "epoch: 2 iter: 934 loss: 0.87000 training acc: 0.67262 testing acc: 0.68\n",
      "epoch: 2 iter: 935 loss: 0.88958 training acc: 0.67968 testing acc: 0.6878\n",
      "epoch: 2 iter: 936 loss: 0.87505 training acc: 0.67780 testing acc: 0.6812\n",
      "epoch: 2 iter: 937 loss: 0.86994 training acc: 0.66036 testing acc: 0.6632\n",
      "epoch: 2 iter: 938 loss: 0.87648 training acc: 0.66114 testing acc: 0.6634\n",
      "epoch: 2 iter: 939 loss: 0.88752 training acc: 0.65182 testing acc: 0.6512\n",
      "epoch: 2 iter: 940 loss: 0.87567 training acc: 0.65922 testing acc: 0.6546\n",
      "epoch: 2 iter: 941 loss: 0.88349 training acc: 0.66892 testing acc: 0.6744\n",
      "epoch: 2 iter: 942 loss: 0.86567 training acc: 0.67378 testing acc: 0.6774\n",
      "epoch: 2 iter: 943 loss: 0.87273 training acc: 0.66266 testing acc: 0.6644\n",
      "epoch: 2 iter: 944 loss: 0.85884 training acc: 0.66928 testing acc: 0.675\n",
      "epoch: 2 iter: 945 loss: 0.85650 training acc: 0.67914 testing acc: 0.6822\n",
      "epoch: 2 iter: 946 loss: 0.84924 training acc: 0.68636 testing acc: 0.6892\n",
      "epoch: 2 iter: 947 loss: 0.85383 training acc: 0.68888 testing acc: 0.6946\n",
      "epoch: 2 iter: 948 loss: 0.85402 training acc: 0.68426 testing acc: 0.688\n",
      "epoch: 2 iter: 949 loss: 0.86943 training acc: 0.68996 testing acc: 0.6936\n",
      "epoch: 2 iter: 950 loss: 0.84544 training acc: 0.69692 testing acc: 0.6988\n",
      "epoch: 2 iter: 951 loss: 0.85042 training acc: 0.68414 testing acc: 0.6878\n",
      "epoch: 2 iter: 952 loss: 0.85154 training acc: 0.68636 testing acc: 0.6834\n",
      "epoch: 2 iter: 953 loss: 0.86896 training acc: 0.67400 testing acc: 0.6746\n",
      "epoch: 2 iter: 954 loss: 0.84542 training acc: 0.69680 testing acc: 0.6984\n",
      "epoch: 2 iter: 955 loss: 0.88587 training acc: 0.68160 testing acc: 0.6814\n",
      "epoch: 2 iter: 956 loss: 0.83599 training acc: 0.69820 testing acc: 0.6956\n",
      "epoch: 2 iter: 957 loss: 0.83821 training acc: 0.69348 testing acc: 0.6932\n",
      "epoch: 2 iter: 958 loss: 0.83984 training acc: 0.69162 testing acc: 0.6944\n",
      "epoch: 2 iter: 959 loss: 0.83215 training acc: 0.68862 testing acc: 0.6912\n",
      "epoch: 2 iter: 960 loss: 0.84045 training acc: 0.68524 testing acc: 0.6874\n",
      "epoch: 2 iter: 961 loss: 0.82858 training acc: 0.70358 testing acc: 0.7046\n",
      "epoch: 2 iter: 962 loss: 0.84259 training acc: 0.70530 testing acc: 0.71\n",
      "epoch: 2 iter: 963 loss: 0.83126 training acc: 0.71658 testing acc: 0.718\n",
      "epoch: 2 iter: 964 loss: 0.82287 training acc: 0.71132 testing acc: 0.7132\n",
      "epoch: 2 iter: 965 loss: 0.85673 training acc: 0.68506 testing acc: 0.6928\n",
      "epoch: 2 iter: 966 loss: 0.82035 training acc: 0.71790 testing acc: 0.72\n",
      "epoch: 2 iter: 967 loss: 0.82187 training acc: 0.71332 testing acc: 0.7166\n",
      "epoch: 2 iter: 968 loss: 0.84383 training acc: 0.69520 testing acc: 0.6964\n",
      "epoch: 2 iter: 969 loss: 0.82758 training acc: 0.70506 testing acc: 0.7072\n",
      "epoch: 2 iter: 970 loss: 0.85325 training acc: 0.70936 testing acc: 0.7078\n",
      "epoch: 2 iter: 971 loss: 0.82116 training acc: 0.71886 testing acc: 0.7204\n",
      "epoch: 2 iter: 972 loss: 0.83472 training acc: 0.71370 testing acc: 0.7144\n",
      "epoch: 2 iter: 973 loss: 0.81234 training acc: 0.71708 testing acc: 0.7192\n",
      "epoch: 2 iter: 974 loss: 0.81182 training acc: 0.72614 testing acc: 0.7292\n",
      "epoch: 2 iter: 975 loss: 0.82447 training acc: 0.71972 testing acc: 0.7216\n",
      "epoch: 2 iter: 976 loss: 0.80448 training acc: 0.73220 testing acc: 0.732\n",
      "epoch: 2 iter: 977 loss: 0.80365 training acc: 0.72542 testing acc: 0.7282\n",
      "epoch: 2 iter: 978 loss: 0.81355 training acc: 0.72790 testing acc: 0.728\n",
      "epoch: 2 iter: 979 loss: 0.80554 training acc: 0.72848 testing acc: 0.7272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 iter: 980 loss: 0.82367 training acc: 0.71616 testing acc: 0.7158\n",
      "epoch: 2 iter: 981 loss: 0.79969 training acc: 0.73494 testing acc: 0.7364\n",
      "epoch: 2 iter: 982 loss: 0.84026 training acc: 0.72430 testing acc: 0.7208\n",
      "epoch: 2 iter: 983 loss: 0.83386 training acc: 0.72800 testing acc: 0.725\n",
      "epoch: 2 iter: 984 loss: 0.79626 training acc: 0.74318 testing acc: 0.743\n",
      "epoch: 2 iter: 985 loss: 0.79248 training acc: 0.73796 testing acc: 0.7358\n",
      "epoch: 2 iter: 986 loss: 0.79855 training acc: 0.72620 testing acc: 0.7278\n",
      "epoch: 2 iter: 987 loss: 0.80809 training acc: 0.72784 testing acc: 0.7276\n",
      "epoch: 2 iter: 988 loss: 0.79447 training acc: 0.73238 testing acc: 0.7316\n",
      "epoch: 2 iter: 989 loss: 0.84769 training acc: 0.71538 testing acc: 0.7134\n",
      "epoch: 2 iter: 990 loss: 0.78998 training acc: 0.73800 testing acc: 0.7402\n",
      "epoch: 2 iter: 991 loss: 0.80149 training acc: 0.73828 testing acc: 0.738\n",
      "epoch: 2 iter: 992 loss: 0.81287 training acc: 0.73604 testing acc: 0.733\n",
      "epoch: 2 iter: 993 loss: 0.78299 training acc: 0.74140 testing acc: 0.7482\n",
      "epoch: 2 iter: 994 loss: 0.78078 training acc: 0.74530 testing acc: 0.746\n",
      "epoch: 2 iter: 995 loss: 0.78412 training acc: 0.74518 testing acc: 0.747\n",
      "epoch: 2 iter: 996 loss: 0.78164 training acc: 0.74800 testing acc: 0.7554\n",
      "epoch: 2 iter: 997 loss: 0.77631 training acc: 0.75412 testing acc: 0.7582\n",
      "epoch: 2 iter: 998 loss: 0.80800 training acc: 0.72648 testing acc: 0.7328\n",
      "epoch: 2 iter: 999 loss: 0.78365 training acc: 0.74890 testing acc: 0.75\n",
      "epoch: 3 iter:   0 loss: 0.77152 training acc: 0.74996 testing acc: 0.7544\n",
      "epoch: 3 iter:   1 loss: 0.78983 training acc: 0.73810 testing acc: 0.7422\n",
      "epoch: 3 iter:   2 loss: 0.76560 training acc: 0.75448 testing acc: 0.7552\n",
      "epoch: 3 iter:   3 loss: 0.76030 training acc: 0.76042 testing acc: 0.7626\n",
      "epoch: 3 iter:   4 loss: 0.76394 training acc: 0.75746 testing acc: 0.757\n",
      "epoch: 3 iter:   5 loss: 0.78093 training acc: 0.74594 testing acc: 0.7472\n",
      "epoch: 3 iter:   6 loss: 0.76410 training acc: 0.75670 testing acc: 0.7588\n",
      "epoch: 3 iter:   7 loss: 0.75479 training acc: 0.76000 testing acc: 0.7626\n",
      "epoch: 3 iter:   8 loss: 0.75674 training acc: 0.75698 testing acc: 0.7606\n",
      "epoch: 3 iter:   9 loss: 0.80427 training acc: 0.73242 testing acc: 0.7358\n",
      "epoch: 3 iter:  10 loss: 0.77449 training acc: 0.74628 testing acc: 0.7506\n",
      "epoch: 3 iter:  11 loss: 0.74719 training acc: 0.75948 testing acc: 0.7642\n",
      "epoch: 3 iter:  12 loss: 0.74661 training acc: 0.76092 testing acc: 0.7662\n",
      "epoch: 3 iter:  13 loss: 0.76035 training acc: 0.75476 testing acc: 0.7578\n",
      "epoch: 3 iter:  14 loss: 0.77597 training acc: 0.74262 testing acc: 0.7466\n",
      "epoch: 3 iter:  15 loss: 0.74644 training acc: 0.76114 testing acc: 0.7674\n",
      "epoch: 3 iter:  16 loss: 0.74689 training acc: 0.75982 testing acc: 0.7642\n",
      "epoch: 3 iter:  17 loss: 0.74949 training acc: 0.76442 testing acc: 0.7676\n",
      "epoch: 3 iter:  18 loss: 0.73823 training acc: 0.76966 testing acc: 0.7724\n",
      "epoch: 3 iter:  19 loss: 0.73926 training acc: 0.77010 testing acc: 0.7708\n",
      "epoch: 3 iter:  20 loss: 0.73049 training acc: 0.77426 testing acc: 0.7792\n",
      "epoch: 3 iter:  21 loss: 0.75977 training acc: 0.75332 testing acc: 0.76\n",
      "epoch: 3 iter:  22 loss: 0.72971 training acc: 0.77224 testing acc: 0.7766\n",
      "epoch: 3 iter:  23 loss: 0.73244 training acc: 0.77114 testing acc: 0.776\n",
      "epoch: 3 iter:  24 loss: 0.73382 training acc: 0.76936 testing acc: 0.7722\n",
      "epoch: 3 iter:  25 loss: 0.74925 training acc: 0.75782 testing acc: 0.7552\n",
      "epoch: 3 iter:  26 loss: 0.72996 training acc: 0.76846 testing acc: 0.775\n",
      "epoch: 3 iter:  27 loss: 0.76234 training acc: 0.75674 testing acc: 0.7586\n",
      "epoch: 3 iter:  28 loss: 0.73547 training acc: 0.76420 testing acc: 0.77\n",
      "epoch: 3 iter:  29 loss: 0.73499 training acc: 0.75648 testing acc: 0.7628\n",
      "epoch: 3 iter:  30 loss: 0.73905 training acc: 0.75376 testing acc: 0.761\n",
      "epoch: 3 iter:  31 loss: 0.72418 training acc: 0.76626 testing acc: 0.7704\n",
      "epoch: 3 iter:  32 loss: 0.72461 training acc: 0.76686 testing acc: 0.7668\n",
      "epoch: 3 iter:  33 loss: 0.71896 training acc: 0.76918 testing acc: 0.7666\n",
      "epoch: 3 iter:  34 loss: 0.72774 training acc: 0.75836 testing acc: 0.7598\n",
      "epoch: 3 iter:  35 loss: 0.72518 training acc: 0.75564 testing acc: 0.758\n",
      "epoch: 3 iter:  36 loss: 0.71458 training acc: 0.76306 testing acc: 0.7678\n",
      "epoch: 3 iter:  37 loss: 0.70781 training acc: 0.76670 testing acc: 0.7678\n",
      "epoch: 3 iter:  38 loss: 0.72992 training acc: 0.75552 testing acc: 0.7582\n",
      "epoch: 3 iter:  39 loss: 0.74418 training acc: 0.74846 testing acc: 0.7544\n",
      "epoch: 3 iter:  40 loss: 0.70893 training acc: 0.76918 testing acc: 0.7682\n",
      "epoch: 3 iter:  41 loss: 0.71798 training acc: 0.76850 testing acc: 0.7682\n",
      "epoch: 3 iter:  42 loss: 0.74896 training acc: 0.74692 testing acc: 0.756\n",
      "epoch: 3 iter:  43 loss: 0.72272 training acc: 0.75516 testing acc: 0.7532\n",
      "epoch: 3 iter:  44 loss: 0.73507 training acc: 0.76442 testing acc: 0.7696\n",
      "epoch: 3 iter:  45 loss: 0.72020 training acc: 0.76786 testing acc: 0.7662\n",
      "epoch: 3 iter:  46 loss: 0.71449 training acc: 0.76928 testing acc: 0.7648\n",
      "epoch: 3 iter:  47 loss: 0.70614 training acc: 0.77070 testing acc: 0.7694\n",
      "epoch: 3 iter:  48 loss: 0.69387 training acc: 0.77378 testing acc: 0.7706\n",
      "epoch: 3 iter:  49 loss: 0.68895 training acc: 0.77886 testing acc: 0.781\n",
      "epoch: 3 iter:  50 loss: 0.68403 training acc: 0.78470 testing acc: 0.7874\n",
      "epoch: 3 iter:  51 loss: 0.68211 training acc: 0.78426 testing acc: 0.7858\n",
      "epoch: 3 iter:  52 loss: 0.69843 training acc: 0.77498 testing acc: 0.7728\n",
      "epoch: 3 iter:  53 loss: 0.68964 training acc: 0.78390 testing acc: 0.7846\n",
      "epoch: 3 iter:  54 loss: 0.68641 training acc: 0.77984 testing acc: 0.7828\n",
      "epoch: 3 iter:  55 loss: 0.69922 training acc: 0.77562 testing acc: 0.7732\n",
      "epoch: 3 iter:  56 loss: 0.67743 training acc: 0.78490 testing acc: 0.7802\n",
      "epoch: 3 iter:  57 loss: 0.68612 training acc: 0.78386 testing acc: 0.7834\n",
      "epoch: 3 iter:  58 loss: 0.67761 training acc: 0.78390 testing acc: 0.7888\n",
      "epoch: 3 iter:  59 loss: 0.67278 training acc: 0.78932 testing acc: 0.7882\n",
      "epoch: 3 iter:  60 loss: 0.66278 training acc: 0.79158 testing acc: 0.794\n",
      "epoch: 3 iter:  61 loss: 0.66412 training acc: 0.78800 testing acc: 0.787\n",
      "epoch: 3 iter:  62 loss: 0.66194 training acc: 0.79038 testing acc: 0.792\n",
      "epoch: 3 iter:  63 loss: 0.66328 training acc: 0.78606 testing acc: 0.786\n",
      "epoch: 3 iter:  64 loss: 0.66797 training acc: 0.78338 testing acc: 0.7856\n",
      "epoch: 3 iter:  65 loss: 0.70555 training acc: 0.76118 testing acc: 0.7636\n",
      "epoch: 3 iter:  66 loss: 0.66402 training acc: 0.79026 testing acc: 0.7902\n",
      "epoch: 3 iter:  67 loss: 0.66545 training acc: 0.78638 testing acc: 0.7854\n",
      "epoch: 3 iter:  68 loss: 0.65648 training acc: 0.79432 testing acc: 0.7938\n",
      "epoch: 3 iter:  69 loss: 0.65397 training acc: 0.79692 testing acc: 0.7962\n",
      "epoch: 3 iter:  70 loss: 0.65738 training acc: 0.79928 testing acc: 0.7986\n",
      "epoch: 3 iter:  71 loss: 0.65358 training acc: 0.79638 testing acc: 0.7966\n",
      "epoch: 3 iter:  72 loss: 0.65265 training acc: 0.79784 testing acc: 0.8012\n",
      "epoch: 3 iter:  73 loss: 0.65441 training acc: 0.80052 testing acc: 0.8018\n",
      "epoch: 3 iter:  74 loss: 0.68131 training acc: 0.79180 testing acc: 0.7896\n",
      "epoch: 3 iter:  75 loss: 0.70918 training acc: 0.76748 testing acc: 0.769\n",
      "epoch: 3 iter:  76 loss: 0.68118 training acc: 0.77922 testing acc: 0.7762\n",
      "epoch: 3 iter:  77 loss: 0.66043 training acc: 0.79324 testing acc: 0.796\n",
      "epoch: 3 iter:  78 loss: 0.66016 training acc: 0.79868 testing acc: 0.7982\n",
      "epoch: 3 iter:  79 loss: 0.63871 training acc: 0.81958 testing acc: 0.8188\n",
      "epoch: 3 iter:  80 loss: 0.64207 training acc: 0.81642 testing acc: 0.8162\n",
      "epoch: 3 iter:  81 loss: 0.63104 training acc: 0.82988 testing acc: 0.8328\n",
      "epoch: 3 iter:  82 loss: 0.64249 training acc: 0.82018 testing acc: 0.8192\n",
      "epoch: 3 iter:  83 loss: 0.64614 training acc: 0.82124 testing acc: 0.823\n",
      "epoch: 3 iter:  84 loss: 0.62759 training acc: 0.82722 testing acc: 0.8292\n",
      "epoch: 3 iter:  85 loss: 0.62088 training acc: 0.83090 testing acc: 0.8352\n",
      "epoch: 3 iter:  86 loss: 0.62676 training acc: 0.82710 testing acc: 0.8276\n",
      "epoch: 3 iter:  87 loss: 0.62015 training acc: 0.83686 testing acc: 0.8386\n",
      "epoch: 3 iter:  88 loss: 0.63604 training acc: 0.82912 testing acc: 0.8326\n",
      "epoch: 3 iter:  89 loss: 0.63087 training acc: 0.82134 testing acc: 0.8238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 iter:  90 loss: 0.61349 training acc: 0.83310 testing acc: 0.8326\n",
      "epoch: 3 iter:  91 loss: 0.64048 training acc: 0.82926 testing acc: 0.8334\n",
      "epoch: 3 iter:  92 loss: 0.67180 training acc: 0.80988 testing acc: 0.8078\n",
      "epoch: 3 iter:  93 loss: 0.62130 training acc: 0.82198 testing acc: 0.8194\n",
      "epoch: 3 iter:  94 loss: 0.60993 training acc: 0.83532 testing acc: 0.8338\n",
      "epoch: 3 iter:  95 loss: 0.63456 training acc: 0.80602 testing acc: 0.8112\n",
      "epoch: 3 iter:  96 loss: 0.64012 training acc: 0.81958 testing acc: 0.813\n",
      "epoch: 3 iter:  97 loss: 0.62689 training acc: 0.80948 testing acc: 0.8084\n",
      "epoch: 3 iter:  98 loss: 0.61617 training acc: 0.81570 testing acc: 0.8168\n",
      "epoch: 3 iter:  99 loss: 0.61864 training acc: 0.81468 testing acc: 0.8168\n",
      "epoch: 3 iter: 100 loss: 0.60751 training acc: 0.81476 testing acc: 0.8164\n",
      "epoch: 3 iter: 101 loss: 0.60628 training acc: 0.80538 testing acc: 0.8058\n",
      "epoch: 3 iter: 102 loss: 0.60443 training acc: 0.80010 testing acc: 0.8004\n",
      "epoch: 3 iter: 103 loss: 0.64200 training acc: 0.78140 testing acc: 0.7802\n",
      "epoch: 3 iter: 104 loss: 0.60472 training acc: 0.79480 testing acc: 0.7958\n",
      "epoch: 3 iter: 105 loss: 0.61683 training acc: 0.79062 testing acc: 0.793\n",
      "epoch: 3 iter: 106 loss: 0.60976 training acc: 0.79678 testing acc: 0.7958\n",
      "epoch: 3 iter: 107 loss: 0.60506 training acc: 0.79418 testing acc: 0.797\n",
      "epoch: 3 iter: 108 loss: 0.60247 training acc: 0.80238 testing acc: 0.801\n",
      "epoch: 3 iter: 109 loss: 0.60917 training acc: 0.79972 testing acc: 0.803\n",
      "epoch: 3 iter: 110 loss: 0.60342 training acc: 0.79372 testing acc: 0.7942\n",
      "epoch: 3 iter: 111 loss: 0.61215 training acc: 0.78994 testing acc: 0.789\n",
      "epoch: 3 iter: 112 loss: 0.62801 training acc: 0.78034 testing acc: 0.7806\n",
      "epoch: 3 iter: 113 loss: 0.60803 training acc: 0.79278 testing acc: 0.7938\n",
      "epoch: 3 iter: 114 loss: 0.59096 training acc: 0.79858 testing acc: 0.799\n",
      "epoch: 3 iter: 115 loss: 0.61046 training acc: 0.79472 testing acc: 0.7948\n",
      "epoch: 3 iter: 116 loss: 0.60459 training acc: 0.79718 testing acc: 0.8004\n",
      "epoch: 3 iter: 117 loss: 0.58485 training acc: 0.81016 testing acc: 0.809\n",
      "epoch: 3 iter: 118 loss: 0.58832 training acc: 0.81528 testing acc: 0.8186\n",
      "epoch: 3 iter: 119 loss: 0.58593 training acc: 0.82222 testing acc: 0.824\n",
      "epoch: 3 iter: 120 loss: 0.58214 training acc: 0.82152 testing acc: 0.8234\n",
      "epoch: 3 iter: 121 loss: 0.58032 training acc: 0.83190 testing acc: 0.8374\n",
      "epoch: 3 iter: 122 loss: 0.60567 training acc: 0.83576 testing acc: 0.8392\n",
      "epoch: 3 iter: 123 loss: 0.59846 training acc: 0.83712 testing acc: 0.8402\n",
      "epoch: 3 iter: 124 loss: 0.58743 training acc: 0.84286 testing acc: 0.8476\n",
      "epoch: 3 iter: 125 loss: 0.58171 training acc: 0.85230 testing acc: 0.8526\n",
      "epoch: 3 iter: 126 loss: 0.58290 training acc: 0.85344 testing acc: 0.8532\n",
      "epoch: 3 iter: 127 loss: 0.56668 training acc: 0.85890 testing acc: 0.8594\n",
      "epoch: 3 iter: 128 loss: 0.56554 training acc: 0.85864 testing acc: 0.8574\n",
      "epoch: 3 iter: 129 loss: 0.57372 training acc: 0.85666 testing acc: 0.8574\n",
      "epoch: 3 iter: 130 loss: 0.58847 training acc: 0.84724 testing acc: 0.847\n",
      "epoch: 3 iter: 131 loss: 0.57156 training acc: 0.85864 testing acc: 0.8606\n",
      "epoch: 3 iter: 132 loss: 0.57631 training acc: 0.85320 testing acc: 0.8536\n",
      "epoch: 3 iter: 133 loss: 0.58037 training acc: 0.84908 testing acc: 0.8468\n",
      "epoch: 3 iter: 134 loss: 0.57136 training acc: 0.84674 testing acc: 0.8422\n",
      "epoch: 3 iter: 135 loss: 0.56159 training acc: 0.85824 testing acc: 0.8546\n",
      "epoch: 3 iter: 136 loss: 0.58630 training acc: 0.84326 testing acc: 0.8424\n",
      "epoch: 3 iter: 137 loss: 0.58272 training acc: 0.84128 testing acc: 0.836\n",
      "epoch: 3 iter: 138 loss: 0.57797 training acc: 0.84640 testing acc: 0.8404\n",
      "epoch: 3 iter: 139 loss: 0.57037 training acc: 0.85388 testing acc: 0.8516\n",
      "epoch: 3 iter: 140 loss: 0.56981 training acc: 0.85200 testing acc: 0.8522\n",
      "epoch: 3 iter: 141 loss: 0.61777 training acc: 0.83664 testing acc: 0.8396\n",
      "epoch: 3 iter: 142 loss: 0.56393 training acc: 0.85618 testing acc: 0.8576\n",
      "epoch: 3 iter: 143 loss: 0.55886 training acc: 0.85620 testing acc: 0.8564\n",
      "epoch: 3 iter: 144 loss: 0.60341 training acc: 0.84074 testing acc: 0.8412\n",
      "epoch: 3 iter: 145 loss: 0.58634 training acc: 0.84368 testing acc: 0.8436\n",
      "epoch: 3 iter: 146 loss: 0.56961 training acc: 0.85708 testing acc: 0.8596\n",
      "epoch: 3 iter: 147 loss: 0.55066 training acc: 0.86366 testing acc: 0.8648\n",
      "epoch: 3 iter: 148 loss: 0.57643 training acc: 0.84352 testing acc: 0.846\n",
      "epoch: 3 iter: 149 loss: 0.55184 training acc: 0.86110 testing acc: 0.8564\n",
      "epoch: 3 iter: 150 loss: 0.56061 training acc: 0.85844 testing acc: 0.8576\n",
      "epoch: 3 iter: 151 loss: 0.55975 training acc: 0.85626 testing acc: 0.853\n",
      "epoch: 3 iter: 152 loss: 0.54361 training acc: 0.86380 testing acc: 0.8624\n",
      "epoch: 3 iter: 153 loss: 0.54570 training acc: 0.86256 testing acc: 0.8612\n",
      "epoch: 3 iter: 154 loss: 0.54998 training acc: 0.85930 testing acc: 0.8566\n",
      "epoch: 3 iter: 155 loss: 0.57064 training acc: 0.85178 testing acc: 0.8486\n",
      "epoch: 3 iter: 156 loss: 0.54470 training acc: 0.86534 testing acc: 0.8638\n",
      "epoch: 3 iter: 157 loss: 0.58542 training acc: 0.84870 testing acc: 0.845\n",
      "epoch: 3 iter: 158 loss: 0.57398 training acc: 0.85288 testing acc: 0.8474\n",
      "epoch: 3 iter: 159 loss: 0.55854 training acc: 0.86232 testing acc: 0.8564\n",
      "epoch: 3 iter: 160 loss: 0.54843 training acc: 0.86286 testing acc: 0.8574\n",
      "epoch: 3 iter: 161 loss: 0.53221 training acc: 0.85344 testing acc: 0.851\n",
      "epoch: 3 iter: 162 loss: 0.54072 training acc: 0.83576 testing acc: 0.8344\n",
      "epoch: 3 iter: 163 loss: 0.53790 training acc: 0.84666 testing acc: 0.846\n",
      "epoch: 3 iter: 164 loss: 0.55709 training acc: 0.82406 testing acc: 0.8228\n",
      "epoch: 3 iter: 165 loss: 0.55824 training acc: 0.81078 testing acc: 0.8028\n",
      "epoch: 3 iter: 166 loss: 0.55237 training acc: 0.81198 testing acc: 0.8088\n",
      "epoch: 3 iter: 167 loss: 0.55565 training acc: 0.81156 testing acc: 0.8082\n",
      "epoch: 3 iter: 168 loss: 0.53423 training acc: 0.82370 testing acc: 0.8216\n",
      "epoch: 3 iter: 169 loss: 0.52200 training acc: 0.86170 testing acc: 0.8588\n",
      "epoch: 3 iter: 170 loss: 0.56680 training acc: 0.85238 testing acc: 0.8504\n",
      "epoch: 3 iter: 171 loss: 0.52746 training acc: 0.86730 testing acc: 0.8614\n",
      "epoch: 3 iter: 172 loss: 0.52631 training acc: 0.86750 testing acc: 0.8624\n",
      "epoch: 3 iter: 173 loss: 0.52910 training acc: 0.86610 testing acc: 0.8646\n",
      "epoch: 3 iter: 174 loss: 0.60689 training acc: 0.84052 testing acc: 0.8402\n",
      "epoch: 3 iter: 175 loss: 0.54319 training acc: 0.86424 testing acc: 0.8624\n",
      "epoch: 3 iter: 176 loss: 0.51481 training acc: 0.87112 testing acc: 0.867\n",
      "epoch: 3 iter: 177 loss: 0.54229 training acc: 0.85970 testing acc: 0.85\n",
      "epoch: 3 iter: 178 loss: 0.51897 training acc: 0.86934 testing acc: 0.866\n",
      "epoch: 3 iter: 179 loss: 0.55965 training acc: 0.84458 testing acc: 0.8426\n",
      "epoch: 3 iter: 180 loss: 0.51529 training acc: 0.87030 testing acc: 0.8684\n",
      "epoch: 3 iter: 181 loss: 0.56315 training acc: 0.84280 testing acc: 0.8368\n",
      "epoch: 3 iter: 182 loss: 0.51967 training acc: 0.86558 testing acc: 0.858\n",
      "epoch: 3 iter: 183 loss: 0.52807 training acc: 0.86080 testing acc: 0.8566\n",
      "epoch: 3 iter: 184 loss: 0.51597 training acc: 0.86850 testing acc: 0.8646\n",
      "epoch: 3 iter: 185 loss: 0.51461 training acc: 0.87082 testing acc: 0.8628\n",
      "epoch: 3 iter: 186 loss: 0.51426 training acc: 0.87292 testing acc: 0.8686\n",
      "epoch: 3 iter: 187 loss: 0.53318 training acc: 0.85978 testing acc: 0.8524\n",
      "epoch: 3 iter: 188 loss: 0.51249 training acc: 0.87164 testing acc: 0.8666\n",
      "epoch: 3 iter: 189 loss: 0.51522 training acc: 0.87070 testing acc: 0.8664\n",
      "epoch: 3 iter: 190 loss: 0.51479 training acc: 0.87086 testing acc: 0.8694\n",
      "epoch: 3 iter: 191 loss: 0.50333 training acc: 0.87400 testing acc: 0.8714\n",
      "epoch: 3 iter: 192 loss: 0.52084 training acc: 0.86182 testing acc: 0.8598\n",
      "epoch: 3 iter: 193 loss: 0.51256 training acc: 0.86940 testing acc: 0.8662\n",
      "epoch: 3 iter: 194 loss: 0.50076 training acc: 0.87506 testing acc: 0.8728\n",
      "epoch: 3 iter: 195 loss: 0.53160 training acc: 0.85880 testing acc: 0.8512\n",
      "epoch: 3 iter: 196 loss: 0.52416 training acc: 0.86786 testing acc: 0.8664\n",
      "epoch: 3 iter: 197 loss: 0.56621 training acc: 0.84268 testing acc: 0.8342\n",
      "epoch: 3 iter: 198 loss: 0.51564 training acc: 0.87100 testing acc: 0.8666\n",
      "epoch: 3 iter: 199 loss: 0.51506 training acc: 0.87266 testing acc: 0.8684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 iter: 200 loss: 0.53663 training acc: 0.85796 testing acc: 0.8508\n",
      "epoch: 3 iter: 201 loss: 0.49821 training acc: 0.87698 testing acc: 0.8698\n",
      "epoch: 3 iter: 202 loss: 0.50095 training acc: 0.87328 testing acc: 0.8718\n",
      "epoch: 3 iter: 203 loss: 0.52925 training acc: 0.86590 testing acc: 0.8622\n",
      "epoch: 3 iter: 204 loss: 0.53935 training acc: 0.86170 testing acc: 0.8564\n",
      "epoch: 3 iter: 205 loss: 0.49705 training acc: 0.87488 testing acc: 0.8686\n",
      "epoch: 3 iter: 206 loss: 0.50083 training acc: 0.87386 testing acc: 0.8734\n",
      "epoch: 3 iter: 207 loss: 0.50946 training acc: 0.86996 testing acc: 0.8664\n",
      "epoch: 3 iter: 208 loss: 0.49325 training acc: 0.87766 testing acc: 0.8718\n",
      "epoch: 3 iter: 209 loss: 0.49291 training acc: 0.87736 testing acc: 0.8696\n",
      "epoch: 3 iter: 210 loss: 0.55325 training acc: 0.85432 testing acc: 0.8506\n",
      "epoch: 3 iter: 211 loss: 0.49720 training acc: 0.86972 testing acc: 0.8626\n",
      "epoch: 3 iter: 212 loss: 0.50389 training acc: 0.87144 testing acc: 0.868\n",
      "epoch: 3 iter: 213 loss: 0.49282 training acc: 0.87142 testing acc: 0.8662\n",
      "epoch: 3 iter: 214 loss: 0.51467 training acc: 0.86440 testing acc: 0.8594\n",
      "epoch: 3 iter: 215 loss: 0.53176 training acc: 0.84240 testing acc: 0.8442\n",
      "epoch: 3 iter: 216 loss: 0.49746 training acc: 0.87338 testing acc: 0.8642\n",
      "epoch: 3 iter: 217 loss: 0.50676 training acc: 0.86794 testing acc: 0.8578\n",
      "epoch: 3 iter: 218 loss: 0.48153 training acc: 0.87992 testing acc: 0.87\n",
      "epoch: 3 iter: 219 loss: 0.48630 training acc: 0.87440 testing acc: 0.869\n",
      "epoch: 3 iter: 220 loss: 0.49241 training acc: 0.87540 testing acc: 0.8708\n",
      "epoch: 3 iter: 221 loss: 0.49565 training acc: 0.86940 testing acc: 0.8666\n",
      "epoch: 3 iter: 222 loss: 0.49689 training acc: 0.87208 testing acc: 0.8692\n",
      "epoch: 3 iter: 223 loss: 0.48115 training acc: 0.87922 testing acc: 0.876\n",
      "epoch: 3 iter: 224 loss: 0.48799 training acc: 0.88032 testing acc: 0.874\n",
      "epoch: 3 iter: 225 loss: 0.50823 training acc: 0.87282 testing acc: 0.8696\n",
      "epoch: 3 iter: 226 loss: 0.49912 training acc: 0.87008 testing acc: 0.857\n",
      "epoch: 3 iter: 227 loss: 0.48936 training acc: 0.87502 testing acc: 0.864\n",
      "epoch: 3 iter: 228 loss: 0.47257 training acc: 0.88400 testing acc: 0.8778\n",
      "epoch: 3 iter: 229 loss: 0.48314 training acc: 0.87868 testing acc: 0.8716\n",
      "epoch: 3 iter: 230 loss: 0.49159 training acc: 0.87450 testing acc: 0.8722\n",
      "epoch: 3 iter: 231 loss: 0.49858 training acc: 0.87368 testing acc: 0.8678\n",
      "epoch: 3 iter: 232 loss: 0.48408 training acc: 0.87768 testing acc: 0.8684\n",
      "epoch: 3 iter: 233 loss: 0.48009 training acc: 0.88088 testing acc: 0.875\n",
      "epoch: 3 iter: 234 loss: 0.48830 training acc: 0.87688 testing acc: 0.872\n",
      "epoch: 3 iter: 235 loss: 0.45927 training acc: 0.88538 testing acc: 0.8802\n",
      "epoch: 3 iter: 236 loss: 0.46455 training acc: 0.88264 testing acc: 0.8786\n",
      "epoch: 3 iter: 237 loss: 0.47613 training acc: 0.87950 testing acc: 0.872\n",
      "epoch: 3 iter: 238 loss: 0.46393 training acc: 0.88316 testing acc: 0.8742\n",
      "epoch: 3 iter: 239 loss: 0.46044 training acc: 0.88336 testing acc: 0.8756\n",
      "epoch: 3 iter: 240 loss: 0.45970 training acc: 0.88256 testing acc: 0.8816\n",
      "epoch: 3 iter: 241 loss: 0.48384 training acc: 0.87406 testing acc: 0.8682\n",
      "epoch: 3 iter: 242 loss: 0.46812 training acc: 0.88170 testing acc: 0.8766\n",
      "epoch: 3 iter: 243 loss: 0.49272 training acc: 0.87262 testing acc: 0.872\n",
      "epoch: 3 iter: 244 loss: 0.51285 training acc: 0.86068 testing acc: 0.8596\n",
      "epoch: 3 iter: 245 loss: 0.46213 training acc: 0.88318 testing acc: 0.8822\n",
      "epoch: 3 iter: 246 loss: 0.49848 training acc: 0.86836 testing acc: 0.8682\n",
      "epoch: 3 iter: 247 loss: 0.48966 training acc: 0.87460 testing acc: 0.8702\n",
      "epoch: 3 iter: 248 loss: 0.45910 training acc: 0.88418 testing acc: 0.8828\n",
      "epoch: 3 iter: 249 loss: 0.46699 training acc: 0.87874 testing acc: 0.8796\n",
      "epoch: 3 iter: 250 loss: 0.47220 training acc: 0.87388 testing acc: 0.8662\n",
      "epoch: 3 iter: 251 loss: 0.49185 training acc: 0.86474 testing acc: 0.8562\n",
      "epoch: 3 iter: 252 loss: 0.46278 training acc: 0.88084 testing acc: 0.8756\n",
      "epoch: 3 iter: 253 loss: 0.47470 training acc: 0.87718 testing acc: 0.8714\n",
      "epoch: 3 iter: 254 loss: 0.46647 training acc: 0.87818 testing acc: 0.8706\n",
      "epoch: 3 iter: 255 loss: 0.45434 training acc: 0.88388 testing acc: 0.8788\n",
      "epoch: 3 iter: 256 loss: 0.51097 training acc: 0.86130 testing acc: 0.854\n",
      "epoch: 3 iter: 257 loss: 0.45916 training acc: 0.87768 testing acc: 0.8688\n",
      "epoch: 3 iter: 258 loss: 0.46462 training acc: 0.87576 testing acc: 0.8704\n",
      "epoch: 3 iter: 259 loss: 0.47077 training acc: 0.87070 testing acc: 0.8632\n",
      "epoch: 3 iter: 260 loss: 0.44355 training acc: 0.88392 testing acc: 0.8798\n",
      "epoch: 3 iter: 261 loss: 0.44590 training acc: 0.88304 testing acc: 0.8776\n",
      "epoch: 3 iter: 262 loss: 0.48753 training acc: 0.86234 testing acc: 0.8562\n",
      "epoch: 3 iter: 263 loss: 0.46595 training acc: 0.87400 testing acc: 0.8678\n",
      "epoch: 3 iter: 264 loss: 0.45782 training acc: 0.87518 testing acc: 0.8684\n",
      "epoch: 3 iter: 265 loss: 0.48340 training acc: 0.86500 testing acc: 0.858\n",
      "epoch: 3 iter: 266 loss: 0.46619 training acc: 0.87656 testing acc: 0.8702\n",
      "epoch: 3 iter: 267 loss: 0.45131 training acc: 0.87866 testing acc: 0.8712\n",
      "epoch: 3 iter: 268 loss: 0.43885 training acc: 0.88322 testing acc: 0.876\n",
      "epoch: 3 iter: 269 loss: 0.45856 training acc: 0.87440 testing acc: 0.866\n",
      "epoch: 3 iter: 270 loss: 0.43465 training acc: 0.88768 testing acc: 0.8812\n",
      "epoch: 3 iter: 271 loss: 0.43795 training acc: 0.88520 testing acc: 0.8758\n",
      "epoch: 3 iter: 272 loss: 0.43944 training acc: 0.88420 testing acc: 0.8726\n",
      "epoch: 3 iter: 273 loss: 0.45969 training acc: 0.87448 testing acc: 0.8702\n",
      "epoch: 3 iter: 274 loss: 0.49675 training acc: 0.86650 testing acc: 0.856\n",
      "epoch: 3 iter: 275 loss: 0.44475 training acc: 0.88358 testing acc: 0.8738\n",
      "epoch: 3 iter: 276 loss: 0.43656 training acc: 0.88674 testing acc: 0.884\n",
      "epoch: 3 iter: 277 loss: 0.42932 training acc: 0.88832 testing acc: 0.8856\n",
      "epoch: 3 iter: 278 loss: 0.49239 training acc: 0.86548 testing acc: 0.8606\n",
      "epoch: 3 iter: 279 loss: 0.49982 training acc: 0.85832 testing acc: 0.8486\n",
      "epoch: 3 iter: 280 loss: 0.45556 training acc: 0.87838 testing acc: 0.8782\n",
      "epoch: 3 iter: 281 loss: 0.42379 training acc: 0.88998 testing acc: 0.885\n",
      "epoch: 3 iter: 282 loss: 0.42216 training acc: 0.88906 testing acc: 0.8832\n",
      "epoch: 3 iter: 283 loss: 0.42279 training acc: 0.88934 testing acc: 0.8822\n",
      "epoch: 3 iter: 284 loss: 0.43422 training acc: 0.88294 testing acc: 0.8706\n",
      "epoch: 3 iter: 285 loss: 0.46150 training acc: 0.87602 testing acc: 0.8616\n",
      "epoch: 3 iter: 286 loss: 0.43353 training acc: 0.88270 testing acc: 0.876\n",
      "epoch: 3 iter: 287 loss: 0.45683 training acc: 0.87406 testing acc: 0.866\n",
      "epoch: 3 iter: 288 loss: 0.43034 training acc: 0.88594 testing acc: 0.8758\n",
      "epoch: 3 iter: 289 loss: 0.43355 training acc: 0.88380 testing acc: 0.8716\n",
      "epoch: 3 iter: 290 loss: 0.45556 training acc: 0.87794 testing acc: 0.8652\n",
      "epoch: 3 iter: 291 loss: 0.44897 training acc: 0.87562 testing acc: 0.8706\n",
      "epoch: 3 iter: 292 loss: 0.42906 training acc: 0.88494 testing acc: 0.877\n",
      "epoch: 3 iter: 293 loss: 0.41700 training acc: 0.89120 testing acc: 0.8854\n",
      "epoch: 3 iter: 294 loss: 0.41155 training acc: 0.89298 testing acc: 0.8844\n",
      "epoch: 3 iter: 295 loss: 0.41326 training acc: 0.89130 testing acc: 0.885\n",
      "epoch: 3 iter: 296 loss: 0.41755 training acc: 0.88776 testing acc: 0.8786\n",
      "epoch: 3 iter: 297 loss: 0.42034 training acc: 0.88976 testing acc: 0.882\n",
      "epoch: 3 iter: 298 loss: 0.47185 training acc: 0.87000 testing acc: 0.8564\n",
      "epoch: 3 iter: 299 loss: 0.42668 training acc: 0.88604 testing acc: 0.8762\n",
      "epoch: 3 iter: 300 loss: 0.42007 training acc: 0.88902 testing acc: 0.8802\n",
      "epoch: 3 iter: 301 loss: 0.44246 training acc: 0.87976 testing acc: 0.8684\n",
      "epoch: 3 iter: 302 loss: 0.41078 training acc: 0.88952 testing acc: 0.8798\n",
      "epoch: 3 iter: 303 loss: 0.43602 training acc: 0.88082 testing acc: 0.8702\n",
      "epoch: 3 iter: 304 loss: 0.42168 training acc: 0.88540 testing acc: 0.8764\n",
      "epoch: 3 iter: 305 loss: 0.44208 training acc: 0.87718 testing acc: 0.8734\n",
      "epoch: 3 iter: 306 loss: 0.42019 training acc: 0.88506 testing acc: 0.8734\n",
      "epoch: 3 iter: 307 loss: 0.42378 training acc: 0.88222 testing acc: 0.8772\n",
      "epoch: 3 iter: 308 loss: 0.43993 training acc: 0.87698 testing acc: 0.872\n",
      "epoch: 3 iter: 309 loss: 0.42387 training acc: 0.88836 testing acc: 0.8774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 iter: 310 loss: 0.41358 training acc: 0.89108 testing acc: 0.8826\n",
      "epoch: 3 iter: 311 loss: 0.40626 training acc: 0.89098 testing acc: 0.8838\n",
      "epoch: 3 iter: 312 loss: 0.41276 training acc: 0.88790 testing acc: 0.8776\n",
      "epoch: 3 iter: 313 loss: 0.40572 training acc: 0.89230 testing acc: 0.8822\n",
      "epoch: 3 iter: 314 loss: 0.41160 training acc: 0.89248 testing acc: 0.885\n",
      "epoch: 3 iter: 315 loss: 0.42325 training acc: 0.88610 testing acc: 0.88\n",
      "epoch: 3 iter: 316 loss: 0.40170 training acc: 0.89440 testing acc: 0.8866\n",
      "epoch: 3 iter: 317 loss: 0.40379 training acc: 0.89148 testing acc: 0.8836\n",
      "epoch: 3 iter: 318 loss: 0.42061 training acc: 0.88492 testing acc: 0.8776\n",
      "epoch: 3 iter: 319 loss: 0.42206 training acc: 0.88176 testing acc: 0.8742\n",
      "epoch: 3 iter: 320 loss: 0.40740 training acc: 0.88840 testing acc: 0.8776\n",
      "epoch: 3 iter: 321 loss: 0.40635 training acc: 0.89246 testing acc: 0.8864\n",
      "epoch: 3 iter: 322 loss: 0.46668 training acc: 0.86444 testing acc: 0.8562\n",
      "epoch: 3 iter: 323 loss: 0.65030 training acc: 0.80402 testing acc: 0.7924\n",
      "epoch: 3 iter: 324 loss: 0.39870 training acc: 0.89226 testing acc: 0.8846\n",
      "epoch: 3 iter: 325 loss: 0.40632 training acc: 0.88770 testing acc: 0.8788\n",
      "epoch: 3 iter: 326 loss: 0.39370 training acc: 0.89398 testing acc: 0.8876\n",
      "epoch: 3 iter: 327 loss: 0.39487 training acc: 0.89510 testing acc: 0.8828\n",
      "epoch: 3 iter: 328 loss: 0.39903 training acc: 0.89282 testing acc: 0.8826\n",
      "epoch: 3 iter: 329 loss: 0.40360 training acc: 0.89050 testing acc: 0.8814\n",
      "epoch: 3 iter: 330 loss: 0.41920 training acc: 0.88172 testing acc: 0.874\n",
      "epoch: 3 iter: 331 loss: 0.42352 training acc: 0.88006 testing acc: 0.8682\n",
      "epoch: 3 iter: 332 loss: 0.40183 training acc: 0.89028 testing acc: 0.8778\n",
      "epoch: 3 iter: 333 loss: 0.39359 training acc: 0.89472 testing acc: 0.8864\n",
      "epoch: 3 iter: 334 loss: 0.39882 training acc: 0.89336 testing acc: 0.8838\n",
      "epoch: 3 iter: 335 loss: 0.40556 training acc: 0.89006 testing acc: 0.8822\n",
      "epoch: 3 iter: 336 loss: 0.39211 training acc: 0.89536 testing acc: 0.8854\n",
      "epoch: 3 iter: 337 loss: 0.40802 training acc: 0.89028 testing acc: 0.8818\n",
      "epoch: 3 iter: 338 loss: 0.41049 training acc: 0.88560 testing acc: 0.8804\n",
      "epoch: 3 iter: 339 loss: 0.39738 training acc: 0.89066 testing acc: 0.8838\n",
      "epoch: 3 iter: 340 loss: 0.43858 training acc: 0.87858 testing acc: 0.863\n",
      "epoch: 3 iter: 341 loss: 0.46145 training acc: 0.86974 testing acc: 0.8524\n",
      "epoch: 3 iter: 342 loss: 0.40638 training acc: 0.89028 testing acc: 0.8798\n",
      "epoch: 3 iter: 343 loss: 0.38596 training acc: 0.89596 testing acc: 0.8846\n",
      "epoch: 3 iter: 344 loss: 0.38332 training acc: 0.89520 testing acc: 0.8858\n",
      "epoch: 3 iter: 345 loss: 0.40763 training acc: 0.88520 testing acc: 0.875\n",
      "epoch: 3 iter: 346 loss: 0.38888 training acc: 0.89420 testing acc: 0.8874\n",
      "epoch: 3 iter: 347 loss: 0.44546 training acc: 0.86904 testing acc: 0.8662\n",
      "epoch: 3 iter: 348 loss: 0.39707 training acc: 0.89400 testing acc: 0.8874\n",
      "epoch: 3 iter: 349 loss: 0.39367 training acc: 0.89318 testing acc: 0.8832\n",
      "epoch: 3 iter: 350 loss: 0.38150 training acc: 0.89840 testing acc: 0.8904\n",
      "epoch: 3 iter: 351 loss: 0.41547 training acc: 0.88270 testing acc: 0.8722\n",
      "epoch: 3 iter: 352 loss: 0.38686 training acc: 0.89626 testing acc: 0.8832\n",
      "epoch: 3 iter: 353 loss: 0.38399 training acc: 0.89796 testing acc: 0.89\n",
      "epoch: 3 iter: 354 loss: 0.40958 training acc: 0.89062 testing acc: 0.875\n",
      "epoch: 3 iter: 355 loss: 0.38944 training acc: 0.89648 testing acc: 0.8862\n",
      "epoch: 3 iter: 356 loss: 0.37686 training acc: 0.89996 testing acc: 0.8896\n",
      "epoch: 3 iter: 357 loss: 0.38788 training acc: 0.89394 testing acc: 0.8832\n",
      "epoch: 3 iter: 358 loss: 0.38258 training acc: 0.89640 testing acc: 0.8842\n",
      "epoch: 3 iter: 359 loss: 0.38415 training acc: 0.89524 testing acc: 0.888\n",
      "epoch: 3 iter: 360 loss: 0.41978 training acc: 0.87986 testing acc: 0.8738\n",
      "epoch: 3 iter: 361 loss: 0.40147 training acc: 0.88450 testing acc: 0.8756\n",
      "epoch: 3 iter: 362 loss: 0.40812 training acc: 0.88524 testing acc: 0.874\n",
      "epoch: 3 iter: 363 loss: 0.40103 training acc: 0.88770 testing acc: 0.8748\n",
      "epoch: 3 iter: 364 loss: 0.41562 training acc: 0.88020 testing acc: 0.871\n",
      "epoch: 3 iter: 365 loss: 0.38874 training acc: 0.89186 testing acc: 0.8832\n",
      "epoch: 3 iter: 366 loss: 0.38229 training acc: 0.89748 testing acc: 0.8902\n",
      "epoch: 3 iter: 367 loss: 0.40363 training acc: 0.88684 testing acc: 0.8766\n",
      "epoch: 3 iter: 368 loss: 0.38879 training acc: 0.89294 testing acc: 0.8802\n",
      "epoch: 3 iter: 369 loss: 0.37807 training acc: 0.89706 testing acc: 0.887\n",
      "epoch: 3 iter: 370 loss: 0.39975 training acc: 0.88654 testing acc: 0.8804\n",
      "epoch: 3 iter: 371 loss: 0.38451 training acc: 0.89408 testing acc: 0.8892\n",
      "epoch: 3 iter: 372 loss: 0.37216 training acc: 0.89858 testing acc: 0.8924\n",
      "epoch: 3 iter: 373 loss: 0.41917 training acc: 0.88612 testing acc: 0.882\n",
      "epoch: 3 iter: 374 loss: 0.37620 training acc: 0.89620 testing acc: 0.8858\n",
      "epoch: 3 iter: 375 loss: 0.37360 training acc: 0.89822 testing acc: 0.889\n",
      "epoch: 3 iter: 376 loss: 0.41085 training acc: 0.88332 testing acc: 0.8712\n",
      "epoch: 3 iter: 377 loss: 0.38209 training acc: 0.89332 testing acc: 0.885\n",
      "epoch: 3 iter: 378 loss: 0.38382 training acc: 0.89418 testing acc: 0.8894\n",
      "epoch: 3 iter: 379 loss: 0.37768 training acc: 0.89830 testing acc: 0.8906\n",
      "epoch: 3 iter: 380 loss: 0.37566 training acc: 0.89736 testing acc: 0.8888\n",
      "epoch: 3 iter: 381 loss: 0.38480 training acc: 0.89468 testing acc: 0.8878\n",
      "epoch: 3 iter: 382 loss: 0.37266 training acc: 0.89926 testing acc: 0.8916\n",
      "epoch: 3 iter: 383 loss: 0.38048 training acc: 0.89412 testing acc: 0.887\n",
      "epoch: 3 iter: 384 loss: 0.39126 training acc: 0.88850 testing acc: 0.8812\n",
      "epoch: 3 iter: 385 loss: 0.37440 training acc: 0.89790 testing acc: 0.8878\n",
      "epoch: 3 iter: 386 loss: 0.36968 training acc: 0.90084 testing acc: 0.8908\n",
      "epoch: 3 iter: 387 loss: 0.40922 training acc: 0.88972 testing acc: 0.8818\n",
      "epoch: 3 iter: 388 loss: 0.39410 training acc: 0.89544 testing acc: 0.8818\n",
      "epoch: 3 iter: 389 loss: 0.40560 training acc: 0.88746 testing acc: 0.875\n",
      "epoch: 3 iter: 390 loss: 0.39155 training acc: 0.89454 testing acc: 0.8814\n",
      "epoch: 3 iter: 391 loss: 0.38407 training acc: 0.89946 testing acc: 0.8858\n",
      "epoch: 3 iter: 392 loss: 0.38792 training acc: 0.89526 testing acc: 0.8852\n",
      "epoch: 3 iter: 393 loss: 0.38447 training acc: 0.89734 testing acc: 0.8882\n",
      "epoch: 3 iter: 394 loss: 0.39362 training acc: 0.89048 testing acc: 0.8826\n",
      "epoch: 3 iter: 395 loss: 0.36580 training acc: 0.90262 testing acc: 0.8876\n",
      "epoch: 3 iter: 396 loss: 0.37169 training acc: 0.89978 testing acc: 0.8898\n",
      "epoch: 3 iter: 397 loss: 0.39075 training acc: 0.89082 testing acc: 0.8858\n",
      "epoch: 3 iter: 398 loss: 0.38159 training acc: 0.89442 testing acc: 0.884\n",
      "epoch: 3 iter: 399 loss: 0.41982 training acc: 0.88742 testing acc: 0.8754\n",
      "epoch: 3 iter: 400 loss: 0.38601 training acc: 0.89220 testing acc: 0.8844\n",
      "epoch: 3 iter: 401 loss: 0.37365 training acc: 0.89670 testing acc: 0.8886\n",
      "epoch: 3 iter: 402 loss: 0.38373 training acc: 0.89846 testing acc: 0.8878\n",
      "epoch: 3 iter: 403 loss: 0.39358 training acc: 0.88798 testing acc: 0.8844\n",
      "epoch: 3 iter: 404 loss: 0.35498 training acc: 0.90474 testing acc: 0.8962\n",
      "epoch: 3 iter: 405 loss: 0.35782 training acc: 0.90362 testing acc: 0.8946\n",
      "epoch: 3 iter: 406 loss: 0.37227 training acc: 0.89932 testing acc: 0.8898\n",
      "epoch: 3 iter: 407 loss: 0.38113 training acc: 0.89700 testing acc: 0.8856\n",
      "epoch: 3 iter: 408 loss: 0.37880 training acc: 0.89692 testing acc: 0.891\n",
      "epoch: 3 iter: 409 loss: 0.37186 training acc: 0.89880 testing acc: 0.889\n",
      "epoch: 3 iter: 410 loss: 0.41307 training acc: 0.87504 testing acc: 0.8712\n",
      "epoch: 3 iter: 411 loss: 0.37303 training acc: 0.89708 testing acc: 0.8866\n",
      "epoch: 3 iter: 412 loss: 0.37368 training acc: 0.89712 testing acc: 0.8932\n",
      "epoch: 3 iter: 413 loss: 0.35914 training acc: 0.90348 testing acc: 0.8944\n",
      "epoch: 3 iter: 414 loss: 0.36826 training acc: 0.89930 testing acc: 0.8914\n",
      "epoch: 3 iter: 415 loss: 0.36328 training acc: 0.89966 testing acc: 0.8922\n",
      "epoch: 3 iter: 416 loss: 0.36063 training acc: 0.90174 testing acc: 0.8922\n",
      "epoch: 3 iter: 417 loss: 0.36200 training acc: 0.90246 testing acc: 0.8962\n",
      "epoch: 3 iter: 418 loss: 0.36908 training acc: 0.89664 testing acc: 0.888\n",
      "epoch: 3 iter: 419 loss: 0.38446 training acc: 0.89296 testing acc: 0.8852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 iter: 420 loss: 0.36355 training acc: 0.90372 testing acc: 0.8938\n",
      "epoch: 3 iter: 421 loss: 0.35709 training acc: 0.90184 testing acc: 0.8922\n",
      "epoch: 3 iter: 422 loss: 0.37905 training acc: 0.89142 testing acc: 0.8856\n",
      "epoch: 3 iter: 423 loss: 0.37757 training acc: 0.89274 testing acc: 0.884\n",
      "epoch: 3 iter: 424 loss: 0.38443 training acc: 0.89072 testing acc: 0.8794\n",
      "epoch: 3 iter: 425 loss: 0.35555 training acc: 0.90402 testing acc: 0.8918\n",
      "epoch: 3 iter: 426 loss: 0.36978 training acc: 0.89534 testing acc: 0.8848\n",
      "epoch: 3 iter: 427 loss: 0.41590 training acc: 0.87756 testing acc: 0.8692\n",
      "epoch: 3 iter: 428 loss: 0.43740 training acc: 0.87944 testing acc: 0.8666\n",
      "epoch: 3 iter: 429 loss: 0.39205 training acc: 0.88758 testing acc: 0.8776\n",
      "epoch: 3 iter: 430 loss: 0.36110 training acc: 0.90038 testing acc: 0.89\n",
      "epoch: 3 iter: 431 loss: 0.36411 training acc: 0.89758 testing acc: 0.8908\n",
      "epoch: 3 iter: 432 loss: 0.36147 training acc: 0.89858 testing acc: 0.892\n",
      "epoch: 3 iter: 433 loss: 0.37495 training acc: 0.89640 testing acc: 0.8872\n",
      "epoch: 3 iter: 434 loss: 0.39317 training acc: 0.88850 testing acc: 0.8782\n",
      "epoch: 3 iter: 435 loss: 0.37276 training acc: 0.89636 testing acc: 0.8816\n",
      "epoch: 3 iter: 436 loss: 0.36326 training acc: 0.89768 testing acc: 0.8874\n",
      "epoch: 3 iter: 437 loss: 0.36183 training acc: 0.89804 testing acc: 0.8864\n",
      "epoch: 3 iter: 438 loss: 0.41039 training acc: 0.88190 testing acc: 0.871\n",
      "epoch: 3 iter: 439 loss: 0.37303 training acc: 0.89910 testing acc: 0.8914\n",
      "epoch: 3 iter: 440 loss: 0.39256 training acc: 0.88960 testing acc: 0.8824\n",
      "epoch: 3 iter: 441 loss: 0.36036 training acc: 0.90046 testing acc: 0.8922\n",
      "epoch: 3 iter: 442 loss: 0.35441 training acc: 0.90350 testing acc: 0.8936\n",
      "epoch: 3 iter: 443 loss: 0.36064 training acc: 0.89942 testing acc: 0.89\n",
      "epoch: 3 iter: 444 loss: 0.36821 training acc: 0.89640 testing acc: 0.8912\n",
      "epoch: 3 iter: 445 loss: 0.35848 training acc: 0.90118 testing acc: 0.8944\n",
      "epoch: 3 iter: 446 loss: 0.35206 training acc: 0.90372 testing acc: 0.8938\n",
      "epoch: 3 iter: 447 loss: 0.39291 training acc: 0.88884 testing acc: 0.8834\n",
      "epoch: 3 iter: 448 loss: 0.34966 training acc: 0.90388 testing acc: 0.8908\n",
      "epoch: 3 iter: 449 loss: 0.34080 training acc: 0.90726 testing acc: 0.8954\n",
      "epoch: 3 iter: 450 loss: 0.35049 training acc: 0.90584 testing acc: 0.896\n",
      "epoch: 3 iter: 451 loss: 0.36946 training acc: 0.89672 testing acc: 0.8854\n",
      "epoch: 3 iter: 452 loss: 0.41843 training acc: 0.87954 testing acc: 0.8688\n",
      "epoch: 3 iter: 453 loss: 0.35345 training acc: 0.90454 testing acc: 0.8938\n",
      "epoch: 3 iter: 454 loss: 0.35571 training acc: 0.90154 testing acc: 0.8906\n",
      "epoch: 3 iter: 455 loss: 0.40217 training acc: 0.88166 testing acc: 0.867\n",
      "epoch: 3 iter: 456 loss: 0.36174 training acc: 0.90008 testing acc: 0.8934\n",
      "epoch: 3 iter: 457 loss: 0.39556 training acc: 0.88308 testing acc: 0.8782\n",
      "epoch: 3 iter: 458 loss: 0.36188 training acc: 0.89670 testing acc: 0.8894\n",
      "epoch: 3 iter: 459 loss: 0.35574 training acc: 0.89950 testing acc: 0.8886\n",
      "epoch: 3 iter: 460 loss: 0.34769 training acc: 0.90274 testing acc: 0.8934\n",
      "epoch: 3 iter: 461 loss: 0.34952 training acc: 0.90438 testing acc: 0.89\n",
      "epoch: 3 iter: 462 loss: 0.33833 training acc: 0.90964 testing acc: 0.8972\n",
      "epoch: 3 iter: 463 loss: 0.34628 training acc: 0.90508 testing acc: 0.8894\n",
      "epoch: 3 iter: 464 loss: 0.33352 training acc: 0.91056 testing acc: 0.8956\n",
      "epoch: 3 iter: 465 loss: 0.38460 training acc: 0.88742 testing acc: 0.8834\n",
      "epoch: 3 iter: 466 loss: 0.34582 training acc: 0.90372 testing acc: 0.8944\n",
      "epoch: 3 iter: 467 loss: 0.33442 training acc: 0.90840 testing acc: 0.8964\n",
      "epoch: 3 iter: 468 loss: 0.33933 training acc: 0.90680 testing acc: 0.893\n",
      "epoch: 3 iter: 469 loss: 0.33936 training acc: 0.90742 testing acc: 0.8942\n",
      "epoch: 3 iter: 470 loss: 0.37527 training acc: 0.89694 testing acc: 0.8882\n",
      "epoch: 3 iter: 471 loss: 0.37388 training acc: 0.89388 testing acc: 0.8824\n",
      "epoch: 3 iter: 472 loss: 0.38199 training acc: 0.89196 testing acc: 0.8796\n",
      "epoch: 3 iter: 473 loss: 0.39486 training acc: 0.89004 testing acc: 0.8824\n",
      "epoch: 3 iter: 474 loss: 0.39743 training acc: 0.88172 testing acc: 0.8774\n",
      "epoch: 3 iter: 475 loss: 0.34880 training acc: 0.90218 testing acc: 0.8972\n",
      "epoch: 3 iter: 476 loss: 0.35813 training acc: 0.89884 testing acc: 0.894\n",
      "epoch: 3 iter: 477 loss: 0.34095 training acc: 0.90542 testing acc: 0.8986\n",
      "epoch: 3 iter: 478 loss: 0.34468 training acc: 0.90708 testing acc: 0.9004\n",
      "epoch: 3 iter: 479 loss: 0.35290 training acc: 0.90162 testing acc: 0.8896\n",
      "epoch: 3 iter: 480 loss: 0.33672 training acc: 0.90576 testing acc: 0.8942\n",
      "epoch: 3 iter: 481 loss: 0.34940 training acc: 0.90116 testing acc: 0.8888\n",
      "epoch: 3 iter: 482 loss: 0.33944 training acc: 0.90906 testing acc: 0.8978\n",
      "epoch: 3 iter: 483 loss: 0.35226 training acc: 0.89872 testing acc: 0.8856\n",
      "epoch: 3 iter: 484 loss: 0.35448 training acc: 0.89630 testing acc: 0.8874\n",
      "epoch: 3 iter: 485 loss: 0.35597 training acc: 0.89856 testing acc: 0.8892\n",
      "epoch: 3 iter: 486 loss: 0.33023 training acc: 0.90854 testing acc: 0.8986\n",
      "epoch: 3 iter: 487 loss: 0.35049 training acc: 0.90074 testing acc: 0.8978\n",
      "epoch: 3 iter: 488 loss: 0.33604 training acc: 0.90874 testing acc: 0.9014\n",
      "epoch: 3 iter: 489 loss: 0.36621 training acc: 0.89422 testing acc: 0.8856\n",
      "epoch: 3 iter: 490 loss: 0.35553 training acc: 0.90044 testing acc: 0.891\n",
      "epoch: 3 iter: 491 loss: 0.34506 training acc: 0.90250 testing acc: 0.8934\n",
      "epoch: 3 iter: 492 loss: 0.38908 training acc: 0.88658 testing acc: 0.8758\n",
      "epoch: 3 iter: 493 loss: 0.35258 training acc: 0.89680 testing acc: 0.8918\n",
      "epoch: 3 iter: 494 loss: 0.32570 training acc: 0.90860 testing acc: 0.8992\n",
      "epoch: 3 iter: 495 loss: 0.32593 training acc: 0.90974 testing acc: 0.903\n",
      "epoch: 3 iter: 496 loss: 0.35566 training acc: 0.89488 testing acc: 0.888\n",
      "epoch: 3 iter: 497 loss: 0.34111 training acc: 0.90156 testing acc: 0.8948\n",
      "epoch: 3 iter: 498 loss: 0.32446 training acc: 0.91048 testing acc: 0.9026\n",
      "epoch: 3 iter: 499 loss: 0.32868 training acc: 0.90892 testing acc: 0.8998\n",
      "epoch: 3 iter: 500 loss: 0.32998 training acc: 0.90914 testing acc: 0.9022\n",
      "epoch: 3 iter: 501 loss: 0.34000 training acc: 0.90496 testing acc: 0.8976\n",
      "epoch: 3 iter: 502 loss: 0.34386 training acc: 0.90758 testing acc: 0.8986\n",
      "epoch: 3 iter: 503 loss: 0.33758 training acc: 0.90492 testing acc: 0.8954\n",
      "epoch: 3 iter: 504 loss: 0.33463 training acc: 0.90782 testing acc: 0.9008\n",
      "epoch: 3 iter: 505 loss: 0.33330 training acc: 0.90916 testing acc: 0.9026\n",
      "epoch: 3 iter: 506 loss: 0.33764 training acc: 0.90792 testing acc: 0.8984\n",
      "epoch: 3 iter: 507 loss: 0.32839 training acc: 0.91144 testing acc: 0.9038\n",
      "epoch: 3 iter: 508 loss: 0.35235 training acc: 0.90514 testing acc: 0.901\n",
      "epoch: 3 iter: 509 loss: 0.33985 training acc: 0.90896 testing acc: 0.903\n",
      "epoch: 3 iter: 510 loss: 0.33273 training acc: 0.91094 testing acc: 0.8982\n",
      "epoch: 3 iter: 511 loss: 0.34881 training acc: 0.90276 testing acc: 0.9\n",
      "epoch: 3 iter: 512 loss: 0.32574 training acc: 0.91190 testing acc: 0.905\n",
      "epoch: 3 iter: 513 loss: 0.33167 training acc: 0.90890 testing acc: 0.9008\n",
      "epoch: 3 iter: 514 loss: 0.32963 training acc: 0.90800 testing acc: 0.904\n",
      "epoch: 3 iter: 515 loss: 0.33144 training acc: 0.90784 testing acc: 0.9024\n",
      "epoch: 3 iter: 516 loss: 0.33808 training acc: 0.90540 testing acc: 0.8984\n",
      "epoch: 3 iter: 517 loss: 0.32569 training acc: 0.90972 testing acc: 0.9026\n",
      "epoch: 3 iter: 518 loss: 0.32523 training acc: 0.91202 testing acc: 0.9048\n",
      "epoch: 3 iter: 519 loss: 0.32904 training acc: 0.91138 testing acc: 0.9042\n",
      "epoch: 3 iter: 520 loss: 0.34119 training acc: 0.90736 testing acc: 0.8976\n",
      "epoch: 3 iter: 521 loss: 0.32190 training acc: 0.91176 testing acc: 0.8992\n",
      "epoch: 3 iter: 522 loss: 0.32744 training acc: 0.90832 testing acc: 0.896\n",
      "epoch: 3 iter: 523 loss: 0.33442 training acc: 0.90460 testing acc: 0.8976\n",
      "epoch: 3 iter: 524 loss: 0.32259 training acc: 0.91220 testing acc: 0.9028\n",
      "epoch: 3 iter: 525 loss: 0.33739 training acc: 0.90566 testing acc: 0.8982\n",
      "epoch: 3 iter: 526 loss: 0.37951 training acc: 0.89148 testing acc: 0.8866\n",
      "epoch: 3 iter: 527 loss: 0.36300 training acc: 0.89686 testing acc: 0.8912\n",
      "epoch: 3 iter: 528 loss: 0.32195 training acc: 0.91088 testing acc: 0.9034\n",
      "epoch: 3 iter: 529 loss: 0.32292 training acc: 0.91224 testing acc: 0.9066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 iter: 530 loss: 0.39487 training acc: 0.87966 testing acc: 0.876\n",
      "epoch: 3 iter: 531 loss: 0.38927 training acc: 0.88088 testing acc: 0.875\n",
      "epoch: 3 iter: 532 loss: 0.35493 training acc: 0.89594 testing acc: 0.8906\n",
      "epoch: 3 iter: 533 loss: 0.31370 training acc: 0.91350 testing acc: 0.9044\n",
      "epoch: 3 iter: 534 loss: 0.32410 training acc: 0.90988 testing acc: 0.9028\n",
      "epoch: 3 iter: 535 loss: 0.31496 training acc: 0.91384 testing acc: 0.9082\n",
      "epoch: 3 iter: 536 loss: 0.32993 training acc: 0.91032 testing acc: 0.9064\n",
      "epoch: 3 iter: 537 loss: 0.33628 training acc: 0.91048 testing acc: 0.9036\n",
      "epoch: 3 iter: 538 loss: 0.34544 training acc: 0.90856 testing acc: 0.8986\n",
      "epoch: 3 iter: 539 loss: 0.31696 training acc: 0.91442 testing acc: 0.9052\n",
      "epoch: 3 iter: 540 loss: 0.31767 training acc: 0.91338 testing acc: 0.9044\n",
      "epoch: 3 iter: 541 loss: 0.31541 training acc: 0.91286 testing acc: 0.9028\n",
      "epoch: 3 iter: 542 loss: 0.33644 training acc: 0.90302 testing acc: 0.8902\n",
      "epoch: 3 iter: 543 loss: 0.35627 training acc: 0.89492 testing acc: 0.8824\n",
      "epoch: 3 iter: 544 loss: 0.31060 training acc: 0.91226 testing acc: 0.9018\n",
      "epoch: 3 iter: 545 loss: 0.35857 training acc: 0.89350 testing acc: 0.8844\n",
      "epoch: 3 iter: 546 loss: 0.31727 training acc: 0.91046 testing acc: 0.8992\n",
      "epoch: 3 iter: 547 loss: 0.31477 training acc: 0.91326 testing acc: 0.9046\n",
      "epoch: 3 iter: 548 loss: 0.31065 training acc: 0.91478 testing acc: 0.9042\n",
      "epoch: 3 iter: 549 loss: 0.33410 training acc: 0.90366 testing acc: 0.8962\n",
      "epoch: 3 iter: 550 loss: 0.31618 training acc: 0.90928 testing acc: 0.8996\n",
      "epoch: 3 iter: 551 loss: 0.32101 training acc: 0.90664 testing acc: 0.8988\n",
      "epoch: 3 iter: 552 loss: 0.33595 training acc: 0.90414 testing acc: 0.8962\n",
      "epoch: 3 iter: 553 loss: 0.32622 training acc: 0.90950 testing acc: 0.9022\n",
      "epoch: 3 iter: 554 loss: 0.33674 training acc: 0.90206 testing acc: 0.8978\n",
      "epoch: 3 iter: 555 loss: 0.34321 training acc: 0.89964 testing acc: 0.8914\n",
      "epoch: 3 iter: 556 loss: 0.35834 training acc: 0.89284 testing acc: 0.8844\n",
      "epoch: 3 iter: 557 loss: 0.33025 training acc: 0.90436 testing acc: 0.8956\n",
      "epoch: 3 iter: 558 loss: 0.30954 training acc: 0.91362 testing acc: 0.9044\n",
      "epoch: 3 iter: 559 loss: 0.33642 training acc: 0.90702 testing acc: 0.9022\n",
      "epoch: 3 iter: 560 loss: 0.32475 training acc: 0.90848 testing acc: 0.9028\n",
      "epoch: 3 iter: 561 loss: 0.31961 training acc: 0.91188 testing acc: 0.9034\n",
      "epoch: 3 iter: 562 loss: 0.31497 training acc: 0.91310 testing acc: 0.9048\n",
      "epoch: 3 iter: 563 loss: 0.33601 training acc: 0.90580 testing acc: 0.897\n",
      "epoch: 3 iter: 564 loss: 0.30644 training acc: 0.91654 testing acc: 0.9032\n",
      "epoch: 3 iter: 565 loss: 0.30830 training acc: 0.91586 testing acc: 0.906\n",
      "epoch: 3 iter: 566 loss: 0.31135 training acc: 0.91230 testing acc: 0.9032\n",
      "epoch: 3 iter: 567 loss: 0.36153 training acc: 0.89334 testing acc: 0.8868\n",
      "epoch: 3 iter: 568 loss: 0.35251 training acc: 0.89938 testing acc: 0.8888\n",
      "epoch: 3 iter: 569 loss: 0.31160 training acc: 0.91228 testing acc: 0.9032\n",
      "epoch: 3 iter: 570 loss: 0.32152 training acc: 0.91086 testing acc: 0.9004\n",
      "epoch: 3 iter: 571 loss: 0.31371 training acc: 0.91068 testing acc: 0.9018\n",
      "epoch: 3 iter: 572 loss: 0.33057 training acc: 0.90532 testing acc: 0.8952\n",
      "epoch: 3 iter: 573 loss: 0.30153 training acc: 0.91644 testing acc: 0.9036\n",
      "epoch: 3 iter: 574 loss: 0.30491 training acc: 0.91478 testing acc: 0.9058\n",
      "epoch: 3 iter: 575 loss: 0.32625 training acc: 0.90768 testing acc: 0.8952\n",
      "epoch: 3 iter: 576 loss: 0.31752 training acc: 0.90906 testing acc: 0.9042\n",
      "epoch: 3 iter: 577 loss: 0.34248 training acc: 0.89846 testing acc: 0.896\n",
      "epoch: 3 iter: 578 loss: 0.30752 training acc: 0.91236 testing acc: 0.9054\n",
      "epoch: 3 iter: 579 loss: 0.31991 training acc: 0.90768 testing acc: 0.8998\n",
      "epoch: 3 iter: 580 loss: 0.32589 training acc: 0.90578 testing acc: 0.8962\n",
      "epoch: 3 iter: 581 loss: 0.30662 training acc: 0.91408 testing acc: 0.9056\n",
      "epoch: 3 iter: 582 loss: 0.31899 training acc: 0.91220 testing acc: 0.9062\n",
      "epoch: 3 iter: 583 loss: 0.30929 training acc: 0.91414 testing acc: 0.9084\n",
      "epoch: 3 iter: 584 loss: 0.30190 training acc: 0.91860 testing acc: 0.9124\n",
      "epoch: 3 iter: 585 loss: 0.29769 training acc: 0.91734 testing acc: 0.9058\n",
      "epoch: 3 iter: 586 loss: 0.30983 training acc: 0.91274 testing acc: 0.901\n",
      "epoch: 3 iter: 587 loss: 0.29964 training acc: 0.91680 testing acc: 0.9056\n",
      "epoch: 3 iter: 588 loss: 0.30839 training acc: 0.91672 testing acc: 0.9062\n",
      "epoch: 3 iter: 589 loss: 0.32599 training acc: 0.90602 testing acc: 0.8972\n",
      "epoch: 3 iter: 590 loss: 0.32931 training acc: 0.90258 testing acc: 0.897\n",
      "epoch: 3 iter: 591 loss: 0.31246 training acc: 0.91362 testing acc: 0.9062\n",
      "epoch: 3 iter: 592 loss: 0.33336 training acc: 0.90514 testing acc: 0.895\n",
      "epoch: 3 iter: 593 loss: 0.30562 training acc: 0.91286 testing acc: 0.901\n",
      "epoch: 3 iter: 594 loss: 0.30321 training acc: 0.91436 testing acc: 0.9044\n",
      "epoch: 3 iter: 595 loss: 0.30256 training acc: 0.91438 testing acc: 0.9044\n",
      "epoch: 3 iter: 596 loss: 0.32021 training acc: 0.90642 testing acc: 0.8946\n",
      "epoch: 3 iter: 597 loss: 0.29753 training acc: 0.91642 testing acc: 0.9068\n",
      "epoch: 3 iter: 598 loss: 0.30205 training acc: 0.91508 testing acc: 0.904\n",
      "epoch: 3 iter: 599 loss: 0.31550 training acc: 0.90972 testing acc: 0.8998\n",
      "epoch: 3 iter: 600 loss: 0.30363 training acc: 0.91610 testing acc: 0.906\n",
      "epoch: 3 iter: 601 loss: 0.30455 training acc: 0.91612 testing acc: 0.9102\n",
      "epoch: 3 iter: 602 loss: 0.29326 training acc: 0.91972 testing acc: 0.9106\n",
      "epoch: 3 iter: 603 loss: 0.29269 training acc: 0.91902 testing acc: 0.9114\n",
      "epoch: 3 iter: 604 loss: 0.31114 training acc: 0.91414 testing acc: 0.9056\n",
      "epoch: 3 iter: 605 loss: 0.30697 training acc: 0.91364 testing acc: 0.905\n",
      "epoch: 3 iter: 606 loss: 0.30190 training acc: 0.91724 testing acc: 0.915\n",
      "epoch: 3 iter: 607 loss: 0.29909 training acc: 0.92022 testing acc: 0.911\n",
      "epoch: 3 iter: 608 loss: 0.29578 training acc: 0.91786 testing acc: 0.9144\n",
      "epoch: 3 iter: 609 loss: 0.30465 training acc: 0.91780 testing acc: 0.9134\n",
      "epoch: 3 iter: 610 loss: 0.30747 training acc: 0.91692 testing acc: 0.9072\n",
      "epoch: 3 iter: 611 loss: 0.31087 training acc: 0.91212 testing acc: 0.9058\n",
      "epoch: 3 iter: 612 loss: 0.30287 training acc: 0.91498 testing acc: 0.9064\n",
      "epoch: 3 iter: 613 loss: 0.29370 training acc: 0.91994 testing acc: 0.9094\n",
      "epoch: 3 iter: 614 loss: 0.30578 training acc: 0.91434 testing acc: 0.9046\n",
      "epoch: 3 iter: 615 loss: 0.33432 training acc: 0.90090 testing acc: 0.8924\n",
      "epoch: 3 iter: 616 loss: 0.30874 training acc: 0.90982 testing acc: 0.9028\n",
      "epoch: 3 iter: 617 loss: 0.31263 training acc: 0.90946 testing acc: 0.9026\n",
      "epoch: 3 iter: 618 loss: 0.32200 training acc: 0.90524 testing acc: 0.8992\n",
      "epoch: 3 iter: 619 loss: 0.32397 training acc: 0.90230 testing acc: 0.8962\n",
      "epoch: 3 iter: 620 loss: 0.31351 training acc: 0.91036 testing acc: 0.9036\n",
      "epoch: 3 iter: 621 loss: 0.30674 training acc: 0.91222 testing acc: 0.9056\n",
      "epoch: 3 iter: 622 loss: 0.31964 training acc: 0.90320 testing acc: 0.898\n",
      "epoch: 3 iter: 623 loss: 0.32312 training acc: 0.90666 testing acc: 0.898\n",
      "epoch: 3 iter: 624 loss: 0.32117 training acc: 0.90608 testing acc: 0.8948\n",
      "epoch: 3 iter: 625 loss: 0.31325 training acc: 0.90838 testing acc: 0.9018\n",
      "epoch: 3 iter: 626 loss: 0.32849 training acc: 0.90408 testing acc: 0.8966\n",
      "epoch: 3 iter: 627 loss: 0.31789 training acc: 0.90824 testing acc: 0.8986\n",
      "epoch: 3 iter: 628 loss: 0.30655 training acc: 0.91172 testing acc: 0.9024\n",
      "epoch: 3 iter: 629 loss: 0.30123 training acc: 0.91444 testing acc: 0.9058\n",
      "epoch: 3 iter: 630 loss: 0.31053 training acc: 0.91340 testing acc: 0.9046\n",
      "epoch: 3 iter: 631 loss: 0.30692 training acc: 0.91248 testing acc: 0.9006\n",
      "epoch: 3 iter: 632 loss: 0.32752 training acc: 0.90384 testing acc: 0.9\n",
      "epoch: 3 iter: 633 loss: 0.28995 training acc: 0.92128 testing acc: 0.9152\n",
      "epoch: 3 iter: 634 loss: 0.29189 training acc: 0.91982 testing acc: 0.9102\n",
      "epoch: 3 iter: 635 loss: 0.29340 training acc: 0.91616 testing acc: 0.9082\n",
      "epoch: 3 iter: 636 loss: 0.28564 training acc: 0.92056 testing acc: 0.9128\n",
      "epoch: 3 iter: 637 loss: 0.28666 training acc: 0.91986 testing acc: 0.9138\n",
      "epoch: 3 iter: 638 loss: 0.29102 training acc: 0.91836 testing acc: 0.908\n",
      "epoch: 3 iter: 639 loss: 0.28582 training acc: 0.92088 testing acc: 0.9144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 iter: 640 loss: 0.28740 training acc: 0.91878 testing acc: 0.915\n",
      "epoch: 3 iter: 641 loss: 0.29966 training acc: 0.91380 testing acc: 0.9072\n",
      "epoch: 3 iter: 642 loss: 0.30400 training acc: 0.91394 testing acc: 0.9072\n",
      "epoch: 3 iter: 643 loss: 0.29912 training acc: 0.91668 testing acc: 0.9082\n",
      "epoch: 3 iter: 644 loss: 0.29629 training acc: 0.91480 testing acc: 0.9084\n",
      "epoch: 3 iter: 645 loss: 0.32044 training acc: 0.90638 testing acc: 0.8956\n",
      "epoch: 3 iter: 646 loss: 0.30362 training acc: 0.91346 testing acc: 0.9022\n",
      "epoch: 3 iter: 647 loss: 0.32575 training acc: 0.90336 testing acc: 0.8932\n",
      "epoch: 3 iter: 648 loss: 0.30334 training acc: 0.91214 testing acc: 0.9042\n",
      "epoch: 3 iter: 649 loss: 0.30064 training acc: 0.91384 testing acc: 0.904\n",
      "epoch: 3 iter: 650 loss: 0.29781 training acc: 0.91554 testing acc: 0.9088\n",
      "epoch: 3 iter: 651 loss: 0.28864 training acc: 0.91954 testing acc: 0.9092\n",
      "epoch: 3 iter: 652 loss: 0.30312 training acc: 0.91628 testing acc: 0.906\n",
      "epoch: 3 iter: 653 loss: 0.30433 training acc: 0.91420 testing acc: 0.9048\n",
      "epoch: 3 iter: 654 loss: 0.28816 training acc: 0.91978 testing acc: 0.9096\n",
      "epoch: 3 iter: 655 loss: 0.30147 training acc: 0.91480 testing acc: 0.906\n",
      "epoch: 3 iter: 656 loss: 0.31688 training acc: 0.90858 testing acc: 0.9036\n",
      "epoch: 3 iter: 657 loss: 0.28791 training acc: 0.92016 testing acc: 0.9116\n",
      "epoch: 3 iter: 658 loss: 0.29884 training acc: 0.91354 testing acc: 0.9068\n",
      "epoch: 3 iter: 659 loss: 0.29559 training acc: 0.91736 testing acc: 0.9096\n",
      "epoch: 3 iter: 660 loss: 0.29985 training acc: 0.91598 testing acc: 0.9068\n",
      "epoch: 3 iter: 661 loss: 0.28525 training acc: 0.91982 testing acc: 0.9142\n",
      "epoch: 3 iter: 662 loss: 0.29877 training acc: 0.91512 testing acc: 0.9098\n",
      "epoch: 3 iter: 663 loss: 0.29086 training acc: 0.91626 testing acc: 0.9068\n",
      "epoch: 3 iter: 664 loss: 0.28438 training acc: 0.92186 testing acc: 0.913\n",
      "epoch: 3 iter: 665 loss: 0.28218 training acc: 0.92116 testing acc: 0.913\n",
      "epoch: 3 iter: 666 loss: 0.28970 training acc: 0.91744 testing acc: 0.9088\n",
      "epoch: 3 iter: 667 loss: 0.29969 training acc: 0.91488 testing acc: 0.906\n",
      "epoch: 3 iter: 668 loss: 0.30132 training acc: 0.91234 testing acc: 0.903\n",
      "epoch: 3 iter: 669 loss: 0.29768 training acc: 0.91478 testing acc: 0.9072\n",
      "epoch: 3 iter: 670 loss: 0.28147 training acc: 0.92004 testing acc: 0.9122\n",
      "epoch: 3 iter: 671 loss: 0.28912 training acc: 0.91816 testing acc: 0.911\n",
      "epoch: 3 iter: 672 loss: 0.28682 training acc: 0.91720 testing acc: 0.911\n",
      "epoch: 3 iter: 673 loss: 0.29062 training acc: 0.91558 testing acc: 0.9096\n",
      "epoch: 3 iter: 674 loss: 0.30522 training acc: 0.91164 testing acc: 0.9024\n",
      "epoch: 3 iter: 675 loss: 0.28377 training acc: 0.92088 testing acc: 0.9112\n",
      "epoch: 3 iter: 676 loss: 0.29834 training acc: 0.91514 testing acc: 0.9032\n",
      "epoch: 3 iter: 677 loss: 0.27614 training acc: 0.92308 testing acc: 0.9144\n",
      "epoch: 3 iter: 678 loss: 0.31074 training acc: 0.90738 testing acc: 0.901\n",
      "epoch: 3 iter: 679 loss: 0.29839 training acc: 0.91466 testing acc: 0.906\n",
      "epoch: 3 iter: 680 loss: 0.29634 training acc: 0.91654 testing acc: 0.9046\n",
      "epoch: 3 iter: 681 loss: 0.28154 training acc: 0.92192 testing acc: 0.914\n",
      "epoch: 3 iter: 682 loss: 0.28326 training acc: 0.92002 testing acc: 0.9124\n",
      "epoch: 3 iter: 683 loss: 0.28284 training acc: 0.92084 testing acc: 0.9128\n",
      "epoch: 3 iter: 684 loss: 0.28911 training acc: 0.91796 testing acc: 0.9092\n",
      "epoch: 3 iter: 685 loss: 0.27546 training acc: 0.92232 testing acc: 0.911\n",
      "epoch: 3 iter: 686 loss: 0.32976 training acc: 0.90498 testing acc: 0.894\n",
      "epoch: 3 iter: 687 loss: 0.30325 training acc: 0.91094 testing acc: 0.9\n",
      "epoch: 3 iter: 688 loss: 0.29980 training acc: 0.91480 testing acc: 0.9118\n",
      "epoch: 3 iter: 689 loss: 0.32703 training acc: 0.90274 testing acc: 0.8914\n",
      "epoch: 3 iter: 690 loss: 0.29425 training acc: 0.91568 testing acc: 0.9084\n",
      "epoch: 3 iter: 691 loss: 0.29211 training acc: 0.91864 testing acc: 0.9144\n",
      "epoch: 3 iter: 692 loss: 0.30328 training acc: 0.91288 testing acc: 0.9072\n",
      "epoch: 3 iter: 693 loss: 0.31811 training acc: 0.91072 testing acc: 0.8994\n",
      "epoch: 3 iter: 694 loss: 0.28970 training acc: 0.91898 testing acc: 0.9134\n",
      "epoch: 3 iter: 695 loss: 0.28222 training acc: 0.92274 testing acc: 0.9142\n",
      "epoch: 3 iter: 696 loss: 0.27328 training acc: 0.92358 testing acc: 0.9198\n",
      "epoch: 3 iter: 697 loss: 0.28100 training acc: 0.92176 testing acc: 0.9184\n",
      "epoch: 3 iter: 698 loss: 0.27578 training acc: 0.92390 testing acc: 0.9176\n",
      "epoch: 3 iter: 699 loss: 0.29001 training acc: 0.91964 testing acc: 0.9136\n",
      "epoch: 3 iter: 700 loss: 0.29491 training acc: 0.91660 testing acc: 0.9056\n",
      "epoch: 3 iter: 701 loss: 0.29037 training acc: 0.91826 testing acc: 0.9092\n",
      "epoch: 3 iter: 702 loss: 0.28395 training acc: 0.92102 testing acc: 0.9118\n",
      "epoch: 3 iter: 703 loss: 0.27644 training acc: 0.92238 testing acc: 0.9162\n",
      "epoch: 3 iter: 704 loss: 0.31715 training acc: 0.90698 testing acc: 0.8962\n",
      "epoch: 3 iter: 705 loss: 0.29166 training acc: 0.91502 testing acc: 0.9094\n",
      "epoch: 3 iter: 706 loss: 0.29154 training acc: 0.91642 testing acc: 0.9094\n",
      "epoch: 3 iter: 707 loss: 0.29922 training acc: 0.91550 testing acc: 0.905\n",
      "epoch: 3 iter: 708 loss: 0.29574 training acc: 0.91496 testing acc: 0.905\n",
      "epoch: 3 iter: 709 loss: 0.29191 training acc: 0.91606 testing acc: 0.9024\n",
      "epoch: 3 iter: 710 loss: 0.28366 training acc: 0.92060 testing acc: 0.9114\n",
      "epoch: 3 iter: 711 loss: 0.27637 training acc: 0.92186 testing acc: 0.9116\n",
      "epoch: 3 iter: 712 loss: 0.27749 training acc: 0.92222 testing acc: 0.9126\n",
      "epoch: 3 iter: 713 loss: 0.28810 training acc: 0.91930 testing acc: 0.9076\n",
      "epoch: 3 iter: 714 loss: 0.27483 training acc: 0.92166 testing acc: 0.9118\n",
      "epoch: 3 iter: 715 loss: 0.27600 training acc: 0.92376 testing acc: 0.9176\n",
      "epoch: 3 iter: 716 loss: 0.28986 training acc: 0.91748 testing acc: 0.9134\n",
      "epoch: 3 iter: 717 loss: 0.27791 training acc: 0.92248 testing acc: 0.9188\n",
      "epoch: 3 iter: 718 loss: 0.27464 training acc: 0.92308 testing acc: 0.9176\n",
      "epoch: 3 iter: 719 loss: 0.28049 training acc: 0.92188 testing acc: 0.9172\n",
      "epoch: 3 iter: 720 loss: 0.27904 training acc: 0.92114 testing acc: 0.9158\n",
      "epoch: 3 iter: 721 loss: 0.31751 training acc: 0.90776 testing acc: 0.8968\n",
      "epoch: 3 iter: 722 loss: 0.31023 training acc: 0.91202 testing acc: 0.9028\n",
      "epoch: 3 iter: 723 loss: 0.28882 training acc: 0.92042 testing acc: 0.913\n",
      "epoch: 3 iter: 724 loss: 0.28286 training acc: 0.92336 testing acc: 0.9158\n",
      "epoch: 3 iter: 725 loss: 0.28048 training acc: 0.91902 testing acc: 0.911\n",
      "epoch: 3 iter: 726 loss: 0.28267 training acc: 0.92008 testing acc: 0.9122\n",
      "epoch: 3 iter: 727 loss: 0.26575 training acc: 0.92576 testing acc: 0.9184\n",
      "epoch: 3 iter: 728 loss: 0.27049 training acc: 0.92284 testing acc: 0.9144\n",
      "epoch: 3 iter: 729 loss: 0.26724 training acc: 0.92520 testing acc: 0.9166\n",
      "epoch: 3 iter: 730 loss: 0.27107 training acc: 0.92382 testing acc: 0.9152\n",
      "epoch: 3 iter: 731 loss: 0.27419 training acc: 0.92270 testing acc: 0.9128\n",
      "epoch: 3 iter: 732 loss: 0.30247 training acc: 0.91332 testing acc: 0.9012\n",
      "epoch: 3 iter: 733 loss: 0.27490 training acc: 0.92062 testing acc: 0.9102\n",
      "epoch: 3 iter: 734 loss: 0.27011 training acc: 0.92500 testing acc: 0.9176\n",
      "epoch: 3 iter: 735 loss: 0.28609 training acc: 0.91810 testing acc: 0.9074\n",
      "epoch: 3 iter: 736 loss: 0.28296 training acc: 0.91852 testing acc: 0.9064\n",
      "epoch: 3 iter: 737 loss: 0.29288 training acc: 0.91430 testing acc: 0.905\n",
      "epoch: 3 iter: 738 loss: 0.28844 training acc: 0.91906 testing acc: 0.9082\n",
      "epoch: 3 iter: 739 loss: 0.30862 training acc: 0.90774 testing acc: 0.8946\n",
      "epoch: 3 iter: 740 loss: 0.28202 training acc: 0.91916 testing acc: 0.9084\n",
      "epoch: 3 iter: 741 loss: 0.28337 training acc: 0.91832 testing acc: 0.9088\n",
      "epoch: 3 iter: 742 loss: 0.29461 training acc: 0.91066 testing acc: 0.904\n",
      "epoch: 3 iter: 743 loss: 0.27439 training acc: 0.92162 testing acc: 0.9138\n",
      "epoch: 3 iter: 744 loss: 0.27767 training acc: 0.92118 testing acc: 0.9094\n",
      "epoch: 3 iter: 745 loss: 0.27351 training acc: 0.92206 testing acc: 0.9106\n",
      "epoch: 3 iter: 746 loss: 0.27085 training acc: 0.92332 testing acc: 0.9132\n",
      "epoch: 3 iter: 747 loss: 0.26797 training acc: 0.92402 testing acc: 0.9122\n",
      "epoch: 3 iter: 748 loss: 0.26619 training acc: 0.92466 testing acc: 0.9172\n",
      "epoch: 3 iter: 749 loss: 0.29001 training acc: 0.91718 testing acc: 0.904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 iter: 750 loss: 0.27622 training acc: 0.92224 testing acc: 0.915\n",
      "epoch: 3 iter: 751 loss: 0.27326 training acc: 0.92310 testing acc: 0.915\n",
      "epoch: 3 iter: 752 loss: 0.27506 training acc: 0.92326 testing acc: 0.914\n",
      "epoch: 3 iter: 753 loss: 0.26779 training acc: 0.92280 testing acc: 0.9142\n",
      "epoch: 3 iter: 754 loss: 0.26635 training acc: 0.92414 testing acc: 0.9164\n",
      "epoch: 3 iter: 755 loss: 0.30465 training acc: 0.90990 testing acc: 0.9022\n",
      "epoch: 3 iter: 756 loss: 0.28216 training acc: 0.91862 testing acc: 0.9114\n",
      "epoch: 3 iter: 757 loss: 0.27881 training acc: 0.91850 testing acc: 0.911\n",
      "epoch: 3 iter: 758 loss: 0.28092 training acc: 0.91752 testing acc: 0.9108\n",
      "epoch: 3 iter: 759 loss: 0.27123 training acc: 0.92364 testing acc: 0.918\n",
      "epoch: 3 iter: 760 loss: 0.27196 training acc: 0.92198 testing acc: 0.916\n",
      "epoch: 3 iter: 761 loss: 0.27823 training acc: 0.92024 testing acc: 0.9134\n",
      "epoch: 3 iter: 762 loss: 0.27859 training acc: 0.92122 testing acc: 0.9154\n",
      "epoch: 3 iter: 763 loss: 0.28680 training acc: 0.91828 testing acc: 0.9134\n",
      "epoch: 3 iter: 764 loss: 0.28477 training acc: 0.91696 testing acc: 0.9124\n",
      "epoch: 3 iter: 765 loss: 0.30906 training acc: 0.90652 testing acc: 0.904\n",
      "epoch: 3 iter: 766 loss: 0.30682 training acc: 0.90828 testing acc: 0.8996\n",
      "epoch: 3 iter: 767 loss: 0.26403 training acc: 0.92524 testing acc: 0.9178\n",
      "epoch: 3 iter: 768 loss: 0.27956 training acc: 0.91802 testing acc: 0.9136\n",
      "epoch: 3 iter: 769 loss: 0.27534 training acc: 0.92064 testing acc: 0.9144\n",
      "epoch: 3 iter: 770 loss: 0.26985 training acc: 0.92202 testing acc: 0.9198\n",
      "epoch: 3 iter: 771 loss: 0.28341 training acc: 0.91710 testing acc: 0.9088\n",
      "epoch: 3 iter: 772 loss: 0.27348 training acc: 0.92108 testing acc: 0.9148\n",
      "epoch: 3 iter: 773 loss: 0.27177 training acc: 0.92354 testing acc: 0.9186\n",
      "epoch: 3 iter: 774 loss: 0.27771 training acc: 0.91910 testing acc: 0.9152\n",
      "epoch: 3 iter: 775 loss: 0.27614 training acc: 0.92132 testing acc: 0.9148\n",
      "epoch: 3 iter: 776 loss: 0.26727 training acc: 0.92486 testing acc: 0.9182\n",
      "epoch: 3 iter: 777 loss: 0.27541 training acc: 0.92278 testing acc: 0.9146\n",
      "epoch: 3 iter: 778 loss: 0.27842 training acc: 0.92088 testing acc: 0.9126\n",
      "epoch: 3 iter: 779 loss: 0.28105 training acc: 0.91882 testing acc: 0.9098\n",
      "epoch: 3 iter: 780 loss: 0.27088 training acc: 0.92324 testing acc: 0.9158\n",
      "epoch: 3 iter: 781 loss: 0.27714 training acc: 0.92170 testing acc: 0.9106\n",
      "epoch: 3 iter: 782 loss: 0.28429 training acc: 0.92076 testing acc: 0.9178\n",
      "epoch: 3 iter: 783 loss: 0.26737 training acc: 0.92432 testing acc: 0.9184\n",
      "epoch: 3 iter: 784 loss: 0.28604 training acc: 0.91754 testing acc: 0.9144\n",
      "epoch: 3 iter: 785 loss: 0.26804 training acc: 0.92242 testing acc: 0.9172\n",
      "epoch: 3 iter: 786 loss: 0.27910 training acc: 0.91640 testing acc: 0.9102\n",
      "epoch: 3 iter: 787 loss: 0.26505 training acc: 0.92344 testing acc: 0.9176\n",
      "epoch: 3 iter: 788 loss: 0.25807 training acc: 0.92598 testing acc: 0.9194\n",
      "epoch: 3 iter: 789 loss: 0.27307 training acc: 0.91932 testing acc: 0.9122\n",
      "epoch: 3 iter: 790 loss: 0.27022 training acc: 0.92220 testing acc: 0.9154\n",
      "epoch: 3 iter: 791 loss: 0.26549 training acc: 0.92432 testing acc: 0.916\n",
      "epoch: 3 iter: 792 loss: 0.27467 training acc: 0.92160 testing acc: 0.9158\n",
      "epoch: 3 iter: 793 loss: 0.26952 training acc: 0.92380 testing acc: 0.9154\n",
      "epoch: 3 iter: 794 loss: 0.27794 training acc: 0.91792 testing acc: 0.9102\n",
      "epoch: 3 iter: 795 loss: 0.27247 training acc: 0.92214 testing acc: 0.9164\n",
      "epoch: 3 iter: 796 loss: 0.26008 training acc: 0.92740 testing acc: 0.9212\n",
      "epoch: 3 iter: 797 loss: 0.30005 training acc: 0.91392 testing acc: 0.9076\n",
      "epoch: 3 iter: 798 loss: 0.26802 training acc: 0.92526 testing acc: 0.9186\n",
      "epoch: 3 iter: 799 loss: 0.26315 training acc: 0.92544 testing acc: 0.919\n",
      "epoch: 3 iter: 800 loss: 0.26065 training acc: 0.92750 testing acc: 0.922\n",
      "epoch: 3 iter: 801 loss: 0.28136 training acc: 0.92136 testing acc: 0.9114\n",
      "epoch: 3 iter: 802 loss: 0.26906 training acc: 0.92370 testing acc: 0.916\n",
      "epoch: 3 iter: 803 loss: 0.26747 training acc: 0.92476 testing acc: 0.917\n",
      "epoch: 3 iter: 804 loss: 0.26592 training acc: 0.92372 testing acc: 0.9174\n",
      "epoch: 3 iter: 805 loss: 0.25666 training acc: 0.92740 testing acc: 0.9192\n",
      "epoch: 3 iter: 806 loss: 0.27759 training acc: 0.92122 testing acc: 0.9188\n",
      "epoch: 3 iter: 807 loss: 0.28327 training acc: 0.91914 testing acc: 0.9142\n",
      "epoch: 3 iter: 808 loss: 0.26183 training acc: 0.92654 testing acc: 0.9216\n",
      "epoch: 3 iter: 809 loss: 0.30404 training acc: 0.91008 testing acc: 0.8984\n",
      "epoch: 3 iter: 810 loss: 0.27534 training acc: 0.92052 testing acc: 0.9128\n",
      "epoch: 3 iter: 811 loss: 0.26449 training acc: 0.92508 testing acc: 0.9178\n",
      "epoch: 3 iter: 812 loss: 0.29634 training acc: 0.91548 testing acc: 0.9106\n",
      "epoch: 3 iter: 813 loss: 0.26517 training acc: 0.92322 testing acc: 0.9184\n",
      "epoch: 3 iter: 814 loss: 0.25770 training acc: 0.92776 testing acc: 0.9234\n",
      "epoch: 3 iter: 815 loss: 0.25271 training acc: 0.92950 testing acc: 0.926\n",
      "epoch: 3 iter: 816 loss: 0.26912 training acc: 0.92512 testing acc: 0.92\n",
      "epoch: 3 iter: 817 loss: 0.27171 training acc: 0.92342 testing acc: 0.9164\n",
      "epoch: 3 iter: 818 loss: 0.25661 training acc: 0.92816 testing acc: 0.918\n",
      "epoch: 3 iter: 819 loss: 0.25624 training acc: 0.92804 testing acc: 0.9208\n",
      "epoch: 3 iter: 820 loss: 0.26183 training acc: 0.92638 testing acc: 0.9164\n",
      "epoch: 3 iter: 821 loss: 0.25579 training acc: 0.92872 testing acc: 0.9236\n",
      "epoch: 3 iter: 822 loss: 0.25823 training acc: 0.92846 testing acc: 0.92\n",
      "epoch: 3 iter: 823 loss: 0.25080 training acc: 0.93084 testing acc: 0.9248\n",
      "epoch: 3 iter: 824 loss: 0.25059 training acc: 0.93084 testing acc: 0.925\n",
      "epoch: 3 iter: 825 loss: 0.33436 training acc: 0.90024 testing acc: 0.8948\n",
      "epoch: 3 iter: 826 loss: 0.26139 training acc: 0.92392 testing acc: 0.9186\n",
      "epoch: 3 iter: 827 loss: 0.27370 training acc: 0.91982 testing acc: 0.914\n",
      "epoch: 3 iter: 828 loss: 0.26243 training acc: 0.92388 testing acc: 0.917\n",
      "epoch: 3 iter: 829 loss: 0.26644 training acc: 0.92406 testing acc: 0.919\n",
      "epoch: 3 iter: 830 loss: 0.25967 training acc: 0.92544 testing acc: 0.9194\n",
      "epoch: 3 iter: 831 loss: 0.28079 training acc: 0.91798 testing acc: 0.9152\n",
      "epoch: 3 iter: 832 loss: 0.25854 training acc: 0.92560 testing acc: 0.9202\n",
      "epoch: 3 iter: 833 loss: 0.31068 training acc: 0.90676 testing acc: 0.9002\n",
      "epoch: 3 iter: 834 loss: 0.25749 training acc: 0.92606 testing acc: 0.9176\n",
      "epoch: 3 iter: 835 loss: 0.25684 training acc: 0.92716 testing acc: 0.9176\n",
      "epoch: 3 iter: 836 loss: 0.26265 training acc: 0.92688 testing acc: 0.9178\n",
      "epoch: 3 iter: 837 loss: 0.26013 training acc: 0.92722 testing acc: 0.9186\n",
      "epoch: 3 iter: 838 loss: 0.27870 training acc: 0.91814 testing acc: 0.91\n",
      "epoch: 3 iter: 839 loss: 0.26749 training acc: 0.92398 testing acc: 0.917\n",
      "epoch: 3 iter: 840 loss: 0.26514 training acc: 0.92532 testing acc: 0.9174\n",
      "epoch: 3 iter: 841 loss: 0.29474 training acc: 0.91516 testing acc: 0.9062\n",
      "epoch: 3 iter: 842 loss: 0.25769 training acc: 0.92732 testing acc: 0.9208\n",
      "epoch: 3 iter: 843 loss: 0.25835 training acc: 0.92576 testing acc: 0.9172\n",
      "epoch: 3 iter: 844 loss: 0.25975 training acc: 0.92322 testing acc: 0.9202\n",
      "epoch: 3 iter: 845 loss: 0.27652 training acc: 0.91860 testing acc: 0.9126\n",
      "epoch: 3 iter: 846 loss: 0.25221 training acc: 0.92842 testing acc: 0.9234\n",
      "epoch: 3 iter: 847 loss: 0.25420 training acc: 0.92636 testing acc: 0.9198\n",
      "epoch: 3 iter: 848 loss: 0.24843 training acc: 0.92796 testing acc: 0.9214\n",
      "epoch: 3 iter: 849 loss: 0.24937 training acc: 0.92938 testing acc: 0.9222\n",
      "epoch: 3 iter: 850 loss: 0.27417 training acc: 0.92234 testing acc: 0.919\n",
      "epoch: 3 iter: 851 loss: 0.25036 training acc: 0.93032 testing acc: 0.924\n",
      "epoch: 3 iter: 852 loss: 0.28806 training acc: 0.91492 testing acc: 0.912\n",
      "epoch: 3 iter: 853 loss: 0.26621 training acc: 0.92164 testing acc: 0.9156\n",
      "epoch: 3 iter: 854 loss: 0.26278 training acc: 0.92420 testing acc: 0.9188\n",
      "epoch: 3 iter: 855 loss: 0.26286 training acc: 0.92382 testing acc: 0.9192\n",
      "epoch: 3 iter: 856 loss: 0.26484 training acc: 0.92284 testing acc: 0.9154\n",
      "epoch: 3 iter: 857 loss: 0.26070 training acc: 0.92374 testing acc: 0.9178\n",
      "epoch: 3 iter: 858 loss: 0.25570 training acc: 0.92730 testing acc: 0.9176\n",
      "epoch: 3 iter: 859 loss: 0.25089 training acc: 0.92824 testing acc: 0.9198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 iter: 860 loss: 0.24940 training acc: 0.92898 testing acc: 0.9218\n",
      "epoch: 3 iter: 861 loss: 0.26910 training acc: 0.92450 testing acc: 0.9176\n",
      "epoch: 3 iter: 862 loss: 0.27012 training acc: 0.92506 testing acc: 0.9192\n",
      "epoch: 3 iter: 863 loss: 0.24842 training acc: 0.92958 testing acc: 0.9238\n",
      "epoch: 3 iter: 864 loss: 0.25007 training acc: 0.92746 testing acc: 0.9234\n",
      "epoch: 3 iter: 865 loss: 0.29861 training acc: 0.91252 testing acc: 0.906\n",
      "epoch: 3 iter: 866 loss: 0.27772 training acc: 0.91940 testing acc: 0.9138\n",
      "epoch: 3 iter: 867 loss: 0.25128 training acc: 0.92824 testing acc: 0.922\n",
      "epoch: 3 iter: 868 loss: 0.25372 training acc: 0.92910 testing acc: 0.924\n",
      "epoch: 3 iter: 869 loss: 0.24588 training acc: 0.92882 testing acc: 0.9206\n",
      "epoch: 3 iter: 870 loss: 0.28586 training acc: 0.91860 testing acc: 0.9108\n",
      "epoch: 3 iter: 871 loss: 0.25031 training acc: 0.92906 testing acc: 0.9266\n",
      "epoch: 3 iter: 872 loss: 0.27510 training acc: 0.92004 testing acc: 0.913\n",
      "epoch: 3 iter: 873 loss: 0.27774 training acc: 0.91848 testing acc: 0.9094\n",
      "epoch: 3 iter: 874 loss: 0.25165 training acc: 0.92880 testing acc: 0.9218\n",
      "epoch: 3 iter: 875 loss: 0.26495 training acc: 0.92474 testing acc: 0.9194\n",
      "epoch: 3 iter: 876 loss: 0.25294 training acc: 0.92570 testing acc: 0.921\n",
      "epoch: 3 iter: 877 loss: 0.24776 training acc: 0.92918 testing acc: 0.9226\n",
      "epoch: 3 iter: 878 loss: 0.25031 training acc: 0.92846 testing acc: 0.9236\n",
      "epoch: 3 iter: 879 loss: 0.26227 training acc: 0.92714 testing acc: 0.9202\n",
      "epoch: 3 iter: 880 loss: 0.26104 training acc: 0.92642 testing acc: 0.9234\n",
      "epoch: 3 iter: 881 loss: 0.31621 training acc: 0.90706 testing acc: 0.905\n",
      "epoch: 3 iter: 882 loss: 0.26262 training acc: 0.92410 testing acc: 0.9182\n",
      "epoch: 3 iter: 883 loss: 0.25740 training acc: 0.92504 testing acc: 0.9188\n",
      "epoch: 3 iter: 884 loss: 0.28791 training acc: 0.91474 testing acc: 0.9112\n",
      "epoch: 3 iter: 885 loss: 0.25501 training acc: 0.92714 testing acc: 0.9234\n",
      "epoch: 3 iter: 886 loss: 0.26963 training acc: 0.91994 testing acc: 0.9158\n",
      "epoch: 3 iter: 887 loss: 0.24978 training acc: 0.92696 testing acc: 0.92\n",
      "epoch: 3 iter: 888 loss: 0.25038 training acc: 0.92772 testing acc: 0.92\n",
      "epoch: 3 iter: 889 loss: 0.26722 training acc: 0.92056 testing acc: 0.916\n",
      "epoch: 3 iter: 890 loss: 0.25318 training acc: 0.92628 testing acc: 0.919\n",
      "epoch: 3 iter: 891 loss: 0.26372 training acc: 0.92528 testing acc: 0.9172\n",
      "epoch: 3 iter: 892 loss: 0.25882 training acc: 0.92812 testing acc: 0.9238\n",
      "epoch: 3 iter: 893 loss: 0.25254 training acc: 0.92808 testing acc: 0.9212\n",
      "epoch: 3 iter: 894 loss: 0.25381 training acc: 0.92746 testing acc: 0.9216\n",
      "epoch: 3 iter: 895 loss: 0.25280 training acc: 0.92774 testing acc: 0.9222\n",
      "epoch: 3 iter: 896 loss: 0.26034 training acc: 0.92698 testing acc: 0.921\n",
      "epoch: 3 iter: 897 loss: 0.25971 training acc: 0.92440 testing acc: 0.9158\n",
      "epoch: 3 iter: 898 loss: 0.26233 training acc: 0.92650 testing acc: 0.9232\n",
      "epoch: 3 iter: 899 loss: 0.25787 training acc: 0.92758 testing acc: 0.9224\n",
      "epoch: 3 iter: 900 loss: 0.27217 training acc: 0.92044 testing acc: 0.9138\n",
      "epoch: 3 iter: 901 loss: 0.25164 training acc: 0.92770 testing acc: 0.9218\n",
      "epoch: 3 iter: 902 loss: 0.26888 training acc: 0.92212 testing acc: 0.9188\n",
      "epoch: 3 iter: 903 loss: 0.26448 training acc: 0.92246 testing acc: 0.9158\n",
      "epoch: 3 iter: 904 loss: 0.23992 training acc: 0.93088 testing acc: 0.9244\n",
      "epoch: 3 iter: 905 loss: 0.25162 training acc: 0.92920 testing acc: 0.9258\n",
      "epoch: 3 iter: 906 loss: 0.24916 training acc: 0.92994 testing acc: 0.923\n",
      "epoch: 3 iter: 907 loss: 0.25510 training acc: 0.92920 testing acc: 0.9238\n",
      "epoch: 3 iter: 908 loss: 0.25487 training acc: 0.92890 testing acc: 0.9232\n",
      "epoch: 3 iter: 909 loss: 0.24455 training acc: 0.93172 testing acc: 0.9244\n",
      "epoch: 3 iter: 910 loss: 0.24005 training acc: 0.93310 testing acc: 0.9266\n",
      "epoch: 3 iter: 911 loss: 0.26830 training acc: 0.92462 testing acc: 0.9202\n",
      "epoch: 3 iter: 912 loss: 0.24827 training acc: 0.92978 testing acc: 0.9236\n",
      "epoch: 3 iter: 913 loss: 0.25480 training acc: 0.92840 testing acc: 0.9218\n",
      "epoch: 3 iter: 914 loss: 0.23973 training acc: 0.93286 testing acc: 0.9268\n",
      "epoch: 3 iter: 915 loss: 0.24270 training acc: 0.93190 testing acc: 0.925\n",
      "epoch: 3 iter: 916 loss: 0.24224 training acc: 0.93204 testing acc: 0.9268\n",
      "epoch: 3 iter: 917 loss: 0.25547 training acc: 0.92788 testing acc: 0.9236\n",
      "epoch: 3 iter: 918 loss: 0.25506 training acc: 0.92396 testing acc: 0.917\n",
      "epoch: 3 iter: 919 loss: 0.24085 training acc: 0.93194 testing acc: 0.9224\n",
      "epoch: 3 iter: 920 loss: 0.24512 training acc: 0.93068 testing acc: 0.923\n",
      "epoch: 3 iter: 921 loss: 0.24076 training acc: 0.93244 testing acc: 0.924\n",
      "epoch: 3 iter: 922 loss: 0.24547 training acc: 0.92972 testing acc: 0.9204\n",
      "epoch: 3 iter: 923 loss: 0.27172 training acc: 0.91710 testing acc: 0.9084\n",
      "epoch: 3 iter: 924 loss: 0.24074 training acc: 0.93046 testing acc: 0.9204\n",
      "epoch: 3 iter: 925 loss: 0.24314 training acc: 0.92896 testing acc: 0.9224\n",
      "epoch: 3 iter: 926 loss: 0.23845 training acc: 0.93210 testing acc: 0.9248\n",
      "epoch: 3 iter: 927 loss: 0.24489 training acc: 0.93082 testing acc: 0.9246\n",
      "epoch: 3 iter: 928 loss: 0.25472 training acc: 0.92556 testing acc: 0.9208\n",
      "epoch: 3 iter: 929 loss: 0.24836 training acc: 0.92798 testing acc: 0.9212\n",
      "epoch: 3 iter: 930 loss: 0.23733 training acc: 0.93216 testing acc: 0.925\n",
      "epoch: 3 iter: 931 loss: 0.26109 training acc: 0.92420 testing acc: 0.9172\n",
      "epoch: 3 iter: 932 loss: 0.24099 training acc: 0.92982 testing acc: 0.9214\n",
      "epoch: 3 iter: 933 loss: 0.24008 training acc: 0.93142 testing acc: 0.9236\n",
      "epoch: 3 iter: 934 loss: 0.24862 training acc: 0.92830 testing acc: 0.9202\n",
      "epoch: 3 iter: 935 loss: 0.26095 training acc: 0.92406 testing acc: 0.9142\n",
      "epoch: 3 iter: 936 loss: 0.25650 training acc: 0.92712 testing acc: 0.9232\n",
      "epoch: 3 iter: 937 loss: 0.24306 training acc: 0.93140 testing acc: 0.9236\n",
      "epoch: 3 iter: 938 loss: 0.25150 training acc: 0.92756 testing acc: 0.9174\n",
      "epoch: 3 iter: 939 loss: 0.25692 training acc: 0.92590 testing acc: 0.9172\n",
      "epoch: 3 iter: 940 loss: 0.24632 training acc: 0.92984 testing acc: 0.9178\n",
      "epoch: 3 iter: 941 loss: 0.24600 training acc: 0.92796 testing acc: 0.9206\n",
      "epoch: 3 iter: 942 loss: 0.27637 training acc: 0.91772 testing acc: 0.9134\n",
      "epoch: 3 iter: 943 loss: 0.24057 training acc: 0.93198 testing acc: 0.9274\n",
      "epoch: 3 iter: 944 loss: 0.24068 training acc: 0.93192 testing acc: 0.9274\n",
      "epoch: 3 iter: 945 loss: 0.23827 training acc: 0.93220 testing acc: 0.9266\n",
      "epoch: 3 iter: 946 loss: 0.27538 training acc: 0.92022 testing acc: 0.9146\n",
      "epoch: 3 iter: 947 loss: 0.24713 training acc: 0.92922 testing acc: 0.9244\n",
      "epoch: 3 iter: 948 loss: 0.25935 training acc: 0.92822 testing acc: 0.9218\n",
      "epoch: 3 iter: 949 loss: 0.24311 training acc: 0.93198 testing acc: 0.9256\n",
      "epoch: 3 iter: 950 loss: 0.23891 training acc: 0.93052 testing acc: 0.922\n",
      "epoch: 3 iter: 951 loss: 0.24159 training acc: 0.92896 testing acc: 0.92\n",
      "epoch: 3 iter: 952 loss: 0.23922 training acc: 0.92976 testing acc: 0.9214\n",
      "epoch: 3 iter: 953 loss: 0.24853 training acc: 0.92728 testing acc: 0.92\n",
      "epoch: 3 iter: 954 loss: 0.23225 training acc: 0.93468 testing acc: 0.9262\n",
      "epoch: 3 iter: 955 loss: 0.23371 training acc: 0.93408 testing acc: 0.9282\n",
      "epoch: 3 iter: 956 loss: 0.24108 training acc: 0.92990 testing acc: 0.9226\n",
      "epoch: 3 iter: 957 loss: 0.24285 training acc: 0.92872 testing acc: 0.9208\n",
      "epoch: 3 iter: 958 loss: 0.24789 training acc: 0.92716 testing acc: 0.9198\n",
      "epoch: 3 iter: 959 loss: 0.24702 training acc: 0.92886 testing acc: 0.9226\n",
      "epoch: 3 iter: 960 loss: 0.24128 training acc: 0.93146 testing acc: 0.9268\n",
      "epoch: 3 iter: 961 loss: 0.23830 training acc: 0.93054 testing acc: 0.9212\n",
      "epoch: 3 iter: 962 loss: 0.24686 training acc: 0.92818 testing acc: 0.919\n",
      "epoch: 3 iter: 963 loss: 0.24470 training acc: 0.92790 testing acc: 0.9184\n",
      "epoch: 3 iter: 964 loss: 0.23838 training acc: 0.93108 testing acc: 0.9212\n",
      "epoch: 3 iter: 965 loss: 0.23511 training acc: 0.93370 testing acc: 0.9258\n",
      "epoch: 3 iter: 966 loss: 0.23578 training acc: 0.93368 testing acc: 0.927\n",
      "epoch: 3 iter: 967 loss: 0.25128 training acc: 0.92630 testing acc: 0.9188\n",
      "epoch: 3 iter: 968 loss: 0.23425 training acc: 0.93286 testing acc: 0.9232\n",
      "epoch: 3 iter: 969 loss: 0.23025 training acc: 0.93472 testing acc: 0.9268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 iter: 970 loss: 0.23136 training acc: 0.93424 testing acc: 0.9276\n",
      "epoch: 3 iter: 971 loss: 0.22831 training acc: 0.93466 testing acc: 0.9266\n",
      "epoch: 3 iter: 972 loss: 0.23193 training acc: 0.93324 testing acc: 0.9266\n",
      "epoch: 3 iter: 973 loss: 0.23628 training acc: 0.93168 testing acc: 0.924\n",
      "epoch: 3 iter: 974 loss: 0.25463 training acc: 0.92546 testing acc: 0.9166\n",
      "epoch: 3 iter: 975 loss: 0.24043 training acc: 0.93162 testing acc: 0.9236\n",
      "epoch: 3 iter: 976 loss: 0.29169 training acc: 0.91216 testing acc: 0.9034\n",
      "epoch: 3 iter: 977 loss: 0.23508 training acc: 0.93316 testing acc: 0.9226\n",
      "epoch: 3 iter: 978 loss: 0.23256 training acc: 0.93550 testing acc: 0.9278\n",
      "epoch: 3 iter: 979 loss: 0.22673 training acc: 0.93726 testing acc: 0.9306\n",
      "epoch: 3 iter: 980 loss: 0.23808 training acc: 0.93140 testing acc: 0.9212\n",
      "epoch: 3 iter: 981 loss: 0.26152 training acc: 0.92268 testing acc: 0.9152\n",
      "epoch: 3 iter: 982 loss: 0.23494 training acc: 0.93272 testing acc: 0.9242\n",
      "epoch: 3 iter: 983 loss: 0.23257 training acc: 0.93414 testing acc: 0.9286\n",
      "epoch: 3 iter: 984 loss: 0.24432 training acc: 0.92950 testing acc: 0.924\n",
      "epoch: 3 iter: 985 loss: 0.25656 training acc: 0.92490 testing acc: 0.9164\n",
      "epoch: 3 iter: 986 loss: 0.23878 training acc: 0.93134 testing acc: 0.9242\n",
      "epoch: 3 iter: 987 loss: 0.24981 training acc: 0.92806 testing acc: 0.9216\n",
      "epoch: 3 iter: 988 loss: 0.24320 training acc: 0.93034 testing acc: 0.9218\n",
      "epoch: 3 iter: 989 loss: 0.23820 training acc: 0.93244 testing acc: 0.9266\n",
      "epoch: 3 iter: 990 loss: 0.24348 training acc: 0.92878 testing acc: 0.9228\n",
      "epoch: 3 iter: 991 loss: 0.23638 training acc: 0.93008 testing acc: 0.9222\n",
      "epoch: 3 iter: 992 loss: 0.22617 training acc: 0.93580 testing acc: 0.9268\n",
      "epoch: 3 iter: 993 loss: 0.25063 training acc: 0.92692 testing acc: 0.9164\n",
      "epoch: 3 iter: 994 loss: 0.26503 training acc: 0.92506 testing acc: 0.9188\n",
      "epoch: 3 iter: 995 loss: 0.23478 training acc: 0.93414 testing acc: 0.9276\n",
      "epoch: 3 iter: 996 loss: 0.22937 training acc: 0.93582 testing acc: 0.9272\n",
      "epoch: 3 iter: 997 loss: 0.22836 training acc: 0.93534 testing acc: 0.9276\n",
      "epoch: 3 iter: 998 loss: 0.23688 training acc: 0.93280 testing acc: 0.9254\n",
      "epoch: 3 iter: 999 loss: 0.22764 training acc: 0.93480 testing acc: 0.9276\n",
      "epoch: 4 iter:   0 loss: 0.22453 training acc: 0.93700 testing acc: 0.9298\n",
      "epoch: 4 iter:   1 loss: 0.22345 training acc: 0.93796 testing acc: 0.9292\n",
      "epoch: 4 iter:   2 loss: 0.22946 training acc: 0.93494 testing acc: 0.9306\n",
      "epoch: 4 iter:   3 loss: 0.24176 training acc: 0.92974 testing acc: 0.9214\n",
      "epoch: 4 iter:   4 loss: 0.23224 training acc: 0.93420 testing acc: 0.9248\n",
      "epoch: 4 iter:   5 loss: 0.22836 training acc: 0.93540 testing acc: 0.9284\n",
      "epoch: 4 iter:   6 loss: 0.23846 training acc: 0.93136 testing acc: 0.9224\n",
      "epoch: 4 iter:   7 loss: 0.23217 training acc: 0.93324 testing acc: 0.9264\n",
      "epoch: 4 iter:   8 loss: 0.23593 training acc: 0.93176 testing acc: 0.9218\n",
      "epoch: 4 iter:   9 loss: 0.27509 training acc: 0.91848 testing acc: 0.9088\n",
      "epoch: 4 iter:  10 loss: 0.23329 training acc: 0.93348 testing acc: 0.9262\n",
      "epoch: 4 iter:  11 loss: 0.23869 training acc: 0.93244 testing acc: 0.9266\n",
      "epoch: 4 iter:  12 loss: 0.23230 training acc: 0.93336 testing acc: 0.9284\n",
      "epoch: 4 iter:  13 loss: 0.23802 training acc: 0.93210 testing acc: 0.9258\n",
      "epoch: 4 iter:  14 loss: 0.22740 training acc: 0.93440 testing acc: 0.9268\n",
      "epoch: 4 iter:  15 loss: 0.24093 training acc: 0.93088 testing acc: 0.924\n",
      "epoch: 4 iter:  16 loss: 0.23061 training acc: 0.93438 testing acc: 0.928\n",
      "epoch: 4 iter:  17 loss: 0.24737 training acc: 0.92788 testing acc: 0.9202\n",
      "epoch: 4 iter:  18 loss: 0.24071 training acc: 0.93014 testing acc: 0.9236\n",
      "epoch: 4 iter:  19 loss: 0.23453 training acc: 0.93154 testing acc: 0.9248\n",
      "epoch: 4 iter:  20 loss: 0.24668 training acc: 0.92802 testing acc: 0.9212\n",
      "epoch: 4 iter:  21 loss: 0.26754 training acc: 0.91900 testing acc: 0.9146\n",
      "epoch: 4 iter:  22 loss: 0.24090 training acc: 0.92892 testing acc: 0.9244\n",
      "epoch: 4 iter:  23 loss: 0.23861 training acc: 0.93064 testing acc: 0.9202\n",
      "epoch: 4 iter:  24 loss: 0.22664 training acc: 0.93466 testing acc: 0.9266\n",
      "epoch: 4 iter:  25 loss: 0.23776 training acc: 0.93190 testing acc: 0.9258\n",
      "epoch: 4 iter:  26 loss: 0.22329 training acc: 0.93680 testing acc: 0.9288\n",
      "epoch: 4 iter:  27 loss: 0.22776 training acc: 0.93556 testing acc: 0.929\n",
      "epoch: 4 iter:  28 loss: 0.23262 training acc: 0.93424 testing acc: 0.9262\n",
      "epoch: 4 iter:  29 loss: 0.22879 training acc: 0.93602 testing acc: 0.9264\n",
      "epoch: 4 iter:  30 loss: 0.24328 training acc: 0.93218 testing acc: 0.9244\n",
      "epoch: 4 iter:  31 loss: 0.24318 training acc: 0.93152 testing acc: 0.924\n",
      "epoch: 4 iter:  32 loss: 0.22750 training acc: 0.93648 testing acc: 0.9304\n",
      "epoch: 4 iter:  33 loss: 0.22880 training acc: 0.93520 testing acc: 0.9292\n",
      "epoch: 4 iter:  34 loss: 0.22909 training acc: 0.93548 testing acc: 0.929\n",
      "epoch: 4 iter:  35 loss: 0.23463 training acc: 0.93302 testing acc: 0.9292\n",
      "epoch: 4 iter:  36 loss: 0.23240 training acc: 0.93476 testing acc: 0.9302\n",
      "epoch: 4 iter:  37 loss: 0.22796 training acc: 0.93592 testing acc: 0.9278\n",
      "epoch: 4 iter:  38 loss: 0.22410 training acc: 0.93724 testing acc: 0.9336\n",
      "epoch: 4 iter:  39 loss: 0.23472 training acc: 0.93348 testing acc: 0.9268\n",
      "epoch: 4 iter:  40 loss: 0.23088 training acc: 0.93462 testing acc: 0.9294\n",
      "epoch: 4 iter:  41 loss: 0.22684 training acc: 0.93518 testing acc: 0.9268\n",
      "epoch: 4 iter:  42 loss: 0.22472 training acc: 0.93666 testing acc: 0.9296\n",
      "epoch: 4 iter:  43 loss: 0.22074 training acc: 0.93712 testing acc: 0.9292\n",
      "epoch: 4 iter:  44 loss: 0.23018 training acc: 0.93528 testing acc: 0.9278\n",
      "epoch: 4 iter:  45 loss: 0.22753 training acc: 0.93588 testing acc: 0.9274\n",
      "epoch: 4 iter:  46 loss: 0.23753 training acc: 0.93224 testing acc: 0.9262\n",
      "epoch: 4 iter:  47 loss: 0.22624 training acc: 0.93708 testing acc: 0.9278\n",
      "epoch: 4 iter:  48 loss: 0.22469 training acc: 0.93728 testing acc: 0.9298\n",
      "epoch: 4 iter:  49 loss: 0.22874 training acc: 0.93356 testing acc: 0.9262\n",
      "epoch: 4 iter:  50 loss: 0.22824 training acc: 0.93464 testing acc: 0.927\n",
      "epoch: 4 iter:  51 loss: 0.22889 training acc: 0.93554 testing acc: 0.9258\n",
      "epoch: 4 iter:  52 loss: 0.25612 training acc: 0.92412 testing acc: 0.9158\n",
      "epoch: 4 iter:  53 loss: 0.23895 training acc: 0.93240 testing acc: 0.9256\n",
      "epoch: 4 iter:  54 loss: 0.23908 training acc: 0.93008 testing acc: 0.922\n",
      "epoch: 4 iter:  55 loss: 0.22764 training acc: 0.93414 testing acc: 0.9256\n",
      "epoch: 4 iter:  56 loss: 0.22479 training acc: 0.93524 testing acc: 0.9266\n",
      "epoch: 4 iter:  57 loss: 0.22520 training acc: 0.93502 testing acc: 0.9266\n",
      "epoch: 4 iter:  58 loss: 0.22700 training acc: 0.93586 testing acc: 0.9298\n",
      "epoch: 4 iter:  59 loss: 0.22839 training acc: 0.93302 testing acc: 0.9238\n",
      "epoch: 4 iter:  60 loss: 0.22330 training acc: 0.93692 testing acc: 0.9284\n",
      "epoch: 4 iter:  61 loss: 0.22170 training acc: 0.93810 testing acc: 0.9306\n",
      "epoch: 4 iter:  62 loss: 0.22141 training acc: 0.93826 testing acc: 0.9314\n",
      "epoch: 4 iter:  63 loss: 0.22966 training acc: 0.93500 testing acc: 0.926\n",
      "epoch: 4 iter:  64 loss: 0.23117 training acc: 0.93328 testing acc: 0.9252\n",
      "epoch: 4 iter:  65 loss: 0.22474 training acc: 0.93632 testing acc: 0.9272\n",
      "epoch: 4 iter:  66 loss: 0.22717 training acc: 0.93656 testing acc: 0.9296\n",
      "epoch: 4 iter:  67 loss: 0.23191 training acc: 0.93622 testing acc: 0.9306\n",
      "epoch: 4 iter:  68 loss: 0.23295 training acc: 0.93598 testing acc: 0.93\n",
      "epoch: 4 iter:  69 loss: 0.22969 training acc: 0.93672 testing acc: 0.9294\n",
      "epoch: 4 iter:  70 loss: 0.26044 training acc: 0.92682 testing acc: 0.918\n",
      "epoch: 4 iter:  71 loss: 0.22745 training acc: 0.93442 testing acc: 0.9264\n",
      "epoch: 4 iter:  72 loss: 0.24235 training acc: 0.93158 testing acc: 0.9226\n",
      "epoch: 4 iter:  73 loss: 0.22043 training acc: 0.93682 testing acc: 0.9288\n",
      "epoch: 4 iter:  74 loss: 0.21749 training acc: 0.93864 testing acc: 0.9294\n",
      "epoch: 4 iter:  75 loss: 0.23361 training acc: 0.93188 testing acc: 0.9238\n",
      "epoch: 4 iter:  76 loss: 0.22718 training acc: 0.93426 testing acc: 0.9248\n",
      "epoch: 4 iter:  77 loss: 0.22341 training acc: 0.93582 testing acc: 0.9276\n",
      "epoch: 4 iter:  78 loss: 0.22465 training acc: 0.93580 testing acc: 0.9258\n",
      "epoch: 4 iter:  79 loss: 0.23606 training acc: 0.93134 testing acc: 0.9212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 iter:  80 loss: 0.23339 training acc: 0.93214 testing acc: 0.9246\n",
      "epoch: 4 iter:  81 loss: 0.23886 training acc: 0.93154 testing acc: 0.9206\n",
      "epoch: 4 iter:  82 loss: 0.24943 training acc: 0.92856 testing acc: 0.9164\n",
      "epoch: 4 iter:  83 loss: 0.25062 training acc: 0.92728 testing acc: 0.9158\n",
      "epoch: 4 iter:  84 loss: 0.22072 training acc: 0.93616 testing acc: 0.9276\n",
      "epoch: 4 iter:  85 loss: 0.22505 training acc: 0.93540 testing acc: 0.9276\n",
      "epoch: 4 iter:  86 loss: 0.23329 training acc: 0.93292 testing acc: 0.9204\n",
      "epoch: 4 iter:  87 loss: 0.22614 training acc: 0.93506 testing acc: 0.9214\n",
      "epoch: 4 iter:  88 loss: 0.21991 training acc: 0.93718 testing acc: 0.9268\n",
      "epoch: 4 iter:  89 loss: 0.22857 training acc: 0.93296 testing acc: 0.923\n",
      "epoch: 4 iter:  90 loss: 0.22620 training acc: 0.93634 testing acc: 0.9256\n",
      "epoch: 4 iter:  91 loss: 0.22233 training acc: 0.93662 testing acc: 0.9254\n",
      "epoch: 4 iter:  92 loss: 0.21576 training acc: 0.93746 testing acc: 0.9306\n",
      "epoch: 4 iter:  93 loss: 0.21499 training acc: 0.93818 testing acc: 0.929\n",
      "epoch: 4 iter:  94 loss: 0.22215 training acc: 0.93554 testing acc: 0.928\n",
      "epoch: 4 iter:  95 loss: 0.21958 training acc: 0.93728 testing acc: 0.9308\n",
      "epoch: 4 iter:  96 loss: 0.23788 training acc: 0.93402 testing acc: 0.924\n",
      "epoch: 4 iter:  97 loss: 0.21940 training acc: 0.93778 testing acc: 0.931\n",
      "epoch: 4 iter:  98 loss: 0.21274 training acc: 0.94008 testing acc: 0.934\n",
      "epoch: 4 iter:  99 loss: 0.21291 training acc: 0.94056 testing acc: 0.934\n",
      "epoch: 4 iter: 100 loss: 0.21918 training acc: 0.93710 testing acc: 0.9312\n",
      "epoch: 4 iter: 101 loss: 0.21840 training acc: 0.93774 testing acc: 0.9296\n",
      "epoch: 4 iter: 102 loss: 0.22285 training acc: 0.93572 testing acc: 0.927\n",
      "epoch: 4 iter: 103 loss: 0.22668 training acc: 0.93544 testing acc: 0.9306\n",
      "epoch: 4 iter: 104 loss: 0.21354 training acc: 0.94020 testing acc: 0.9326\n",
      "epoch: 4 iter: 105 loss: 0.21567 training acc: 0.93896 testing acc: 0.9284\n",
      "epoch: 4 iter: 106 loss: 0.21912 training acc: 0.93748 testing acc: 0.9288\n",
      "epoch: 4 iter: 107 loss: 0.23534 training acc: 0.93210 testing acc: 0.9244\n",
      "epoch: 4 iter: 108 loss: 0.22433 training acc: 0.93572 testing acc: 0.9248\n",
      "epoch: 4 iter: 109 loss: 0.21803 training acc: 0.93866 testing acc: 0.9306\n",
      "epoch: 4 iter: 110 loss: 0.21934 training acc: 0.93894 testing acc: 0.932\n",
      "epoch: 4 iter: 111 loss: 0.22558 training acc: 0.93592 testing acc: 0.9288\n",
      "epoch: 4 iter: 112 loss: 0.21910 training acc: 0.93712 testing acc: 0.9292\n",
      "epoch: 4 iter: 113 loss: 0.21501 training acc: 0.93938 testing acc: 0.93\n",
      "epoch: 4 iter: 114 loss: 0.21765 training acc: 0.93936 testing acc: 0.9312\n",
      "epoch: 4 iter: 115 loss: 0.21477 training acc: 0.94082 testing acc: 0.9324\n",
      "epoch: 4 iter: 116 loss: 0.21821 training acc: 0.93908 testing acc: 0.9306\n",
      "epoch: 4 iter: 117 loss: 0.21915 training acc: 0.93794 testing acc: 0.9292\n",
      "epoch: 4 iter: 118 loss: 0.21803 training acc: 0.93798 testing acc: 0.9288\n",
      "epoch: 4 iter: 119 loss: 0.26322 training acc: 0.91998 testing acc: 0.9114\n",
      "epoch: 4 iter: 120 loss: 0.23586 training acc: 0.92980 testing acc: 0.9214\n",
      "epoch: 4 iter: 121 loss: 0.22805 training acc: 0.93294 testing acc: 0.9218\n",
      "epoch: 4 iter: 122 loss: 0.23526 training acc: 0.93250 testing acc: 0.9226\n",
      "epoch: 4 iter: 123 loss: 0.22106 training acc: 0.93702 testing acc: 0.9294\n",
      "epoch: 4 iter: 124 loss: 0.21991 training acc: 0.93622 testing acc: 0.9256\n",
      "epoch: 4 iter: 125 loss: 0.25379 training acc: 0.92408 testing acc: 0.913\n",
      "epoch: 4 iter: 126 loss: 0.23718 training acc: 0.92914 testing acc: 0.916\n",
      "epoch: 4 iter: 127 loss: 0.23651 training acc: 0.92916 testing acc: 0.9158\n",
      "epoch: 4 iter: 128 loss: 0.22492 training acc: 0.93364 testing acc: 0.923\n",
      "epoch: 4 iter: 129 loss: 0.23254 training acc: 0.93040 testing acc: 0.921\n",
      "epoch: 4 iter: 130 loss: 0.22534 training acc: 0.93320 testing acc: 0.9218\n",
      "epoch: 4 iter: 131 loss: 0.25168 training acc: 0.92318 testing acc: 0.9146\n",
      "epoch: 4 iter: 132 loss: 0.22031 training acc: 0.93650 testing acc: 0.9274\n",
      "epoch: 4 iter: 133 loss: 0.21731 training acc: 0.93806 testing acc: 0.9292\n",
      "epoch: 4 iter: 134 loss: 0.21190 training acc: 0.93872 testing acc: 0.9294\n",
      "epoch: 4 iter: 135 loss: 0.21391 training acc: 0.93852 testing acc: 0.929\n",
      "epoch: 4 iter: 136 loss: 0.23535 training acc: 0.93342 testing acc: 0.9224\n",
      "epoch: 4 iter: 137 loss: 0.21530 training acc: 0.93942 testing acc: 0.929\n",
      "epoch: 4 iter: 138 loss: 0.22277 training acc: 0.93646 testing acc: 0.9262\n",
      "epoch: 4 iter: 139 loss: 0.23620 training acc: 0.92986 testing acc: 0.923\n",
      "epoch: 4 iter: 140 loss: 0.21815 training acc: 0.93750 testing acc: 0.9262\n",
      "epoch: 4 iter: 141 loss: 0.22969 training acc: 0.93406 testing acc: 0.9244\n",
      "epoch: 4 iter: 142 loss: 0.23138 training acc: 0.93190 testing acc: 0.9212\n",
      "epoch: 4 iter: 143 loss: 0.21912 training acc: 0.93644 testing acc: 0.9258\n",
      "epoch: 4 iter: 144 loss: 0.21384 training acc: 0.93786 testing acc: 0.9298\n",
      "epoch: 4 iter: 145 loss: 0.22239 training acc: 0.93560 testing acc: 0.926\n",
      "epoch: 4 iter: 146 loss: 0.22690 training acc: 0.93442 testing acc: 0.9306\n",
      "epoch: 4 iter: 147 loss: 0.22072 training acc: 0.93588 testing acc: 0.9288\n",
      "epoch: 4 iter: 148 loss: 0.22617 training acc: 0.93426 testing acc: 0.926\n",
      "epoch: 4 iter: 149 loss: 0.23118 training acc: 0.93358 testing acc: 0.9262\n",
      "epoch: 4 iter: 150 loss: 0.25301 training acc: 0.92406 testing acc: 0.9184\n",
      "epoch: 4 iter: 151 loss: 0.22868 training acc: 0.93236 testing acc: 0.9252\n",
      "epoch: 4 iter: 152 loss: 0.23681 training acc: 0.92944 testing acc: 0.9212\n",
      "epoch: 4 iter: 153 loss: 0.22513 training acc: 0.93490 testing acc: 0.926\n",
      "epoch: 4 iter: 154 loss: 0.23305 training acc: 0.93200 testing acc: 0.9244\n",
      "epoch: 4 iter: 155 loss: 0.22074 training acc: 0.93706 testing acc: 0.9296\n",
      "epoch: 4 iter: 156 loss: 0.22413 training acc: 0.93546 testing acc: 0.927\n",
      "epoch: 4 iter: 157 loss: 0.22516 training acc: 0.93566 testing acc: 0.9294\n",
      "epoch: 4 iter: 158 loss: 0.22110 training acc: 0.93682 testing acc: 0.9278\n",
      "epoch: 4 iter: 159 loss: 0.21438 training acc: 0.93924 testing acc: 0.9314\n",
      "epoch: 4 iter: 160 loss: 0.24345 training acc: 0.92726 testing acc: 0.9192\n",
      "epoch: 4 iter: 161 loss: 0.23474 training acc: 0.92976 testing acc: 0.9228\n",
      "epoch: 4 iter: 162 loss: 0.22619 training acc: 0.93280 testing acc: 0.9252\n",
      "epoch: 4 iter: 163 loss: 0.24628 training acc: 0.92508 testing acc: 0.919\n",
      "epoch: 4 iter: 164 loss: 0.21911 training acc: 0.93664 testing acc: 0.9316\n",
      "epoch: 4 iter: 165 loss: 0.21963 training acc: 0.93576 testing acc: 0.9322\n",
      "epoch: 4 iter: 166 loss: 0.22841 training acc: 0.93476 testing acc: 0.9256\n",
      "epoch: 4 iter: 167 loss: 0.21966 training acc: 0.93614 testing acc: 0.931\n",
      "epoch: 4 iter: 168 loss: 0.24503 training acc: 0.92572 testing acc: 0.9202\n",
      "epoch: 4 iter: 169 loss: 0.22342 training acc: 0.93628 testing acc: 0.9282\n",
      "epoch: 4 iter: 170 loss: 0.23373 training acc: 0.93066 testing acc: 0.9204\n",
      "epoch: 4 iter: 171 loss: 0.22223 training acc: 0.93548 testing acc: 0.9246\n",
      "epoch: 4 iter: 172 loss: 0.21242 training acc: 0.94010 testing acc: 0.9322\n",
      "epoch: 4 iter: 173 loss: 0.22769 training acc: 0.93458 testing acc: 0.927\n",
      "epoch: 4 iter: 174 loss: 0.21743 training acc: 0.93690 testing acc: 0.9324\n",
      "epoch: 4 iter: 175 loss: 0.21566 training acc: 0.93770 testing acc: 0.9292\n",
      "epoch: 4 iter: 176 loss: 0.23894 training acc: 0.92824 testing acc: 0.9194\n",
      "epoch: 4 iter: 177 loss: 0.21986 training acc: 0.93464 testing acc: 0.9262\n",
      "epoch: 4 iter: 178 loss: 0.22277 training acc: 0.93500 testing acc: 0.9306\n",
      "epoch: 4 iter: 179 loss: 0.22198 training acc: 0.93528 testing acc: 0.9304\n",
      "epoch: 4 iter: 180 loss: 0.23036 training acc: 0.93468 testing acc: 0.929\n",
      "epoch: 4 iter: 181 loss: 0.22266 training acc: 0.93664 testing acc: 0.9338\n",
      "epoch: 4 iter: 182 loss: 0.22170 training acc: 0.93680 testing acc: 0.9306\n",
      "epoch: 4 iter: 183 loss: 0.22354 training acc: 0.93764 testing acc: 0.9348\n",
      "epoch: 4 iter: 184 loss: 0.21285 training acc: 0.93946 testing acc: 0.9346\n",
      "epoch: 4 iter: 185 loss: 0.21207 training acc: 0.93976 testing acc: 0.9378\n",
      "epoch: 4 iter: 186 loss: 0.21421 training acc: 0.93906 testing acc: 0.9352\n",
      "epoch: 4 iter: 187 loss: 0.23103 training acc: 0.93450 testing acc: 0.93\n",
      "epoch: 4 iter: 188 loss: 0.23175 training acc: 0.93588 testing acc: 0.9342\n",
      "epoch: 4 iter: 189 loss: 0.22496 training acc: 0.93788 testing acc: 0.9366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 iter: 190 loss: 0.21620 training acc: 0.93934 testing acc: 0.9362\n",
      "epoch: 4 iter: 191 loss: 0.22579 training acc: 0.93516 testing acc: 0.931\n",
      "epoch: 4 iter: 192 loss: 0.22227 training acc: 0.93422 testing acc: 0.9274\n",
      "epoch: 4 iter: 193 loss: 0.21403 training acc: 0.93860 testing acc: 0.9344\n",
      "epoch: 4 iter: 194 loss: 0.21023 training acc: 0.94084 testing acc: 0.9366\n",
      "epoch: 4 iter: 195 loss: 0.21489 training acc: 0.93908 testing acc: 0.9342\n",
      "epoch: 4 iter: 196 loss: 0.21220 training acc: 0.94026 testing acc: 0.9356\n",
      "epoch: 4 iter: 197 loss: 0.21312 training acc: 0.94050 testing acc: 0.9372\n",
      "epoch: 4 iter: 198 loss: 0.21945 training acc: 0.93840 testing acc: 0.9348\n",
      "epoch: 4 iter: 199 loss: 0.23041 training acc: 0.93122 testing acc: 0.927\n",
      "epoch: 4 iter: 200 loss: 0.23001 training acc: 0.93328 testing acc: 0.924\n",
      "epoch: 4 iter: 201 loss: 0.22413 training acc: 0.93398 testing acc: 0.932\n",
      "epoch: 4 iter: 202 loss: 0.21053 training acc: 0.93978 testing acc: 0.936\n",
      "epoch: 4 iter: 203 loss: 0.21924 training acc: 0.93678 testing acc: 0.9332\n",
      "epoch: 4 iter: 204 loss: 0.20655 training acc: 0.94102 testing acc: 0.936\n",
      "epoch: 4 iter: 205 loss: 0.20123 training acc: 0.94314 testing acc: 0.9402\n",
      "epoch: 4 iter: 206 loss: 0.20150 training acc: 0.94290 testing acc: 0.939\n",
      "epoch: 4 iter: 207 loss: 0.21681 training acc: 0.93734 testing acc: 0.936\n",
      "epoch: 4 iter: 208 loss: 0.22756 training acc: 0.93172 testing acc: 0.9286\n",
      "epoch: 4 iter: 209 loss: 0.21911 training acc: 0.93534 testing acc: 0.9322\n",
      "epoch: 4 iter: 210 loss: 0.21579 training acc: 0.93720 testing acc: 0.9308\n",
      "epoch: 4 iter: 211 loss: 0.21328 training acc: 0.93790 testing acc: 0.9326\n",
      "epoch: 4 iter: 212 loss: 0.21502 training acc: 0.93790 testing acc: 0.933\n",
      "epoch: 4 iter: 213 loss: 0.21853 training acc: 0.93564 testing acc: 0.9324\n",
      "epoch: 4 iter: 214 loss: 0.23334 training acc: 0.93440 testing acc: 0.9316\n",
      "epoch: 4 iter: 215 loss: 0.23498 training acc: 0.93222 testing acc: 0.9258\n",
      "epoch: 4 iter: 216 loss: 0.22098 training acc: 0.93620 testing acc: 0.931\n",
      "epoch: 4 iter: 217 loss: 0.22591 training acc: 0.93484 testing acc: 0.9278\n",
      "epoch: 4 iter: 218 loss: 0.22222 training acc: 0.93536 testing acc: 0.929\n",
      "epoch: 4 iter: 219 loss: 0.21992 training acc: 0.93476 testing acc: 0.927\n",
      "epoch: 4 iter: 220 loss: 0.21583 training acc: 0.93748 testing acc: 0.9304\n",
      "epoch: 4 iter: 221 loss: 0.22234 training acc: 0.93628 testing acc: 0.9306\n",
      "epoch: 4 iter: 222 loss: 0.20783 training acc: 0.94036 testing acc: 0.9336\n",
      "epoch: 4 iter: 223 loss: 0.20654 training acc: 0.94202 testing acc: 0.938\n",
      "epoch: 4 iter: 224 loss: 0.21889 training acc: 0.93656 testing acc: 0.9324\n",
      "epoch: 4 iter: 225 loss: 0.21487 training acc: 0.93842 testing acc: 0.9336\n",
      "epoch: 4 iter: 226 loss: 0.22155 training acc: 0.93402 testing acc: 0.9296\n",
      "epoch: 4 iter: 227 loss: 0.21527 training acc: 0.93608 testing acc: 0.9304\n",
      "epoch: 4 iter: 228 loss: 0.22567 training acc: 0.93474 testing acc: 0.9298\n",
      "epoch: 4 iter: 229 loss: 0.22015 training acc: 0.93572 testing acc: 0.9318\n",
      "epoch: 4 iter: 230 loss: 0.22274 training acc: 0.93304 testing acc: 0.9272\n",
      "epoch: 4 iter: 231 loss: 0.21851 training acc: 0.93488 testing acc: 0.9286\n",
      "epoch: 4 iter: 232 loss: 0.21368 training acc: 0.93594 testing acc: 0.931\n",
      "epoch: 4 iter: 233 loss: 0.21635 training acc: 0.93738 testing acc: 0.9308\n",
      "epoch: 4 iter: 234 loss: 0.21763 training acc: 0.93724 testing acc: 0.931\n",
      "epoch: 4 iter: 235 loss: 0.21925 training acc: 0.93682 testing acc: 0.929\n",
      "epoch: 4 iter: 236 loss: 0.21754 training acc: 0.93612 testing acc: 0.9264\n",
      "epoch: 4 iter: 237 loss: 0.22226 training acc: 0.93406 testing acc: 0.926\n",
      "epoch: 4 iter: 238 loss: 0.20357 training acc: 0.94134 testing acc: 0.9388\n",
      "epoch: 4 iter: 239 loss: 0.20969 training acc: 0.93834 testing acc: 0.9324\n",
      "epoch: 4 iter: 240 loss: 0.20698 training acc: 0.94000 testing acc: 0.934\n",
      "epoch: 4 iter: 241 loss: 0.22069 training acc: 0.93404 testing acc: 0.9244\n",
      "epoch: 4 iter: 242 loss: 0.20739 training acc: 0.94038 testing acc: 0.935\n",
      "epoch: 4 iter: 243 loss: 0.22161 training acc: 0.93634 testing acc: 0.928\n",
      "epoch: 4 iter: 244 loss: 0.20290 training acc: 0.94208 testing acc: 0.9346\n",
      "epoch: 4 iter: 245 loss: 0.20750 training acc: 0.94114 testing acc: 0.9346\n",
      "epoch: 4 iter: 246 loss: 0.21444 training acc: 0.93706 testing acc: 0.9292\n",
      "epoch: 4 iter: 247 loss: 0.20592 training acc: 0.94056 testing acc: 0.933\n",
      "epoch: 4 iter: 248 loss: 0.20674 training acc: 0.94054 testing acc: 0.9352\n",
      "epoch: 4 iter: 249 loss: 0.21416 training acc: 0.93742 testing acc: 0.931\n",
      "epoch: 4 iter: 250 loss: 0.21011 training acc: 0.94032 testing acc: 0.9362\n",
      "epoch: 4 iter: 251 loss: 0.20668 training acc: 0.94018 testing acc: 0.9332\n",
      "epoch: 4 iter: 252 loss: 0.20530 training acc: 0.94026 testing acc: 0.9332\n",
      "epoch: 4 iter: 253 loss: 0.20795 training acc: 0.94064 testing acc: 0.9364\n",
      "epoch: 4 iter: 254 loss: 0.20692 training acc: 0.94058 testing acc: 0.9374\n",
      "epoch: 4 iter: 255 loss: 0.21093 training acc: 0.94018 testing acc: 0.937\n",
      "epoch: 4 iter: 256 loss: 0.20608 training acc: 0.94130 testing acc: 0.9394\n",
      "epoch: 4 iter: 257 loss: 0.21071 training acc: 0.93956 testing acc: 0.937\n",
      "epoch: 4 iter: 258 loss: 0.20289 training acc: 0.94118 testing acc: 0.937\n",
      "epoch: 4 iter: 259 loss: 0.20980 training acc: 0.93920 testing acc: 0.9358\n",
      "epoch: 4 iter: 260 loss: 0.20611 training acc: 0.94042 testing acc: 0.9336\n",
      "epoch: 4 iter: 261 loss: 0.20223 training acc: 0.94158 testing acc: 0.9376\n",
      "epoch: 4 iter: 262 loss: 0.21683 training acc: 0.93642 testing acc: 0.9286\n",
      "epoch: 4 iter: 263 loss: 0.20723 training acc: 0.93996 testing acc: 0.9346\n",
      "epoch: 4 iter: 264 loss: 0.20161 training acc: 0.94168 testing acc: 0.9358\n",
      "epoch: 4 iter: 265 loss: 0.19967 training acc: 0.94158 testing acc: 0.9358\n",
      "epoch: 4 iter: 266 loss: 0.19904 training acc: 0.94264 testing acc: 0.9372\n",
      "epoch: 4 iter: 267 loss: 0.21443 training acc: 0.93564 testing acc: 0.926\n",
      "epoch: 4 iter: 268 loss: 0.21807 training acc: 0.93432 testing acc: 0.9264\n",
      "epoch: 4 iter: 269 loss: 0.22332 training acc: 0.93416 testing acc: 0.925\n",
      "epoch: 4 iter: 270 loss: 0.21599 training acc: 0.93590 testing acc: 0.9308\n",
      "epoch: 4 iter: 271 loss: 0.20777 training acc: 0.94034 testing acc: 0.9348\n",
      "epoch: 4 iter: 272 loss: 0.21368 training acc: 0.93778 testing acc: 0.9308\n",
      "epoch: 4 iter: 273 loss: 0.20637 training acc: 0.93958 testing acc: 0.935\n",
      "epoch: 4 iter: 274 loss: 0.21244 training acc: 0.93740 testing acc: 0.9324\n",
      "epoch: 4 iter: 275 loss: 0.21245 training acc: 0.93928 testing acc: 0.9348\n",
      "epoch: 4 iter: 276 loss: 0.21640 training acc: 0.93926 testing acc: 0.9356\n",
      "epoch: 4 iter: 277 loss: 0.21386 training acc: 0.93988 testing acc: 0.9358\n",
      "epoch: 4 iter: 278 loss: 0.21224 training acc: 0.93694 testing acc: 0.9318\n",
      "epoch: 4 iter: 279 loss: 0.20262 training acc: 0.94188 testing acc: 0.9366\n",
      "epoch: 4 iter: 280 loss: 0.22313 training acc: 0.93616 testing acc: 0.933\n",
      "epoch: 4 iter: 281 loss: 0.21424 training acc: 0.93940 testing acc: 0.9324\n",
      "epoch: 4 iter: 282 loss: 0.21163 training acc: 0.93834 testing acc: 0.93\n",
      "epoch: 4 iter: 283 loss: 0.20742 training acc: 0.94004 testing acc: 0.933\n",
      "epoch: 4 iter: 284 loss: 0.20829 training acc: 0.94034 testing acc: 0.9318\n",
      "epoch: 4 iter: 285 loss: 0.20107 training acc: 0.94300 testing acc: 0.936\n",
      "epoch: 4 iter: 286 loss: 0.22005 training acc: 0.93660 testing acc: 0.9284\n",
      "epoch: 4 iter: 287 loss: 0.22775 training acc: 0.93152 testing acc: 0.921\n",
      "epoch: 4 iter: 288 loss: 0.20917 training acc: 0.93976 testing acc: 0.931\n",
      "epoch: 4 iter: 289 loss: 0.20265 training acc: 0.94106 testing acc: 0.9332\n",
      "epoch: 4 iter: 290 loss: 0.20169 training acc: 0.94272 testing acc: 0.9332\n",
      "epoch: 4 iter: 291 loss: 0.20035 training acc: 0.94308 testing acc: 0.935\n",
      "epoch: 4 iter: 292 loss: 0.20244 training acc: 0.94234 testing acc: 0.9348\n",
      "epoch: 4 iter: 293 loss: 0.20807 training acc: 0.93898 testing acc: 0.935\n",
      "epoch: 4 iter: 294 loss: 0.20005 training acc: 0.94288 testing acc: 0.9378\n",
      "epoch: 4 iter: 295 loss: 0.21209 training acc: 0.93948 testing acc: 0.936\n",
      "epoch: 4 iter: 296 loss: 0.20930 training acc: 0.94034 testing acc: 0.9354\n",
      "epoch: 4 iter: 297 loss: 0.19962 training acc: 0.94444 testing acc: 0.941\n",
      "epoch: 4 iter: 298 loss: 0.20991 training acc: 0.94190 testing acc: 0.9378\n",
      "epoch: 4 iter: 299 loss: 0.21010 training acc: 0.94152 testing acc: 0.936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 iter: 300 loss: 0.20785 training acc: 0.94232 testing acc: 0.9384\n",
      "epoch: 4 iter: 301 loss: 0.20602 training acc: 0.94258 testing acc: 0.9388\n",
      "epoch: 4 iter: 302 loss: 0.20816 training acc: 0.94206 testing acc: 0.9392\n",
      "epoch: 4 iter: 303 loss: 0.21634 training acc: 0.93956 testing acc: 0.9364\n",
      "epoch: 4 iter: 304 loss: 0.21486 training acc: 0.93936 testing acc: 0.9334\n",
      "epoch: 4 iter: 305 loss: 0.22134 training acc: 0.93602 testing acc: 0.9314\n",
      "epoch: 4 iter: 306 loss: 0.20616 training acc: 0.94162 testing acc: 0.9366\n",
      "epoch: 4 iter: 307 loss: 0.22259 training acc: 0.93388 testing acc: 0.9266\n",
      "epoch: 4 iter: 308 loss: 0.23011 training acc: 0.93026 testing acc: 0.924\n",
      "epoch: 4 iter: 309 loss: 0.20422 training acc: 0.93990 testing acc: 0.9314\n",
      "epoch: 4 iter: 310 loss: 0.20466 training acc: 0.94008 testing acc: 0.9352\n",
      "epoch: 4 iter: 311 loss: 0.20460 training acc: 0.94160 testing acc: 0.932\n",
      "epoch: 4 iter: 312 loss: 0.21747 training acc: 0.93700 testing acc: 0.9264\n",
      "epoch: 4 iter: 313 loss: 0.20609 training acc: 0.93944 testing acc: 0.934\n",
      "epoch: 4 iter: 314 loss: 0.21317 training acc: 0.93864 testing acc: 0.9328\n",
      "epoch: 4 iter: 315 loss: 0.20287 training acc: 0.94248 testing acc: 0.9396\n",
      "epoch: 4 iter: 316 loss: 0.21736 training acc: 0.93828 testing acc: 0.9338\n",
      "epoch: 4 iter: 317 loss: 0.20943 training acc: 0.94112 testing acc: 0.9386\n",
      "epoch: 4 iter: 318 loss: 0.22548 training acc: 0.93378 testing acc: 0.93\n",
      "epoch: 4 iter: 319 loss: 0.21232 training acc: 0.93628 testing acc: 0.9274\n",
      "epoch: 4 iter: 320 loss: 0.21133 training acc: 0.93844 testing acc: 0.9296\n",
      "epoch: 4 iter: 321 loss: 0.20290 training acc: 0.94174 testing acc: 0.9382\n",
      "epoch: 4 iter: 322 loss: 0.20508 training acc: 0.94104 testing acc: 0.9326\n",
      "epoch: 4 iter: 323 loss: 0.24384 training acc: 0.92986 testing acc: 0.925\n",
      "epoch: 4 iter: 324 loss: 0.21559 training acc: 0.93846 testing acc: 0.9318\n",
      "epoch: 4 iter: 325 loss: 0.21470 training acc: 0.93796 testing acc: 0.9338\n",
      "epoch: 4 iter: 326 loss: 0.20410 training acc: 0.94160 testing acc: 0.9378\n",
      "epoch: 4 iter: 327 loss: 0.20417 training acc: 0.94288 testing acc: 0.9406\n",
      "epoch: 4 iter: 328 loss: 0.20431 training acc: 0.94276 testing acc: 0.94\n",
      "epoch: 4 iter: 329 loss: 0.20501 training acc: 0.94162 testing acc: 0.937\n",
      "epoch: 4 iter: 330 loss: 0.19982 training acc: 0.94248 testing acc: 0.9382\n",
      "epoch: 4 iter: 331 loss: 0.20638 training acc: 0.93916 testing acc: 0.9314\n",
      "epoch: 4 iter: 332 loss: 0.21156 training acc: 0.93756 testing acc: 0.9318\n",
      "epoch: 4 iter: 333 loss: 0.20361 training acc: 0.94130 testing acc: 0.9316\n",
      "epoch: 4 iter: 334 loss: 0.20813 training acc: 0.94182 testing acc: 0.9336\n",
      "epoch: 4 iter: 335 loss: 0.21472 training acc: 0.93918 testing acc: 0.9354\n",
      "epoch: 4 iter: 336 loss: 0.21222 training acc: 0.94022 testing acc: 0.9354\n",
      "epoch: 4 iter: 337 loss: 0.21465 training acc: 0.93890 testing acc: 0.9334\n",
      "epoch: 4 iter: 338 loss: 0.24145 training acc: 0.93108 testing acc: 0.9262\n",
      "epoch: 4 iter: 339 loss: 0.20372 training acc: 0.94420 testing acc: 0.9362\n",
      "epoch: 4 iter: 340 loss: 0.20934 training acc: 0.93974 testing acc: 0.9316\n",
      "epoch: 4 iter: 341 loss: 0.20632 training acc: 0.94122 testing acc: 0.932\n",
      "epoch: 4 iter: 342 loss: 0.19857 training acc: 0.94442 testing acc: 0.9376\n",
      "epoch: 4 iter: 343 loss: 0.19967 training acc: 0.94326 testing acc: 0.938\n",
      "epoch: 4 iter: 344 loss: 0.22154 training acc: 0.93340 testing acc: 0.9302\n",
      "epoch: 4 iter: 345 loss: 0.21250 training acc: 0.93772 testing acc: 0.932\n",
      "epoch: 4 iter: 346 loss: 0.21315 training acc: 0.93652 testing acc: 0.932\n",
      "epoch: 4 iter: 347 loss: 0.21549 training acc: 0.93562 testing acc: 0.9298\n",
      "epoch: 4 iter: 348 loss: 0.20744 training acc: 0.93934 testing acc: 0.9324\n",
      "epoch: 4 iter: 349 loss: 0.21936 training acc: 0.93500 testing acc: 0.9294\n",
      "epoch: 4 iter: 350 loss: 0.20060 training acc: 0.94200 testing acc: 0.9322\n",
      "epoch: 4 iter: 351 loss: 0.20525 training acc: 0.94232 testing acc: 0.9382\n",
      "epoch: 4 iter: 352 loss: 0.20770 training acc: 0.93976 testing acc: 0.9352\n",
      "epoch: 4 iter: 353 loss: 0.21050 training acc: 0.94104 testing acc: 0.9384\n",
      "epoch: 4 iter: 354 loss: 0.19950 training acc: 0.94278 testing acc: 0.9378\n",
      "epoch: 4 iter: 355 loss: 0.20175 training acc: 0.94338 testing acc: 0.937\n",
      "epoch: 4 iter: 356 loss: 0.20450 training acc: 0.94068 testing acc: 0.932\n",
      "epoch: 4 iter: 357 loss: 0.19892 training acc: 0.94292 testing acc: 0.9366\n",
      "epoch: 4 iter: 358 loss: 0.20104 training acc: 0.94144 testing acc: 0.9356\n",
      "epoch: 4 iter: 359 loss: 0.20084 training acc: 0.94174 testing acc: 0.9358\n",
      "epoch: 4 iter: 360 loss: 0.20823 training acc: 0.93862 testing acc: 0.935\n",
      "epoch: 4 iter: 361 loss: 0.21520 training acc: 0.93562 testing acc: 0.935\n",
      "epoch: 4 iter: 362 loss: 0.20644 training acc: 0.93764 testing acc: 0.9308\n",
      "epoch: 4 iter: 363 loss: 0.20425 training acc: 0.94070 testing acc: 0.937\n",
      "epoch: 4 iter: 364 loss: 0.20077 training acc: 0.94208 testing acc: 0.936\n",
      "epoch: 4 iter: 365 loss: 0.19891 training acc: 0.94180 testing acc: 0.9376\n",
      "epoch: 4 iter: 366 loss: 0.20526 training acc: 0.94096 testing acc: 0.9354\n",
      "epoch: 4 iter: 367 loss: 0.20450 training acc: 0.94164 testing acc: 0.9374\n",
      "epoch: 4 iter: 368 loss: 0.20463 training acc: 0.94028 testing acc: 0.934\n",
      "epoch: 4 iter: 369 loss: 0.20080 training acc: 0.94194 testing acc: 0.9366\n",
      "epoch: 4 iter: 370 loss: 0.21664 training acc: 0.93724 testing acc: 0.933\n",
      "epoch: 4 iter: 371 loss: 0.24174 training acc: 0.92550 testing acc: 0.9158\n",
      "epoch: 4 iter: 372 loss: 0.21122 training acc: 0.93796 testing acc: 0.9284\n",
      "epoch: 4 iter: 373 loss: 0.20817 training acc: 0.93760 testing acc: 0.9304\n",
      "epoch: 4 iter: 374 loss: 0.21948 training acc: 0.93344 testing acc: 0.9256\n",
      "epoch: 4 iter: 375 loss: 0.22893 training acc: 0.93118 testing acc: 0.9232\n",
      "epoch: 4 iter: 376 loss: 0.21247 training acc: 0.93716 testing acc: 0.9288\n",
      "epoch: 4 iter: 377 loss: 0.19153 training acc: 0.94580 testing acc: 0.9426\n",
      "epoch: 4 iter: 378 loss: 0.19795 training acc: 0.94436 testing acc: 0.9426\n",
      "epoch: 4 iter: 379 loss: 0.19554 training acc: 0.94446 testing acc: 0.9412\n",
      "epoch: 4 iter: 380 loss: 0.19952 training acc: 0.94392 testing acc: 0.9414\n",
      "epoch: 4 iter: 381 loss: 0.19163 training acc: 0.94620 testing acc: 0.9414\n",
      "epoch: 4 iter: 382 loss: 0.19492 training acc: 0.94490 testing acc: 0.941\n",
      "epoch: 4 iter: 383 loss: 0.18845 training acc: 0.94720 testing acc: 0.938\n",
      "epoch: 4 iter: 384 loss: 0.19203 training acc: 0.94534 testing acc: 0.9352\n",
      "epoch: 4 iter: 385 loss: 0.19167 training acc: 0.94432 testing acc: 0.9348\n",
      "epoch: 4 iter: 386 loss: 0.21752 training acc: 0.93728 testing acc: 0.9338\n",
      "epoch: 4 iter: 387 loss: 0.20087 training acc: 0.94408 testing acc: 0.9368\n",
      "epoch: 4 iter: 388 loss: 0.20180 training acc: 0.94324 testing acc: 0.9368\n",
      "epoch: 4 iter: 389 loss: 0.19923 training acc: 0.94326 testing acc: 0.9362\n",
      "epoch: 4 iter: 390 loss: 0.19618 training acc: 0.94522 testing acc: 0.941\n",
      "epoch: 4 iter: 391 loss: 0.19559 training acc: 0.94430 testing acc: 0.9384\n",
      "epoch: 4 iter: 392 loss: 0.19805 training acc: 0.94244 testing acc: 0.9376\n",
      "epoch: 4 iter: 393 loss: 0.19286 training acc: 0.94422 testing acc: 0.9398\n",
      "epoch: 4 iter: 394 loss: 0.19968 training acc: 0.94216 testing acc: 0.9378\n",
      "epoch: 4 iter: 395 loss: 0.19437 training acc: 0.94416 testing acc: 0.9384\n",
      "epoch: 4 iter: 396 loss: 0.20545 training acc: 0.94088 testing acc: 0.933\n",
      "epoch: 4 iter: 397 loss: 0.19961 training acc: 0.94356 testing acc: 0.9372\n",
      "epoch: 4 iter: 398 loss: 0.20023 training acc: 0.94232 testing acc: 0.9324\n",
      "epoch: 4 iter: 399 loss: 0.20471 training acc: 0.94128 testing acc: 0.9296\n",
      "epoch: 4 iter: 400 loss: 0.20861 training acc: 0.93938 testing acc: 0.935\n",
      "epoch: 4 iter: 401 loss: 0.19650 training acc: 0.94480 testing acc: 0.9372\n",
      "epoch: 4 iter: 402 loss: 0.20512 training acc: 0.94248 testing acc: 0.936\n",
      "epoch: 4 iter: 403 loss: 0.19497 training acc: 0.94460 testing acc: 0.9382\n",
      "epoch: 4 iter: 404 loss: 0.21152 training acc: 0.93750 testing acc: 0.9304\n",
      "epoch: 4 iter: 405 loss: 0.20253 training acc: 0.94110 testing acc: 0.932\n",
      "epoch: 4 iter: 406 loss: 0.20174 training acc: 0.94034 testing acc: 0.9298\n",
      "epoch: 4 iter: 407 loss: 0.20382 training acc: 0.94002 testing acc: 0.9314\n",
      "epoch: 4 iter: 408 loss: 0.21351 training acc: 0.93772 testing acc: 0.9268\n",
      "epoch: 4 iter: 409 loss: 0.19865 training acc: 0.94256 testing acc: 0.9332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 iter: 410 loss: 0.19487 training acc: 0.94570 testing acc: 0.939\n",
      "epoch: 4 iter: 411 loss: 0.19515 training acc: 0.94506 testing acc: 0.9364\n",
      "epoch: 4 iter: 412 loss: 0.23530 training acc: 0.93256 testing acc: 0.9254\n",
      "epoch: 4 iter: 413 loss: 0.21418 training acc: 0.93898 testing acc: 0.9322\n",
      "epoch: 4 iter: 414 loss: 0.19552 training acc: 0.94524 testing acc: 0.941\n",
      "epoch: 4 iter: 415 loss: 0.21110 training acc: 0.94016 testing acc: 0.939\n",
      "epoch: 4 iter: 416 loss: 0.20312 training acc: 0.94262 testing acc: 0.9408\n",
      "epoch: 4 iter: 417 loss: 0.19808 training acc: 0.94310 testing acc: 0.9382\n",
      "epoch: 4 iter: 418 loss: 0.20098 training acc: 0.94272 testing acc: 0.9374\n",
      "epoch: 4 iter: 419 loss: 0.21711 training acc: 0.93822 testing acc: 0.9326\n",
      "epoch: 4 iter: 420 loss: 0.21099 training acc: 0.93830 testing acc: 0.929\n",
      "epoch: 4 iter: 421 loss: 0.22894 training acc: 0.93180 testing acc: 0.923\n",
      "epoch: 4 iter: 422 loss: 0.22482 training acc: 0.93126 testing acc: 0.9242\n",
      "epoch: 4 iter: 423 loss: 0.20099 training acc: 0.94108 testing acc: 0.9322\n",
      "epoch: 4 iter: 424 loss: 0.21605 training acc: 0.93554 testing acc: 0.9278\n",
      "epoch: 4 iter: 425 loss: 0.20415 training acc: 0.93962 testing acc: 0.9306\n",
      "epoch: 4 iter: 426 loss: 0.19780 training acc: 0.94332 testing acc: 0.9356\n",
      "epoch: 4 iter: 427 loss: 0.20250 training acc: 0.94190 testing acc: 0.9358\n",
      "epoch: 4 iter: 428 loss: 0.19925 training acc: 0.94380 testing acc: 0.9396\n",
      "epoch: 4 iter: 429 loss: 0.19536 training acc: 0.94544 testing acc: 0.9382\n",
      "epoch: 4 iter: 430 loss: 0.18989 training acc: 0.94692 testing acc: 0.9418\n",
      "epoch: 4 iter: 431 loss: 0.19466 training acc: 0.94470 testing acc: 0.9394\n",
      "epoch: 4 iter: 432 loss: 0.19188 training acc: 0.94590 testing acc: 0.9402\n",
      "epoch: 4 iter: 433 loss: 0.19312 training acc: 0.94390 testing acc: 0.9374\n",
      "epoch: 4 iter: 434 loss: 0.19118 training acc: 0.94470 testing acc: 0.9392\n",
      "epoch: 4 iter: 435 loss: 0.19260 training acc: 0.94412 testing acc: 0.9398\n",
      "epoch: 4 iter: 436 loss: 0.19132 training acc: 0.94522 testing acc: 0.94\n",
      "epoch: 4 iter: 437 loss: 0.20008 training acc: 0.94358 testing acc: 0.9392\n",
      "epoch: 4 iter: 438 loss: 0.21313 training acc: 0.93898 testing acc: 0.9352\n",
      "epoch: 4 iter: 439 loss: 0.20671 training acc: 0.94224 testing acc: 0.9384\n",
      "epoch: 4 iter: 440 loss: 0.22951 training acc: 0.93544 testing acc: 0.9292\n",
      "epoch: 4 iter: 441 loss: 0.20660 training acc: 0.94120 testing acc: 0.936\n",
      "epoch: 4 iter: 442 loss: 0.20915 training acc: 0.93920 testing acc: 0.9336\n",
      "epoch: 4 iter: 443 loss: 0.21167 training acc: 0.93574 testing acc: 0.9286\n",
      "epoch: 4 iter: 444 loss: 0.19883 training acc: 0.94070 testing acc: 0.9348\n",
      "epoch: 4 iter: 445 loss: 0.19174 training acc: 0.94348 testing acc: 0.9364\n",
      "epoch: 4 iter: 446 loss: 0.19925 training acc: 0.94124 testing acc: 0.9332\n",
      "epoch: 4 iter: 447 loss: 0.19567 training acc: 0.94500 testing acc: 0.9424\n",
      "epoch: 4 iter: 448 loss: 0.19549 training acc: 0.94438 testing acc: 0.9412\n",
      "epoch: 4 iter: 449 loss: 0.20928 training acc: 0.93758 testing acc: 0.9312\n",
      "epoch: 4 iter: 450 loss: 0.19885 training acc: 0.94224 testing acc: 0.9384\n",
      "epoch: 4 iter: 451 loss: 0.19172 training acc: 0.94548 testing acc: 0.9406\n",
      "epoch: 4 iter: 452 loss: 0.20421 training acc: 0.94214 testing acc: 0.9386\n",
      "epoch: 4 iter: 453 loss: 0.20868 training acc: 0.93990 testing acc: 0.9376\n",
      "epoch: 4 iter: 454 loss: 0.20068 training acc: 0.94312 testing acc: 0.9384\n",
      "epoch: 4 iter: 455 loss: 0.20593 training acc: 0.94068 testing acc: 0.936\n",
      "epoch: 4 iter: 456 loss: 0.20481 training acc: 0.94152 testing acc: 0.937\n",
      "epoch: 4 iter: 457 loss: 0.20716 training acc: 0.93892 testing acc: 0.9346\n",
      "epoch: 4 iter: 458 loss: 0.19460 training acc: 0.94410 testing acc: 0.9394\n",
      "epoch: 4 iter: 459 loss: 0.19135 training acc: 0.94482 testing acc: 0.9398\n",
      "epoch: 4 iter: 460 loss: 0.18815 training acc: 0.94672 testing acc: 0.9396\n",
      "epoch: 4 iter: 461 loss: 0.20563 training acc: 0.94006 testing acc: 0.934\n",
      "epoch: 4 iter: 462 loss: 0.19047 training acc: 0.94564 testing acc: 0.9396\n",
      "epoch: 4 iter: 463 loss: 0.19435 training acc: 0.94386 testing acc: 0.9374\n",
      "epoch: 4 iter: 464 loss: 0.22144 training acc: 0.93624 testing acc: 0.9346\n",
      "epoch: 4 iter: 465 loss: 0.20023 training acc: 0.94308 testing acc: 0.9394\n",
      "epoch: 4 iter: 466 loss: 0.20034 training acc: 0.94326 testing acc: 0.9418\n",
      "epoch: 4 iter: 467 loss: 0.19449 training acc: 0.94444 testing acc: 0.9414\n",
      "epoch: 4 iter: 468 loss: 0.20297 training acc: 0.94230 testing acc: 0.9362\n",
      "epoch: 4 iter: 469 loss: 0.19957 training acc: 0.94298 testing acc: 0.9382\n",
      "epoch: 4 iter: 470 loss: 0.19556 training acc: 0.94222 testing acc: 0.9382\n",
      "epoch: 4 iter: 471 loss: 0.19481 training acc: 0.94388 testing acc: 0.94\n",
      "epoch: 4 iter: 472 loss: 0.19513 training acc: 0.94524 testing acc: 0.9432\n",
      "epoch: 4 iter: 473 loss: 0.20478 training acc: 0.94074 testing acc: 0.9398\n",
      "epoch: 4 iter: 474 loss: 0.20067 training acc: 0.94266 testing acc: 0.9382\n",
      "epoch: 4 iter: 475 loss: 0.19267 training acc: 0.94522 testing acc: 0.9402\n",
      "epoch: 4 iter: 476 loss: 0.20043 training acc: 0.94150 testing acc: 0.9354\n",
      "epoch: 4 iter: 477 loss: 0.20582 training acc: 0.94094 testing acc: 0.9352\n",
      "epoch: 4 iter: 478 loss: 0.23115 training acc: 0.93162 testing acc: 0.9278\n",
      "epoch: 4 iter: 479 loss: 0.22448 training acc: 0.93538 testing acc: 0.9306\n",
      "epoch: 4 iter: 480 loss: 0.20534 training acc: 0.94154 testing acc: 0.9374\n",
      "epoch: 4 iter: 481 loss: 0.19762 training acc: 0.94476 testing acc: 0.9384\n",
      "epoch: 4 iter: 482 loss: 0.22668 training acc: 0.93332 testing acc: 0.9276\n",
      "epoch: 4 iter: 483 loss: 0.20394 training acc: 0.94210 testing acc: 0.9378\n",
      "epoch: 4 iter: 484 loss: 0.20604 training acc: 0.94132 testing acc: 0.9378\n",
      "epoch: 4 iter: 485 loss: 0.20851 training acc: 0.94104 testing acc: 0.9356\n",
      "epoch: 4 iter: 486 loss: 0.22337 training acc: 0.93486 testing acc: 0.9308\n",
      "epoch: 4 iter: 487 loss: 0.20672 training acc: 0.94054 testing acc: 0.939\n",
      "epoch: 4 iter: 488 loss: 0.23955 training acc: 0.92598 testing acc: 0.9218\n",
      "epoch: 4 iter: 489 loss: 0.20522 training acc: 0.93908 testing acc: 0.9362\n",
      "epoch: 4 iter: 490 loss: 0.20195 training acc: 0.94212 testing acc: 0.936\n",
      "epoch: 4 iter: 491 loss: 0.19948 training acc: 0.94360 testing acc: 0.9374\n",
      "epoch: 4 iter: 492 loss: 0.19550 training acc: 0.94288 testing acc: 0.94\n",
      "epoch: 4 iter: 493 loss: 0.18937 training acc: 0.94588 testing acc: 0.944\n",
      "epoch: 4 iter: 494 loss: 0.19804 training acc: 0.94392 testing acc: 0.94\n",
      "epoch: 4 iter: 495 loss: 0.19344 training acc: 0.94296 testing acc: 0.9398\n",
      "epoch: 4 iter: 496 loss: 0.19832 training acc: 0.94090 testing acc: 0.9358\n",
      "epoch: 4 iter: 497 loss: 0.19680 training acc: 0.94078 testing acc: 0.9378\n",
      "epoch: 4 iter: 498 loss: 0.19441 training acc: 0.94076 testing acc: 0.94\n",
      "epoch: 4 iter: 499 loss: 0.21794 training acc: 0.93254 testing acc: 0.9276\n",
      "epoch: 4 iter: 500 loss: 0.18917 training acc: 0.94606 testing acc: 0.9424\n",
      "epoch: 4 iter: 501 loss: 0.18798 training acc: 0.94664 testing acc: 0.9446\n",
      "epoch: 4 iter: 502 loss: 0.18872 training acc: 0.94594 testing acc: 0.9434\n",
      "epoch: 4 iter: 503 loss: 0.18499 training acc: 0.94594 testing acc: 0.9432\n",
      "epoch: 4 iter: 504 loss: 0.18921 training acc: 0.94460 testing acc: 0.9394\n",
      "epoch: 4 iter: 505 loss: 0.20484 training acc: 0.93998 testing acc: 0.9322\n",
      "epoch: 4 iter: 506 loss: 0.19809 training acc: 0.94222 testing acc: 0.9342\n",
      "epoch: 4 iter: 507 loss: 0.20191 training acc: 0.94118 testing acc: 0.9372\n",
      "epoch: 4 iter: 508 loss: 0.19691 training acc: 0.94266 testing acc: 0.9358\n",
      "epoch: 4 iter: 509 loss: 0.20839 training acc: 0.93932 testing acc: 0.9292\n",
      "epoch: 4 iter: 510 loss: 0.21395 training acc: 0.93706 testing acc: 0.9302\n",
      "epoch: 4 iter: 511 loss: 0.20364 training acc: 0.94018 testing acc: 0.9352\n",
      "epoch: 4 iter: 512 loss: 0.18792 training acc: 0.94632 testing acc: 0.9426\n",
      "epoch: 4 iter: 513 loss: 0.19225 training acc: 0.94434 testing acc: 0.9378\n",
      "epoch: 4 iter: 514 loss: 0.21507 training acc: 0.93398 testing acc: 0.9284\n",
      "epoch: 4 iter: 515 loss: 0.21440 training acc: 0.93500 testing acc: 0.9262\n",
      "epoch: 4 iter: 516 loss: 0.22923 training acc: 0.93004 testing acc: 0.921\n",
      "epoch: 4 iter: 517 loss: 0.22073 training acc: 0.93274 testing acc: 0.9252\n",
      "epoch: 4 iter: 518 loss: 0.19023 training acc: 0.94450 testing acc: 0.9416\n",
      "epoch: 4 iter: 519 loss: 0.19272 training acc: 0.94528 testing acc: 0.9398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 iter: 520 loss: 0.18937 training acc: 0.94552 testing acc: 0.9432\n",
      "epoch: 4 iter: 521 loss: 0.19552 training acc: 0.94280 testing acc: 0.9384\n",
      "epoch: 4 iter: 522 loss: 0.19411 training acc: 0.94302 testing acc: 0.938\n",
      "epoch: 4 iter: 523 loss: 0.19337 training acc: 0.94364 testing acc: 0.939\n",
      "epoch: 4 iter: 524 loss: 0.19733 training acc: 0.94226 testing acc: 0.941\n",
      "epoch: 4 iter: 525 loss: 0.19411 training acc: 0.94344 testing acc: 0.9408\n",
      "epoch: 4 iter: 526 loss: 0.18813 training acc: 0.94590 testing acc: 0.9452\n",
      "epoch: 4 iter: 527 loss: 0.19846 training acc: 0.94066 testing acc: 0.938\n",
      "epoch: 4 iter: 528 loss: 0.19420 training acc: 0.94300 testing acc: 0.9384\n",
      "epoch: 4 iter: 529 loss: 0.20404 training acc: 0.93930 testing acc: 0.9358\n",
      "epoch: 4 iter: 530 loss: 0.20043 training acc: 0.94058 testing acc: 0.9384\n",
      "epoch: 4 iter: 531 loss: 0.19919 training acc: 0.94064 testing acc: 0.9368\n",
      "epoch: 4 iter: 532 loss: 0.19000 training acc: 0.94364 testing acc: 0.938\n",
      "epoch: 4 iter: 533 loss: 0.19033 training acc: 0.94330 testing acc: 0.9386\n",
      "epoch: 4 iter: 534 loss: 0.19402 training acc: 0.94238 testing acc: 0.9382\n",
      "epoch: 4 iter: 535 loss: 0.18835 training acc: 0.94402 testing acc: 0.9408\n",
      "epoch: 4 iter: 536 loss: 0.18853 training acc: 0.94356 testing acc: 0.9396\n",
      "epoch: 4 iter: 537 loss: 0.18597 training acc: 0.94430 testing acc: 0.9416\n",
      "epoch: 4 iter: 538 loss: 0.18401 training acc: 0.94646 testing acc: 0.9424\n",
      "epoch: 4 iter: 539 loss: 0.18476 training acc: 0.94698 testing acc: 0.9442\n",
      "epoch: 4 iter: 540 loss: 0.18539 training acc: 0.94666 testing acc: 0.9448\n",
      "epoch: 4 iter: 541 loss: 0.18198 training acc: 0.94818 testing acc: 0.9452\n",
      "epoch: 4 iter: 542 loss: 0.21991 training acc: 0.93504 testing acc: 0.9288\n",
      "epoch: 4 iter: 543 loss: 0.23229 training acc: 0.93288 testing acc: 0.926\n",
      "epoch: 4 iter: 544 loss: 0.20260 training acc: 0.94360 testing acc: 0.938\n",
      "epoch: 4 iter: 545 loss: 0.20197 training acc: 0.94184 testing acc: 0.9382\n",
      "epoch: 4 iter: 546 loss: 0.19603 training acc: 0.94266 testing acc: 0.9394\n",
      "epoch: 4 iter: 547 loss: 0.19056 training acc: 0.94480 testing acc: 0.9428\n",
      "epoch: 4 iter: 548 loss: 0.19386 training acc: 0.94372 testing acc: 0.942\n",
      "epoch: 4 iter: 549 loss: 0.19062 training acc: 0.94492 testing acc: 0.9438\n",
      "epoch: 4 iter: 550 loss: 0.19656 training acc: 0.94308 testing acc: 0.9416\n",
      "epoch: 4 iter: 551 loss: 0.19678 training acc: 0.94122 testing acc: 0.9378\n",
      "epoch: 4 iter: 552 loss: 0.20012 training acc: 0.94004 testing acc: 0.934\n",
      "epoch: 4 iter: 553 loss: 0.23447 training acc: 0.93054 testing acc: 0.9308\n",
      "epoch: 4 iter: 554 loss: 0.20782 training acc: 0.93760 testing acc: 0.932\n",
      "epoch: 4 iter: 555 loss: 0.21215 training acc: 0.93684 testing acc: 0.9302\n",
      "epoch: 4 iter: 556 loss: 0.19477 training acc: 0.94244 testing acc: 0.94\n",
      "epoch: 4 iter: 557 loss: 0.20297 training acc: 0.93952 testing acc: 0.936\n",
      "epoch: 4 iter: 558 loss: 0.20873 training acc: 0.93944 testing acc: 0.9364\n",
      "epoch: 4 iter: 559 loss: 0.19643 training acc: 0.94350 testing acc: 0.9406\n",
      "epoch: 4 iter: 560 loss: 0.18794 training acc: 0.94568 testing acc: 0.9428\n",
      "epoch: 4 iter: 561 loss: 0.18310 training acc: 0.94728 testing acc: 0.9458\n",
      "epoch: 4 iter: 562 loss: 0.19140 training acc: 0.94510 testing acc: 0.944\n",
      "epoch: 4 iter: 563 loss: 0.20041 training acc: 0.94112 testing acc: 0.9406\n",
      "epoch: 4 iter: 564 loss: 0.18970 training acc: 0.94506 testing acc: 0.9424\n",
      "epoch: 4 iter: 565 loss: 0.23085 training acc: 0.93308 testing acc: 0.9258\n",
      "epoch: 4 iter: 566 loss: 0.19774 training acc: 0.94272 testing acc: 0.9372\n",
      "epoch: 4 iter: 567 loss: 0.19669 training acc: 0.94320 testing acc: 0.939\n",
      "epoch: 4 iter: 568 loss: 0.18853 training acc: 0.94538 testing acc: 0.9426\n",
      "epoch: 4 iter: 569 loss: 0.18264 training acc: 0.94696 testing acc: 0.9462\n",
      "epoch: 4 iter: 570 loss: 0.20722 training acc: 0.93848 testing acc: 0.9348\n",
      "epoch: 4 iter: 571 loss: 0.20500 training acc: 0.93744 testing acc: 0.9306\n",
      "epoch: 4 iter: 572 loss: 0.19351 training acc: 0.94166 testing acc: 0.9372\n",
      "epoch: 4 iter: 573 loss: 0.20920 training acc: 0.93570 testing acc: 0.9308\n",
      "epoch: 4 iter: 574 loss: 0.18894 training acc: 0.94440 testing acc: 0.9412\n",
      "epoch: 4 iter: 575 loss: 0.20032 training acc: 0.94100 testing acc: 0.9384\n",
      "epoch: 4 iter: 576 loss: 0.20501 training acc: 0.93858 testing acc: 0.9346\n",
      "epoch: 4 iter: 577 loss: 0.19553 training acc: 0.94166 testing acc: 0.936\n",
      "epoch: 4 iter: 578 loss: 0.18507 training acc: 0.94548 testing acc: 0.9452\n",
      "epoch: 4 iter: 579 loss: 0.17814 training acc: 0.94874 testing acc: 0.9446\n",
      "epoch: 4 iter: 580 loss: 0.17677 training acc: 0.94968 testing acc: 0.9458\n",
      "epoch: 4 iter: 581 loss: 0.18010 training acc: 0.94830 testing acc: 0.9424\n",
      "epoch: 4 iter: 582 loss: 0.18775 training acc: 0.94484 testing acc: 0.9396\n",
      "epoch: 4 iter: 583 loss: 0.18671 training acc: 0.94452 testing acc: 0.9404\n",
      "epoch: 4 iter: 584 loss: 0.18872 training acc: 0.94414 testing acc: 0.9374\n",
      "epoch: 4 iter: 585 loss: 0.19707 training acc: 0.94114 testing acc: 0.9342\n",
      "epoch: 4 iter: 586 loss: 0.17713 training acc: 0.94816 testing acc: 0.9438\n",
      "epoch: 4 iter: 587 loss: 0.18118 training acc: 0.94740 testing acc: 0.9436\n",
      "epoch: 4 iter: 588 loss: 0.18646 training acc: 0.94512 testing acc: 0.939\n",
      "epoch: 4 iter: 589 loss: 0.18381 training acc: 0.94596 testing acc: 0.9404\n",
      "epoch: 4 iter: 590 loss: 0.18363 training acc: 0.94610 testing acc: 0.9416\n",
      "epoch: 4 iter: 591 loss: 0.19778 training acc: 0.94196 testing acc: 0.936\n",
      "epoch: 4 iter: 592 loss: 0.18983 training acc: 0.94472 testing acc: 0.9398\n",
      "epoch: 4 iter: 593 loss: 0.18207 training acc: 0.94674 testing acc: 0.9426\n",
      "epoch: 4 iter: 594 loss: 0.18258 training acc: 0.94634 testing acc: 0.9414\n",
      "epoch: 4 iter: 595 loss: 0.17824 training acc: 0.94818 testing acc: 0.9436\n",
      "epoch: 4 iter: 596 loss: 0.18347 training acc: 0.94694 testing acc: 0.9402\n",
      "epoch: 4 iter: 597 loss: 0.17980 training acc: 0.94886 testing acc: 0.943\n",
      "epoch: 4 iter: 598 loss: 0.18131 training acc: 0.94776 testing acc: 0.9418\n",
      "epoch: 4 iter: 599 loss: 0.18009 training acc: 0.94792 testing acc: 0.9436\n",
      "epoch: 4 iter: 600 loss: 0.18477 training acc: 0.94638 testing acc: 0.9412\n",
      "epoch: 4 iter: 601 loss: 0.18044 training acc: 0.94850 testing acc: 0.944\n",
      "epoch: 4 iter: 602 loss: 0.17724 training acc: 0.94836 testing acc: 0.9448\n",
      "epoch: 4 iter: 603 loss: 0.18033 training acc: 0.94728 testing acc: 0.9416\n",
      "epoch: 4 iter: 604 loss: 0.18192 training acc: 0.94702 testing acc: 0.9442\n",
      "epoch: 4 iter: 605 loss: 0.17835 training acc: 0.94854 testing acc: 0.9452\n",
      "epoch: 4 iter: 606 loss: 0.17869 training acc: 0.94896 testing acc: 0.9432\n",
      "epoch: 4 iter: 607 loss: 0.17651 training acc: 0.94986 testing acc: 0.9452\n",
      "epoch: 4 iter: 608 loss: 0.19122 training acc: 0.94524 testing acc: 0.9436\n",
      "epoch: 4 iter: 609 loss: 0.17807 training acc: 0.94858 testing acc: 0.946\n",
      "epoch: 4 iter: 610 loss: 0.17599 training acc: 0.94960 testing acc: 0.9454\n",
      "epoch: 4 iter: 611 loss: 0.18022 training acc: 0.94918 testing acc: 0.9462\n",
      "epoch: 4 iter: 612 loss: 0.17912 training acc: 0.94986 testing acc: 0.949\n",
      "epoch: 4 iter: 613 loss: 0.18417 training acc: 0.94876 testing acc: 0.9482\n",
      "epoch: 4 iter: 614 loss: 0.18373 training acc: 0.94858 testing acc: 0.9476\n",
      "epoch: 4 iter: 615 loss: 0.20868 training acc: 0.94174 testing acc: 0.9382\n",
      "epoch: 4 iter: 616 loss: 0.19189 training acc: 0.94564 testing acc: 0.946\n",
      "epoch: 4 iter: 617 loss: 0.19177 training acc: 0.94608 testing acc: 0.9444\n",
      "epoch: 4 iter: 618 loss: 0.17831 training acc: 0.94894 testing acc: 0.9454\n",
      "epoch: 4 iter: 619 loss: 0.18196 training acc: 0.94672 testing acc: 0.9434\n",
      "epoch: 4 iter: 620 loss: 0.17761 training acc: 0.94792 testing acc: 0.945\n",
      "epoch: 4 iter: 621 loss: 0.18191 training acc: 0.94738 testing acc: 0.9458\n",
      "epoch: 4 iter: 622 loss: 0.17323 training acc: 0.94964 testing acc: 0.9476\n",
      "epoch: 4 iter: 623 loss: 0.17695 training acc: 0.94870 testing acc: 0.9478\n",
      "epoch: 4 iter: 624 loss: 0.18057 training acc: 0.94732 testing acc: 0.9454\n",
      "epoch: 4 iter: 625 loss: 0.19425 training acc: 0.94294 testing acc: 0.94\n",
      "epoch: 4 iter: 626 loss: 0.21692 training acc: 0.93664 testing acc: 0.9298\n",
      "epoch: 4 iter: 627 loss: 0.18943 training acc: 0.94586 testing acc: 0.9412\n",
      "epoch: 4 iter: 628 loss: 0.19217 training acc: 0.94466 testing acc: 0.9408\n",
      "epoch: 4 iter: 629 loss: 0.18377 training acc: 0.94798 testing acc: 0.9474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 iter: 630 loss: 0.18333 training acc: 0.94796 testing acc: 0.9442\n",
      "epoch: 4 iter: 631 loss: 0.20314 training acc: 0.93860 testing acc: 0.9338\n",
      "epoch: 4 iter: 632 loss: 0.19600 training acc: 0.94176 testing acc: 0.9372\n",
      "epoch: 4 iter: 633 loss: 0.20012 training acc: 0.94196 testing acc: 0.9376\n",
      "epoch: 4 iter: 634 loss: 0.23644 training acc: 0.92688 testing acc: 0.923\n",
      "epoch: 4 iter: 635 loss: 0.19881 training acc: 0.94122 testing acc: 0.9342\n",
      "epoch: 4 iter: 636 loss: 0.19136 training acc: 0.94300 testing acc: 0.9382\n",
      "epoch: 4 iter: 637 loss: 0.17831 training acc: 0.94814 testing acc: 0.945\n",
      "epoch: 4 iter: 638 loss: 0.18780 training acc: 0.94400 testing acc: 0.9392\n",
      "epoch: 4 iter: 639 loss: 0.19020 training acc: 0.94472 testing acc: 0.945\n",
      "epoch: 4 iter: 640 loss: 0.18389 training acc: 0.94600 testing acc: 0.945\n",
      "epoch: 4 iter: 641 loss: 0.17905 training acc: 0.94848 testing acc: 0.946\n",
      "epoch: 4 iter: 642 loss: 0.18588 training acc: 0.94506 testing acc: 0.9426\n",
      "epoch: 4 iter: 643 loss: 0.19994 training acc: 0.94222 testing acc: 0.9388\n",
      "epoch: 4 iter: 644 loss: 0.19252 training acc: 0.94500 testing acc: 0.9432\n",
      "epoch: 4 iter: 645 loss: 0.18493 training acc: 0.94712 testing acc: 0.9442\n",
      "epoch: 4 iter: 646 loss: 0.18280 training acc: 0.94738 testing acc: 0.9438\n",
      "epoch: 4 iter: 647 loss: 0.18379 training acc: 0.94748 testing acc: 0.9454\n",
      "epoch: 4 iter: 648 loss: 0.18271 training acc: 0.94744 testing acc: 0.9436\n",
      "epoch: 4 iter: 649 loss: 0.17477 training acc: 0.94950 testing acc: 0.9484\n",
      "epoch: 4 iter: 650 loss: 0.17987 training acc: 0.94810 testing acc: 0.947\n",
      "epoch: 4 iter: 651 loss: 0.17536 training acc: 0.94874 testing acc: 0.947\n",
      "epoch: 4 iter: 652 loss: 0.18679 training acc: 0.94580 testing acc: 0.9436\n",
      "epoch: 4 iter: 653 loss: 0.18852 training acc: 0.94492 testing acc: 0.942\n",
      "epoch: 4 iter: 654 loss: 0.19173 training acc: 0.94448 testing acc: 0.945\n",
      "epoch: 4 iter: 655 loss: 0.18383 training acc: 0.94722 testing acc: 0.9452\n",
      "epoch: 4 iter: 656 loss: 0.17847 training acc: 0.94826 testing acc: 0.9458\n",
      "epoch: 4 iter: 657 loss: 0.17201 training acc: 0.95062 testing acc: 0.9486\n",
      "epoch: 4 iter: 658 loss: 0.17478 training acc: 0.95022 testing acc: 0.9472\n",
      "epoch: 4 iter: 659 loss: 0.18985 training acc: 0.94454 testing acc: 0.94\n",
      "epoch: 4 iter: 660 loss: 0.18018 training acc: 0.94862 testing acc: 0.9438\n",
      "epoch: 4 iter: 661 loss: 0.18131 training acc: 0.94852 testing acc: 0.945\n",
      "epoch: 4 iter: 662 loss: 0.17512 training acc: 0.95062 testing acc: 0.9494\n",
      "epoch: 4 iter: 663 loss: 0.18738 training acc: 0.94654 testing acc: 0.945\n",
      "epoch: 4 iter: 664 loss: 0.18071 training acc: 0.94926 testing acc: 0.9484\n",
      "epoch: 4 iter: 665 loss: 0.17947 training acc: 0.94972 testing acc: 0.9486\n",
      "epoch: 4 iter: 666 loss: 0.18707 training acc: 0.94792 testing acc: 0.9442\n",
      "epoch: 4 iter: 667 loss: 0.18085 training acc: 0.94782 testing acc: 0.9442\n",
      "epoch: 4 iter: 668 loss: 0.18146 training acc: 0.94842 testing acc: 0.9448\n",
      "epoch: 4 iter: 669 loss: 0.18342 training acc: 0.94710 testing acc: 0.9428\n",
      "epoch: 4 iter: 670 loss: 0.17361 training acc: 0.95106 testing acc: 0.9476\n",
      "epoch: 4 iter: 671 loss: 0.17233 training acc: 0.95148 testing acc: 0.9482\n",
      "epoch: 4 iter: 672 loss: 0.17337 training acc: 0.95112 testing acc: 0.9464\n",
      "epoch: 4 iter: 673 loss: 0.17639 training acc: 0.95106 testing acc: 0.9456\n",
      "epoch: 4 iter: 674 loss: 0.18693 training acc: 0.94784 testing acc: 0.9408\n",
      "epoch: 4 iter: 675 loss: 0.17836 training acc: 0.95056 testing acc: 0.9446\n",
      "epoch: 4 iter: 676 loss: 0.18337 training acc: 0.94860 testing acc: 0.9414\n",
      "epoch: 4 iter: 677 loss: 0.17803 training acc: 0.95082 testing acc: 0.9442\n",
      "epoch: 4 iter: 678 loss: 0.18153 training acc: 0.95012 testing acc: 0.945\n",
      "epoch: 4 iter: 679 loss: 0.18645 training acc: 0.94564 testing acc: 0.9402\n",
      "epoch: 4 iter: 680 loss: 0.17936 training acc: 0.94810 testing acc: 0.9418\n",
      "epoch: 4 iter: 681 loss: 0.23436 training acc: 0.92956 testing acc: 0.9236\n",
      "epoch: 4 iter: 682 loss: 0.21025 training acc: 0.93852 testing acc: 0.9336\n",
      "epoch: 4 iter: 683 loss: 0.18784 training acc: 0.94608 testing acc: 0.9414\n",
      "epoch: 4 iter: 684 loss: 0.17922 training acc: 0.94842 testing acc: 0.945\n",
      "epoch: 4 iter: 685 loss: 0.17983 training acc: 0.94852 testing acc: 0.9434\n",
      "epoch: 4 iter: 686 loss: 0.16939 training acc: 0.95152 testing acc: 0.9462\n",
      "epoch: 4 iter: 687 loss: 0.17971 training acc: 0.94816 testing acc: 0.9424\n",
      "epoch: 4 iter: 688 loss: 0.17915 training acc: 0.94914 testing acc: 0.941\n",
      "epoch: 4 iter: 689 loss: 0.17924 training acc: 0.94878 testing acc: 0.9418\n",
      "epoch: 4 iter: 690 loss: 0.17401 training acc: 0.95058 testing acc: 0.9424\n",
      "epoch: 4 iter: 691 loss: 0.17806 training acc: 0.94948 testing acc: 0.941\n",
      "epoch: 4 iter: 692 loss: 0.18158 training acc: 0.94872 testing acc: 0.943\n",
      "epoch: 4 iter: 693 loss: 0.18217 training acc: 0.94798 testing acc: 0.9384\n",
      "epoch: 4 iter: 694 loss: 0.17982 training acc: 0.94842 testing acc: 0.937\n",
      "epoch: 4 iter: 695 loss: 0.20925 training acc: 0.93850 testing acc: 0.932\n",
      "epoch: 4 iter: 696 loss: 0.17467 training acc: 0.95036 testing acc: 0.9436\n",
      "epoch: 4 iter: 697 loss: 0.17219 training acc: 0.95118 testing acc: 0.944\n",
      "epoch: 4 iter: 698 loss: 0.17082 training acc: 0.95240 testing acc: 0.9456\n",
      "epoch: 4 iter: 699 loss: 0.17155 training acc: 0.95174 testing acc: 0.9458\n",
      "epoch: 4 iter: 700 loss: 0.16861 training acc: 0.95228 testing acc: 0.9462\n",
      "epoch: 4 iter: 701 loss: 0.17855 training acc: 0.94886 testing acc: 0.9404\n",
      "epoch: 4 iter: 702 loss: 0.18314 training acc: 0.94852 testing acc: 0.9416\n",
      "epoch: 4 iter: 703 loss: 0.18652 training acc: 0.94636 testing acc: 0.9428\n",
      "epoch: 4 iter: 704 loss: 0.18156 training acc: 0.94874 testing acc: 0.9442\n",
      "epoch: 4 iter: 705 loss: 0.17772 training acc: 0.94862 testing acc: 0.9416\n",
      "epoch: 4 iter: 706 loss: 0.18658 training acc: 0.94516 testing acc: 0.9388\n",
      "epoch: 4 iter: 707 loss: 0.17828 training acc: 0.94810 testing acc: 0.9428\n",
      "epoch: 4 iter: 708 loss: 0.17843 training acc: 0.94996 testing acc: 0.9418\n",
      "epoch: 4 iter: 709 loss: 0.18167 training acc: 0.94804 testing acc: 0.9402\n",
      "epoch: 4 iter: 710 loss: 0.17797 training acc: 0.94974 testing acc: 0.944\n",
      "epoch: 4 iter: 711 loss: 0.17898 training acc: 0.94996 testing acc: 0.9426\n",
      "epoch: 4 iter: 712 loss: 0.17835 training acc: 0.94932 testing acc: 0.9452\n",
      "epoch: 4 iter: 713 loss: 0.17327 training acc: 0.95152 testing acc: 0.9466\n",
      "epoch: 4 iter: 714 loss: 0.17915 training acc: 0.94912 testing acc: 0.943\n",
      "epoch: 4 iter: 715 loss: 0.19142 training acc: 0.94568 testing acc: 0.9424\n",
      "epoch: 4 iter: 716 loss: 0.18483 training acc: 0.94686 testing acc: 0.94\n",
      "epoch: 4 iter: 717 loss: 0.18911 training acc: 0.94524 testing acc: 0.9374\n",
      "epoch: 4 iter: 718 loss: 0.19958 training acc: 0.94166 testing acc: 0.9358\n",
      "epoch: 4 iter: 719 loss: 0.18038 training acc: 0.94840 testing acc: 0.9414\n",
      "epoch: 4 iter: 720 loss: 0.18021 training acc: 0.94844 testing acc: 0.9408\n",
      "epoch: 4 iter: 721 loss: 0.17453 training acc: 0.95010 testing acc: 0.9418\n",
      "epoch: 4 iter: 722 loss: 0.18456 training acc: 0.94544 testing acc: 0.9358\n",
      "epoch: 4 iter: 723 loss: 0.18214 training acc: 0.94696 testing acc: 0.9402\n",
      "epoch: 4 iter: 724 loss: 0.18505 training acc: 0.94566 testing acc: 0.937\n",
      "epoch: 4 iter: 725 loss: 0.18497 training acc: 0.94602 testing acc: 0.9374\n",
      "epoch: 4 iter: 726 loss: 0.18405 training acc: 0.94608 testing acc: 0.9378\n",
      "epoch: 4 iter: 727 loss: 0.17418 training acc: 0.95008 testing acc: 0.9426\n",
      "epoch: 4 iter: 728 loss: 0.17758 training acc: 0.94908 testing acc: 0.9434\n",
      "epoch: 4 iter: 729 loss: 0.17262 training acc: 0.95092 testing acc: 0.9406\n",
      "epoch: 4 iter: 730 loss: 0.17233 training acc: 0.95132 testing acc: 0.9402\n",
      "epoch: 4 iter: 731 loss: 0.20098 training acc: 0.94084 testing acc: 0.933\n",
      "epoch: 4 iter: 732 loss: 0.18492 training acc: 0.94604 testing acc: 0.939\n",
      "epoch: 4 iter: 733 loss: 0.17175 training acc: 0.95080 testing acc: 0.943\n",
      "epoch: 4 iter: 734 loss: 0.17082 training acc: 0.95066 testing acc: 0.9436\n",
      "epoch: 4 iter: 735 loss: 0.16776 training acc: 0.95102 testing acc: 0.9428\n",
      "epoch: 4 iter: 736 loss: 0.18011 training acc: 0.94848 testing acc: 0.9398\n",
      "epoch: 4 iter: 737 loss: 0.18838 training acc: 0.94490 testing acc: 0.9396\n",
      "epoch: 4 iter: 738 loss: 0.17387 training acc: 0.94988 testing acc: 0.9432\n",
      "epoch: 4 iter: 739 loss: 0.17223 training acc: 0.95036 testing acc: 0.9428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 iter: 740 loss: 0.17257 training acc: 0.94948 testing acc: 0.9432\n",
      "epoch: 4 iter: 741 loss: 0.16883 training acc: 0.95092 testing acc: 0.9462\n",
      "epoch: 4 iter: 742 loss: 0.19165 training acc: 0.94246 testing acc: 0.9358\n",
      "epoch: 4 iter: 743 loss: 0.19295 training acc: 0.94300 testing acc: 0.9392\n",
      "epoch: 4 iter: 744 loss: 0.19500 training acc: 0.94236 testing acc: 0.9366\n",
      "epoch: 4 iter: 745 loss: 0.17651 training acc: 0.94886 testing acc: 0.9452\n",
      "epoch: 4 iter: 746 loss: 0.17280 training acc: 0.95058 testing acc: 0.9456\n",
      "epoch: 4 iter: 747 loss: 0.17005 training acc: 0.95304 testing acc: 0.9466\n",
      "epoch: 4 iter: 748 loss: 0.16909 training acc: 0.95338 testing acc: 0.9476\n",
      "epoch: 4 iter: 749 loss: 0.17590 training acc: 0.94984 testing acc: 0.945\n",
      "epoch: 4 iter: 750 loss: 0.17469 training acc: 0.94958 testing acc: 0.945\n",
      "epoch: 4 iter: 751 loss: 0.16987 training acc: 0.95124 testing acc: 0.9452\n",
      "epoch: 4 iter: 752 loss: 0.16805 training acc: 0.95210 testing acc: 0.9458\n",
      "epoch: 4 iter: 753 loss: 0.18732 training acc: 0.94618 testing acc: 0.9418\n",
      "epoch: 4 iter: 754 loss: 0.17628 training acc: 0.95042 testing acc: 0.9452\n",
      "epoch: 4 iter: 755 loss: 0.17167 training acc: 0.95188 testing acc: 0.9452\n",
      "epoch: 4 iter: 756 loss: 0.16695 training acc: 0.95304 testing acc: 0.9464\n",
      "epoch: 4 iter: 757 loss: 0.19316 training acc: 0.94380 testing acc: 0.9376\n",
      "epoch: 4 iter: 758 loss: 0.18161 training acc: 0.94708 testing acc: 0.9424\n",
      "epoch: 4 iter: 759 loss: 0.18517 training acc: 0.94722 testing acc: 0.9414\n",
      "epoch: 4 iter: 760 loss: 0.17912 training acc: 0.94862 testing acc: 0.9426\n",
      "epoch: 4 iter: 761 loss: 0.19871 training acc: 0.94382 testing acc: 0.938\n",
      "epoch: 4 iter: 762 loss: 0.16987 training acc: 0.95198 testing acc: 0.9432\n",
      "epoch: 4 iter: 763 loss: 0.17044 training acc: 0.95170 testing acc: 0.946\n",
      "epoch: 4 iter: 764 loss: 0.17714 training acc: 0.95018 testing acc: 0.9426\n",
      "epoch: 4 iter: 765 loss: 0.16659 training acc: 0.95260 testing acc: 0.9476\n",
      "epoch: 4 iter: 766 loss: 0.16823 training acc: 0.95208 testing acc: 0.947\n",
      "epoch: 4 iter: 767 loss: 0.17057 training acc: 0.95146 testing acc: 0.944\n",
      "epoch: 4 iter: 768 loss: 0.17181 training acc: 0.95108 testing acc: 0.9464\n",
      "epoch: 4 iter: 769 loss: 0.17349 training acc: 0.95034 testing acc: 0.9466\n",
      "epoch: 4 iter: 770 loss: 0.18528 training acc: 0.94554 testing acc: 0.9408\n",
      "epoch: 4 iter: 771 loss: 0.19514 training acc: 0.94228 testing acc: 0.937\n",
      "epoch: 4 iter: 772 loss: 0.17334 training acc: 0.95052 testing acc: 0.9464\n",
      "epoch: 4 iter: 773 loss: 0.16756 training acc: 0.95240 testing acc: 0.948\n",
      "epoch: 4 iter: 774 loss: 0.16592 training acc: 0.95224 testing acc: 0.9486\n",
      "epoch: 4 iter: 775 loss: 0.16519 training acc: 0.95308 testing acc: 0.9468\n",
      "epoch: 4 iter: 776 loss: 0.17372 training acc: 0.95068 testing acc: 0.9466\n",
      "epoch: 4 iter: 777 loss: 0.17298 training acc: 0.95056 testing acc: 0.9468\n",
      "epoch: 4 iter: 778 loss: 0.17165 training acc: 0.95124 testing acc: 0.9482\n",
      "epoch: 4 iter: 779 loss: 0.17983 training acc: 0.94932 testing acc: 0.9448\n",
      "epoch: 4 iter: 780 loss: 0.17238 training acc: 0.95118 testing acc: 0.9464\n",
      "epoch: 4 iter: 781 loss: 0.16624 training acc: 0.95352 testing acc: 0.9494\n",
      "epoch: 4 iter: 782 loss: 0.17471 training acc: 0.94994 testing acc: 0.9474\n",
      "epoch: 4 iter: 783 loss: 0.16496 training acc: 0.95242 testing acc: 0.9466\n",
      "epoch: 4 iter: 784 loss: 0.16343 training acc: 0.95330 testing acc: 0.9474\n",
      "epoch: 4 iter: 785 loss: 0.16598 training acc: 0.95318 testing acc: 0.9462\n",
      "epoch: 4 iter: 786 loss: 0.17654 training acc: 0.94936 testing acc: 0.9456\n",
      "epoch: 4 iter: 787 loss: 0.17533 training acc: 0.94938 testing acc: 0.946\n",
      "epoch: 4 iter: 788 loss: 0.17589 training acc: 0.94838 testing acc: 0.9462\n",
      "epoch: 4 iter: 789 loss: 0.17546 training acc: 0.95068 testing acc: 0.943\n",
      "epoch: 4 iter: 790 loss: 0.16668 training acc: 0.95314 testing acc: 0.9468\n",
      "epoch: 4 iter: 791 loss: 0.16558 training acc: 0.95248 testing acc: 0.9458\n",
      "epoch: 4 iter: 792 loss: 0.16923 training acc: 0.95068 testing acc: 0.9446\n",
      "epoch: 4 iter: 793 loss: 0.17620 training acc: 0.94910 testing acc: 0.9434\n",
      "epoch: 4 iter: 794 loss: 0.16784 training acc: 0.95154 testing acc: 0.9458\n",
      "epoch: 4 iter: 795 loss: 0.16247 training acc: 0.95356 testing acc: 0.9458\n",
      "epoch: 4 iter: 796 loss: 0.16584 training acc: 0.95428 testing acc: 0.945\n",
      "epoch: 4 iter: 797 loss: 0.16348 training acc: 0.95378 testing acc: 0.9458\n",
      "epoch: 4 iter: 798 loss: 0.17308 training acc: 0.95246 testing acc: 0.9474\n",
      "epoch: 4 iter: 799 loss: 0.17179 training acc: 0.95238 testing acc: 0.9458\n",
      "epoch: 4 iter: 800 loss: 0.17517 training acc: 0.95068 testing acc: 0.944\n",
      "epoch: 4 iter: 801 loss: 0.16521 training acc: 0.95334 testing acc: 0.9486\n",
      "epoch: 4 iter: 802 loss: 0.17213 training acc: 0.95138 testing acc: 0.9462\n",
      "epoch: 4 iter: 803 loss: 0.16786 training acc: 0.95194 testing acc: 0.9436\n",
      "epoch: 4 iter: 804 loss: 0.16786 training acc: 0.95296 testing acc: 0.9494\n",
      "epoch: 4 iter: 805 loss: 0.16824 training acc: 0.95254 testing acc: 0.9458\n",
      "epoch: 4 iter: 806 loss: 0.16745 training acc: 0.95236 testing acc: 0.9458\n",
      "epoch: 4 iter: 807 loss: 0.16796 training acc: 0.95174 testing acc: 0.9466\n",
      "epoch: 4 iter: 808 loss: 0.16787 training acc: 0.95156 testing acc: 0.9454\n",
      "epoch: 4 iter: 809 loss: 0.16698 training acc: 0.95234 testing acc: 0.9462\n",
      "epoch: 4 iter: 810 loss: 0.17343 training acc: 0.95158 testing acc: 0.9446\n",
      "epoch: 4 iter: 811 loss: 0.17136 training acc: 0.95186 testing acc: 0.9484\n",
      "epoch: 4 iter: 812 loss: 0.16472 training acc: 0.95328 testing acc: 0.9474\n",
      "epoch: 4 iter: 813 loss: 0.16646 training acc: 0.95202 testing acc: 0.9466\n",
      "epoch: 4 iter: 814 loss: 0.17384 training acc: 0.95056 testing acc: 0.9446\n",
      "epoch: 4 iter: 815 loss: 0.16974 training acc: 0.95236 testing acc: 0.9476\n",
      "epoch: 4 iter: 816 loss: 0.16608 training acc: 0.95374 testing acc: 0.948\n",
      "epoch: 4 iter: 817 loss: 0.17259 training acc: 0.95200 testing acc: 0.9442\n",
      "epoch: 4 iter: 818 loss: 0.17125 training acc: 0.95196 testing acc: 0.9444\n",
      "epoch: 4 iter: 819 loss: 0.16341 training acc: 0.95408 testing acc: 0.947\n",
      "epoch: 4 iter: 820 loss: 0.16355 training acc: 0.95338 testing acc: 0.9458\n",
      "epoch: 4 iter: 821 loss: 0.17325 training acc: 0.94996 testing acc: 0.942\n",
      "epoch: 4 iter: 822 loss: 0.16530 training acc: 0.95356 testing acc: 0.9454\n",
      "epoch: 4 iter: 823 loss: 0.17402 training acc: 0.95080 testing acc: 0.941\n",
      "epoch: 4 iter: 824 loss: 0.18680 training acc: 0.94546 testing acc: 0.9378\n",
      "epoch: 4 iter: 825 loss: 0.17770 training acc: 0.94822 testing acc: 0.9396\n",
      "epoch: 4 iter: 826 loss: 0.18063 training acc: 0.94692 testing acc: 0.939\n",
      "epoch: 4 iter: 827 loss: 0.16604 training acc: 0.95152 testing acc: 0.9452\n",
      "epoch: 4 iter: 828 loss: 0.16751 training acc: 0.95208 testing acc: 0.9416\n",
      "epoch: 4 iter: 829 loss: 0.17102 training acc: 0.94986 testing acc: 0.945\n",
      "epoch: 4 iter: 830 loss: 0.16637 training acc: 0.95152 testing acc: 0.9436\n",
      "epoch: 4 iter: 831 loss: 0.16661 training acc: 0.95224 testing acc: 0.9448\n",
      "epoch: 4 iter: 832 loss: 0.16434 training acc: 0.95354 testing acc: 0.9474\n",
      "epoch: 4 iter: 833 loss: 0.17198 training acc: 0.95146 testing acc: 0.9456\n",
      "epoch: 4 iter: 834 loss: 0.17716 training acc: 0.95014 testing acc: 0.945\n",
      "epoch: 4 iter: 835 loss: 0.16992 training acc: 0.95194 testing acc: 0.9464\n",
      "epoch: 4 iter: 836 loss: 0.17085 training acc: 0.95174 testing acc: 0.9466\n",
      "epoch: 4 iter: 837 loss: 0.16581 training acc: 0.95306 testing acc: 0.9478\n",
      "epoch: 4 iter: 838 loss: 0.16192 training acc: 0.95426 testing acc: 0.9486\n",
      "epoch: 4 iter: 839 loss: 0.16519 training acc: 0.95260 testing acc: 0.947\n",
      "epoch: 4 iter: 840 loss: 0.16323 training acc: 0.95344 testing acc: 0.9462\n",
      "epoch: 4 iter: 841 loss: 0.16817 training acc: 0.95186 testing acc: 0.946\n",
      "epoch: 4 iter: 842 loss: 0.17210 training acc: 0.94986 testing acc: 0.9458\n",
      "epoch: 4 iter: 843 loss: 0.17229 training acc: 0.94994 testing acc: 0.9454\n",
      "epoch: 4 iter: 844 loss: 0.16213 training acc: 0.95348 testing acc: 0.9476\n",
      "epoch: 4 iter: 845 loss: 0.16976 training acc: 0.95044 testing acc: 0.9456\n",
      "epoch: 4 iter: 846 loss: 0.16091 training acc: 0.95458 testing acc: 0.9474\n",
      "epoch: 4 iter: 847 loss: 0.16041 training acc: 0.95428 testing acc: 0.95\n",
      "epoch: 4 iter: 848 loss: 0.16255 training acc: 0.95378 testing acc: 0.95\n",
      "epoch: 4 iter: 849 loss: 0.16916 training acc: 0.95064 testing acc: 0.946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 iter: 850 loss: 0.16877 training acc: 0.95122 testing acc: 0.9442\n",
      "epoch: 4 iter: 851 loss: 0.15986 training acc: 0.95434 testing acc: 0.9478\n",
      "epoch: 4 iter: 852 loss: 0.18667 training acc: 0.94602 testing acc: 0.9416\n",
      "epoch: 4 iter: 853 loss: 0.18125 training acc: 0.94866 testing acc: 0.9436\n",
      "epoch: 4 iter: 854 loss: 0.21255 training acc: 0.93840 testing acc: 0.9346\n",
      "epoch: 4 iter: 855 loss: 0.18554 training acc: 0.94610 testing acc: 0.943\n",
      "epoch: 4 iter: 856 loss: 0.17288 training acc: 0.95102 testing acc: 0.947\n",
      "epoch: 4 iter: 857 loss: 0.17333 training acc: 0.95048 testing acc: 0.9478\n",
      "epoch: 4 iter: 858 loss: 0.16765 training acc: 0.95198 testing acc: 0.9496\n",
      "epoch: 4 iter: 859 loss: 0.16334 training acc: 0.95260 testing acc: 0.95\n",
      "epoch: 4 iter: 860 loss: 0.17113 training acc: 0.95078 testing acc: 0.946\n",
      "epoch: 4 iter: 861 loss: 0.17608 training acc: 0.94812 testing acc: 0.942\n",
      "epoch: 4 iter: 862 loss: 0.18995 training acc: 0.94414 testing acc: 0.9362\n",
      "epoch: 4 iter: 863 loss: 0.18761 training acc: 0.94490 testing acc: 0.9392\n",
      "epoch: 4 iter: 864 loss: 0.18314 training acc: 0.94474 testing acc: 0.9396\n",
      "epoch: 4 iter: 865 loss: 0.17375 training acc: 0.94800 testing acc: 0.9418\n",
      "epoch: 4 iter: 866 loss: 0.17492 training acc: 0.94798 testing acc: 0.9432\n",
      "epoch: 4 iter: 867 loss: 0.16917 training acc: 0.94974 testing acc: 0.9436\n",
      "epoch: 4 iter: 868 loss: 0.16274 training acc: 0.95302 testing acc: 0.947\n",
      "epoch: 4 iter: 869 loss: 0.17311 training acc: 0.95074 testing acc: 0.9436\n",
      "epoch: 4 iter: 870 loss: 0.16424 training acc: 0.95432 testing acc: 0.946\n",
      "epoch: 4 iter: 871 loss: 0.16171 training acc: 0.95428 testing acc: 0.948\n",
      "epoch: 4 iter: 872 loss: 0.16546 training acc: 0.95276 testing acc: 0.9488\n",
      "epoch: 4 iter: 873 loss: 0.17434 training acc: 0.94980 testing acc: 0.9454\n",
      "epoch: 4 iter: 874 loss: 0.17773 training acc: 0.94778 testing acc: 0.9474\n",
      "epoch: 4 iter: 875 loss: 0.16103 training acc: 0.95394 testing acc: 0.9486\n",
      "epoch: 4 iter: 876 loss: 0.15893 training acc: 0.95444 testing acc: 0.948\n",
      "epoch: 4 iter: 877 loss: 0.17313 training acc: 0.95088 testing acc: 0.9422\n",
      "epoch: 4 iter: 878 loss: 0.16956 training acc: 0.95144 testing acc: 0.9436\n",
      "epoch: 4 iter: 879 loss: 0.16160 training acc: 0.95418 testing acc: 0.9458\n",
      "epoch: 4 iter: 880 loss: 0.15690 training acc: 0.95574 testing acc: 0.948\n",
      "epoch: 4 iter: 881 loss: 0.16084 training acc: 0.95280 testing acc: 0.944\n",
      "epoch: 4 iter: 882 loss: 0.16156 training acc: 0.95314 testing acc: 0.9442\n",
      "epoch: 4 iter: 883 loss: 0.16579 training acc: 0.95264 testing acc: 0.9462\n",
      "epoch: 4 iter: 884 loss: 0.16001 training acc: 0.95444 testing acc: 0.9458\n",
      "epoch: 4 iter: 885 loss: 0.16375 training acc: 0.95360 testing acc: 0.9474\n",
      "epoch: 4 iter: 886 loss: 0.16587 training acc: 0.95322 testing acc: 0.9464\n",
      "epoch: 4 iter: 887 loss: 0.16052 training acc: 0.95496 testing acc: 0.947\n",
      "epoch: 4 iter: 888 loss: 0.16289 training acc: 0.95362 testing acc: 0.947\n",
      "epoch: 4 iter: 889 loss: 0.16224 training acc: 0.95324 testing acc: 0.9486\n",
      "epoch: 4 iter: 890 loss: 0.18907 training acc: 0.94642 testing acc: 0.944\n",
      "epoch: 4 iter: 891 loss: 0.17717 training acc: 0.94936 testing acc: 0.9466\n",
      "epoch: 4 iter: 892 loss: 0.19136 training acc: 0.94462 testing acc: 0.941\n",
      "epoch: 4 iter: 893 loss: 0.16779 training acc: 0.95282 testing acc: 0.9472\n",
      "epoch: 4 iter: 894 loss: 0.16665 training acc: 0.95224 testing acc: 0.948\n",
      "epoch: 4 iter: 895 loss: 0.18493 training acc: 0.94554 testing acc: 0.9426\n",
      "epoch: 4 iter: 896 loss: 0.21442 training acc: 0.93458 testing acc: 0.9304\n",
      "epoch: 4 iter: 897 loss: 0.17727 training acc: 0.94786 testing acc: 0.9458\n",
      "epoch: 4 iter: 898 loss: 0.17016 training acc: 0.95094 testing acc: 0.9446\n",
      "epoch: 4 iter: 899 loss: 0.17010 training acc: 0.95002 testing acc: 0.9448\n",
      "epoch: 4 iter: 900 loss: 0.20761 training acc: 0.93970 testing acc: 0.9356\n",
      "epoch: 4 iter: 901 loss: 0.16287 training acc: 0.95290 testing acc: 0.9498\n",
      "epoch: 4 iter: 902 loss: 0.17343 training acc: 0.94854 testing acc: 0.9444\n",
      "epoch: 4 iter: 903 loss: 0.16751 training acc: 0.95030 testing acc: 0.9458\n",
      "epoch: 4 iter: 904 loss: 0.16370 training acc: 0.95180 testing acc: 0.947\n",
      "epoch: 4 iter: 905 loss: 0.16728 training acc: 0.95144 testing acc: 0.942\n",
      "epoch: 4 iter: 906 loss: 0.17077 training acc: 0.94960 testing acc: 0.9406\n",
      "epoch: 4 iter: 907 loss: 0.16377 training acc: 0.95280 testing acc: 0.945\n",
      "epoch: 4 iter: 908 loss: 0.16154 training acc: 0.95390 testing acc: 0.9454\n",
      "epoch: 4 iter: 909 loss: 0.16559 training acc: 0.95360 testing acc: 0.9466\n",
      "epoch: 4 iter: 910 loss: 0.16117 training acc: 0.95408 testing acc: 0.9462\n",
      "epoch: 4 iter: 911 loss: 0.15647 training acc: 0.95496 testing acc: 0.9482\n",
      "epoch: 4 iter: 912 loss: 0.15896 training acc: 0.95358 testing acc: 0.9476\n",
      "epoch: 4 iter: 913 loss: 0.16885 training acc: 0.95022 testing acc: 0.9426\n",
      "epoch: 4 iter: 914 loss: 0.16422 training acc: 0.95200 testing acc: 0.9478\n",
      "epoch: 4 iter: 915 loss: 0.15443 training acc: 0.95532 testing acc: 0.9476\n",
      "epoch: 4 iter: 916 loss: 0.16270 training acc: 0.95336 testing acc: 0.946\n",
      "epoch: 4 iter: 917 loss: 0.17402 training acc: 0.94974 testing acc: 0.9432\n",
      "epoch: 4 iter: 918 loss: 0.15819 training acc: 0.95456 testing acc: 0.948\n",
      "epoch: 4 iter: 919 loss: 0.16167 training acc: 0.95362 testing acc: 0.9432\n",
      "epoch: 4 iter: 920 loss: 0.16367 training acc: 0.95264 testing acc: 0.9478\n",
      "epoch: 4 iter: 921 loss: 0.20199 training acc: 0.93964 testing acc: 0.9344\n",
      "epoch: 4 iter: 922 loss: 0.16495 training acc: 0.95234 testing acc: 0.9472\n",
      "epoch: 4 iter: 923 loss: 0.15969 training acc: 0.95478 testing acc: 0.9488\n",
      "epoch: 4 iter: 924 loss: 0.15369 training acc: 0.95602 testing acc: 0.9506\n",
      "epoch: 4 iter: 925 loss: 0.18431 training acc: 0.94738 testing acc: 0.9402\n",
      "epoch: 4 iter: 926 loss: 0.17674 training acc: 0.94900 testing acc: 0.9428\n",
      "epoch: 4 iter: 927 loss: 0.16088 training acc: 0.95492 testing acc: 0.9478\n",
      "epoch: 4 iter: 928 loss: 0.15894 training acc: 0.95526 testing acc: 0.9518\n",
      "epoch: 4 iter: 929 loss: 0.16002 training acc: 0.95550 testing acc: 0.9514\n",
      "epoch: 4 iter: 930 loss: 0.16608 training acc: 0.95264 testing acc: 0.95\n",
      "epoch: 4 iter: 931 loss: 0.18227 training acc: 0.94782 testing acc: 0.9442\n",
      "epoch: 4 iter: 932 loss: 0.17754 training acc: 0.94792 testing acc: 0.9462\n",
      "epoch: 4 iter: 933 loss: 0.18315 training acc: 0.94644 testing acc: 0.9414\n",
      "epoch: 4 iter: 934 loss: 0.16646 training acc: 0.95218 testing acc: 0.9498\n",
      "epoch: 4 iter: 935 loss: 0.16427 training acc: 0.95230 testing acc: 0.949\n",
      "epoch: 4 iter: 936 loss: 0.15741 training acc: 0.95496 testing acc: 0.9498\n",
      "epoch: 4 iter: 937 loss: 0.15666 training acc: 0.95516 testing acc: 0.9476\n",
      "epoch: 4 iter: 938 loss: 0.17251 training acc: 0.94914 testing acc: 0.9432\n",
      "epoch: 4 iter: 939 loss: 0.16334 training acc: 0.95272 testing acc: 0.9456\n",
      "epoch: 4 iter: 940 loss: 0.17029 training acc: 0.95046 testing acc: 0.942\n",
      "epoch: 4 iter: 941 loss: 0.16210 training acc: 0.95454 testing acc: 0.9502\n",
      "epoch: 4 iter: 942 loss: 0.17435 training acc: 0.94884 testing acc: 0.9418\n",
      "epoch: 4 iter: 943 loss: 0.15955 training acc: 0.95496 testing acc: 0.9492\n",
      "epoch: 4 iter: 944 loss: 0.15901 training acc: 0.95592 testing acc: 0.948\n",
      "epoch: 4 iter: 945 loss: 0.17625 training acc: 0.94986 testing acc: 0.9402\n",
      "epoch: 4 iter: 946 loss: 0.16614 training acc: 0.95300 testing acc: 0.9448\n",
      "epoch: 4 iter: 947 loss: 0.16345 training acc: 0.95382 testing acc: 0.9484\n",
      "epoch: 4 iter: 948 loss: 0.16860 training acc: 0.95228 testing acc: 0.9482\n",
      "epoch: 4 iter: 949 loss: 0.15535 training acc: 0.95664 testing acc: 0.9492\n",
      "epoch: 4 iter: 950 loss: 0.15981 training acc: 0.95468 testing acc: 0.9496\n",
      "epoch: 4 iter: 951 loss: 0.16111 training acc: 0.95494 testing acc: 0.949\n",
      "epoch: 4 iter: 952 loss: 0.16509 training acc: 0.95312 testing acc: 0.9464\n",
      "epoch: 4 iter: 953 loss: 0.17164 training acc: 0.95056 testing acc: 0.9462\n",
      "epoch: 4 iter: 954 loss: 0.16132 training acc: 0.95452 testing acc: 0.9486\n",
      "epoch: 4 iter: 955 loss: 0.16262 training acc: 0.95380 testing acc: 0.947\n",
      "epoch: 4 iter: 956 loss: 0.16061 training acc: 0.95444 testing acc: 0.948\n",
      "epoch: 4 iter: 957 loss: 0.16251 training acc: 0.95406 testing acc: 0.9464\n",
      "epoch: 4 iter: 958 loss: 0.15431 training acc: 0.95624 testing acc: 0.9498\n",
      "epoch: 4 iter: 959 loss: 0.15327 training acc: 0.95682 testing acc: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 iter: 960 loss: 0.16560 training acc: 0.95276 testing acc: 0.9476\n",
      "epoch: 4 iter: 961 loss: 0.15930 training acc: 0.95450 testing acc: 0.948\n",
      "epoch: 4 iter: 962 loss: 0.15201 training acc: 0.95676 testing acc: 0.9512\n",
      "epoch: 4 iter: 963 loss: 0.14939 training acc: 0.95766 testing acc: 0.9514\n",
      "epoch: 4 iter: 964 loss: 0.15258 training acc: 0.95722 testing acc: 0.9504\n",
      "epoch: 4 iter: 965 loss: 0.15691 training acc: 0.95592 testing acc: 0.9496\n",
      "epoch: 4 iter: 966 loss: 0.17763 training acc: 0.95018 testing acc: 0.9484\n",
      "epoch: 4 iter: 967 loss: 0.15593 training acc: 0.95594 testing acc: 0.9488\n",
      "epoch: 4 iter: 968 loss: 0.16460 training acc: 0.95270 testing acc: 0.9466\n",
      "epoch: 4 iter: 969 loss: 0.16798 training acc: 0.95152 testing acc: 0.9448\n",
      "epoch: 4 iter: 970 loss: 0.15985 training acc: 0.95386 testing acc: 0.9466\n",
      "epoch: 4 iter: 971 loss: 0.15281 training acc: 0.95574 testing acc: 0.9476\n",
      "epoch: 4 iter: 972 loss: 0.15567 training acc: 0.95520 testing acc: 0.9462\n",
      "epoch: 4 iter: 973 loss: 0.15755 training acc: 0.95438 testing acc: 0.9464\n",
      "epoch: 4 iter: 974 loss: 0.16850 training acc: 0.94998 testing acc: 0.9448\n",
      "epoch: 4 iter: 975 loss: 0.15772 training acc: 0.95428 testing acc: 0.9508\n",
      "epoch: 4 iter: 976 loss: 0.16091 training acc: 0.95432 testing acc: 0.9508\n",
      "epoch: 4 iter: 977 loss: 0.15560 training acc: 0.95558 testing acc: 0.9502\n",
      "epoch: 4 iter: 978 loss: 0.16416 training acc: 0.95440 testing acc: 0.9488\n",
      "epoch: 4 iter: 979 loss: 0.17614 training acc: 0.94852 testing acc: 0.941\n",
      "epoch: 4 iter: 980 loss: 0.15650 training acc: 0.95662 testing acc: 0.9492\n",
      "epoch: 4 iter: 981 loss: 0.15782 training acc: 0.95564 testing acc: 0.9472\n",
      "epoch: 4 iter: 982 loss: 0.16962 training acc: 0.95042 testing acc: 0.9408\n",
      "epoch: 4 iter: 983 loss: 0.15244 training acc: 0.95722 testing acc: 0.9518\n",
      "epoch: 4 iter: 984 loss: 0.15422 training acc: 0.95628 testing acc: 0.9508\n",
      "epoch: 4 iter: 985 loss: 0.15101 training acc: 0.95720 testing acc: 0.9498\n",
      "epoch: 4 iter: 986 loss: 0.15754 training acc: 0.95608 testing acc: 0.9518\n",
      "epoch: 4 iter: 987 loss: 0.16648 training acc: 0.95302 testing acc: 0.9484\n",
      "epoch: 4 iter: 988 loss: 0.15337 training acc: 0.95584 testing acc: 0.9504\n",
      "epoch: 4 iter: 989 loss: 0.15653 training acc: 0.95508 testing acc: 0.9502\n",
      "epoch: 4 iter: 990 loss: 0.15688 training acc: 0.95404 testing acc: 0.9464\n",
      "epoch: 4 iter: 991 loss: 0.16106 training acc: 0.95288 testing acc: 0.944\n",
      "epoch: 4 iter: 992 loss: 0.16438 training acc: 0.95138 testing acc: 0.9422\n",
      "epoch: 4 iter: 993 loss: 0.16562 training acc: 0.95076 testing acc: 0.9434\n",
      "epoch: 4 iter: 994 loss: 0.16347 training acc: 0.95232 testing acc: 0.9454\n",
      "epoch: 4 iter: 995 loss: 0.15144 training acc: 0.95732 testing acc: 0.9522\n",
      "epoch: 4 iter: 996 loss: 0.15752 training acc: 0.95520 testing acc: 0.9506\n",
      "epoch: 4 iter: 997 loss: 0.15764 training acc: 0.95520 testing acc: 0.951\n",
      "epoch: 4 iter: 998 loss: 0.15951 training acc: 0.95382 testing acc: 0.9464\n",
      "epoch: 4 iter: 999 loss: 0.15603 training acc: 0.95676 testing acc: 0.9498\n",
      "epoch: 5 iter:   0 loss: 0.16256 training acc: 0.95608 testing acc: 0.9508\n",
      "epoch: 5 iter:   1 loss: 0.16438 training acc: 0.95460 testing acc: 0.9502\n",
      "epoch: 5 iter:   2 loss: 0.15623 training acc: 0.95666 testing acc: 0.9518\n",
      "epoch: 5 iter:   3 loss: 0.16077 training acc: 0.95372 testing acc: 0.9474\n",
      "epoch: 5 iter:   4 loss: 0.16527 training acc: 0.95372 testing acc: 0.9488\n",
      "epoch: 5 iter:   5 loss: 0.15462 training acc: 0.95632 testing acc: 0.95\n",
      "epoch: 5 iter:   6 loss: 0.15233 training acc: 0.95662 testing acc: 0.9518\n",
      "epoch: 5 iter:   7 loss: 0.15151 training acc: 0.95776 testing acc: 0.9526\n",
      "epoch: 5 iter:   8 loss: 0.15305 training acc: 0.95664 testing acc: 0.9518\n",
      "epoch: 5 iter:   9 loss: 0.15593 training acc: 0.95588 testing acc: 0.9492\n",
      "epoch: 5 iter:  10 loss: 0.15764 training acc: 0.95634 testing acc: 0.9492\n",
      "epoch: 5 iter:  11 loss: 0.16470 training acc: 0.95274 testing acc: 0.945\n",
      "epoch: 5 iter:  12 loss: 0.15269 training acc: 0.95766 testing acc: 0.9506\n",
      "epoch: 5 iter:  13 loss: 0.16090 training acc: 0.95394 testing acc: 0.9474\n",
      "epoch: 5 iter:  14 loss: 0.17236 training acc: 0.94850 testing acc: 0.9392\n",
      "epoch: 5 iter:  15 loss: 0.16237 training acc: 0.95360 testing acc: 0.9452\n",
      "epoch: 5 iter:  16 loss: 0.15820 training acc: 0.95512 testing acc: 0.9474\n",
      "epoch: 5 iter:  17 loss: 0.15701 training acc: 0.95488 testing acc: 0.9476\n",
      "epoch: 5 iter:  18 loss: 0.15491 training acc: 0.95576 testing acc: 0.947\n",
      "epoch: 5 iter:  19 loss: 0.15012 training acc: 0.95714 testing acc: 0.9478\n",
      "epoch: 5 iter:  20 loss: 0.16165 training acc: 0.95244 testing acc: 0.9448\n",
      "epoch: 5 iter:  21 loss: 0.15197 training acc: 0.95620 testing acc: 0.9494\n",
      "epoch: 5 iter:  22 loss: 0.17248 training acc: 0.95034 testing acc: 0.9424\n",
      "epoch: 5 iter:  23 loss: 0.15915 training acc: 0.95444 testing acc: 0.9488\n",
      "epoch: 5 iter:  24 loss: 0.15631 training acc: 0.95516 testing acc: 0.9498\n",
      "epoch: 5 iter:  25 loss: 0.15370 training acc: 0.95522 testing acc: 0.9482\n",
      "epoch: 5 iter:  26 loss: 0.15625 training acc: 0.95496 testing acc: 0.9488\n",
      "epoch: 5 iter:  27 loss: 0.15417 training acc: 0.95424 testing acc: 0.9446\n",
      "epoch: 5 iter:  28 loss: 0.15188 training acc: 0.95524 testing acc: 0.9488\n",
      "epoch: 5 iter:  29 loss: 0.16706 training acc: 0.95070 testing acc: 0.945\n",
      "epoch: 5 iter:  30 loss: 0.15602 training acc: 0.95514 testing acc: 0.9476\n",
      "epoch: 5 iter:  31 loss: 0.15070 training acc: 0.95654 testing acc: 0.9494\n",
      "epoch: 5 iter:  32 loss: 0.14906 training acc: 0.95744 testing acc: 0.9502\n",
      "epoch: 5 iter:  33 loss: 0.14873 training acc: 0.95764 testing acc: 0.9498\n",
      "epoch: 5 iter:  34 loss: 0.14901 training acc: 0.95802 testing acc: 0.9504\n",
      "epoch: 5 iter:  35 loss: 0.15502 training acc: 0.95592 testing acc: 0.9492\n",
      "epoch: 5 iter:  36 loss: 0.16361 training acc: 0.95280 testing acc: 0.946\n",
      "epoch: 5 iter:  37 loss: 0.15101 training acc: 0.95554 testing acc: 0.9486\n",
      "epoch: 5 iter:  38 loss: 0.15655 training acc: 0.95374 testing acc: 0.9444\n",
      "epoch: 5 iter:  39 loss: 0.15298 training acc: 0.95540 testing acc: 0.9486\n",
      "epoch: 5 iter:  40 loss: 0.15135 training acc: 0.95538 testing acc: 0.9482\n",
      "epoch: 5 iter:  41 loss: 0.16027 training acc: 0.95316 testing acc: 0.9474\n",
      "epoch: 5 iter:  42 loss: 0.15172 training acc: 0.95644 testing acc: 0.9512\n",
      "epoch: 5 iter:  43 loss: 0.15035 training acc: 0.95656 testing acc: 0.9472\n",
      "epoch: 5 iter:  44 loss: 0.15200 training acc: 0.95658 testing acc: 0.9488\n",
      "epoch: 5 iter:  45 loss: 0.15386 training acc: 0.95602 testing acc: 0.9456\n",
      "epoch: 5 iter:  46 loss: 0.15729 training acc: 0.95506 testing acc: 0.9448\n",
      "epoch: 5 iter:  47 loss: 0.16240 training acc: 0.95274 testing acc: 0.945\n",
      "epoch: 5 iter:  48 loss: 0.15657 training acc: 0.95522 testing acc: 0.9498\n",
      "epoch: 5 iter:  49 loss: 0.15403 training acc: 0.95582 testing acc: 0.9494\n",
      "epoch: 5 iter:  50 loss: 0.15355 training acc: 0.95662 testing acc: 0.9506\n",
      "epoch: 5 iter:  51 loss: 0.15763 training acc: 0.95532 testing acc: 0.9498\n",
      "epoch: 5 iter:  52 loss: 0.15291 training acc: 0.95742 testing acc: 0.9504\n",
      "epoch: 5 iter:  53 loss: 0.17013 training acc: 0.95130 testing acc: 0.9432\n",
      "epoch: 5 iter:  54 loss: 0.16417 training acc: 0.95280 testing acc: 0.9464\n",
      "epoch: 5 iter:  55 loss: 0.15327 training acc: 0.95658 testing acc: 0.95\n",
      "epoch: 5 iter:  56 loss: 0.16582 training acc: 0.95300 testing acc: 0.9488\n",
      "epoch: 5 iter:  57 loss: 0.15150 training acc: 0.95716 testing acc: 0.949\n",
      "epoch: 5 iter:  58 loss: 0.15217 training acc: 0.95734 testing acc: 0.9494\n",
      "epoch: 5 iter:  59 loss: 0.14910 training acc: 0.95724 testing acc: 0.9496\n",
      "epoch: 5 iter:  60 loss: 0.15665 training acc: 0.95614 testing acc: 0.9518\n",
      "epoch: 5 iter:  61 loss: 0.15006 training acc: 0.95772 testing acc: 0.9534\n",
      "epoch: 5 iter:  62 loss: 0.15617 training acc: 0.95594 testing acc: 0.9502\n",
      "epoch: 5 iter:  63 loss: 0.15461 training acc: 0.95534 testing acc: 0.9498\n",
      "epoch: 5 iter:  64 loss: 0.19599 training acc: 0.93902 testing acc: 0.9304\n",
      "epoch: 5 iter:  65 loss: 0.20494 training acc: 0.93636 testing acc: 0.927\n",
      "epoch: 5 iter:  66 loss: 0.17216 training acc: 0.94890 testing acc: 0.9436\n",
      "epoch: 5 iter:  67 loss: 0.16085 training acc: 0.95278 testing acc: 0.9438\n",
      "epoch: 5 iter:  68 loss: 0.16972 training acc: 0.95000 testing acc: 0.9432\n",
      "epoch: 5 iter:  69 loss: 0.15983 training acc: 0.95368 testing acc: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 iter:  70 loss: 0.16087 training acc: 0.95310 testing acc: 0.949\n",
      "epoch: 5 iter:  71 loss: 0.15946 training acc: 0.95388 testing acc: 0.9484\n",
      "epoch: 5 iter:  72 loss: 0.17710 training acc: 0.94690 testing acc: 0.9426\n",
      "epoch: 5 iter:  73 loss: 0.16404 training acc: 0.95186 testing acc: 0.9468\n",
      "epoch: 5 iter:  74 loss: 0.16635 training acc: 0.95038 testing acc: 0.9454\n",
      "epoch: 5 iter:  75 loss: 0.16721 training acc: 0.95192 testing acc: 0.9452\n",
      "epoch: 5 iter:  76 loss: 0.16512 training acc: 0.95202 testing acc: 0.943\n",
      "epoch: 5 iter:  77 loss: 0.18207 training acc: 0.94476 testing acc: 0.9356\n",
      "epoch: 5 iter:  78 loss: 0.18622 training acc: 0.94312 testing acc: 0.9322\n",
      "epoch: 5 iter:  79 loss: 0.16279 training acc: 0.95254 testing acc: 0.9462\n",
      "epoch: 5 iter:  80 loss: 0.16461 training acc: 0.95296 testing acc: 0.9438\n",
      "epoch: 5 iter:  81 loss: 0.16195 training acc: 0.95376 testing acc: 0.947\n",
      "epoch: 5 iter:  82 loss: 0.16444 training acc: 0.95330 testing acc: 0.9488\n",
      "epoch: 5 iter:  83 loss: 0.16358 training acc: 0.95326 testing acc: 0.9474\n",
      "epoch: 5 iter:  84 loss: 0.15410 training acc: 0.95562 testing acc: 0.9504\n",
      "epoch: 5 iter:  85 loss: 0.16368 training acc: 0.95208 testing acc: 0.9472\n",
      "epoch: 5 iter:  86 loss: 0.15055 training acc: 0.95654 testing acc: 0.9496\n",
      "epoch: 5 iter:  87 loss: 0.15222 training acc: 0.95616 testing acc: 0.9486\n",
      "epoch: 5 iter:  88 loss: 0.15179 training acc: 0.95496 testing acc: 0.95\n",
      "epoch: 5 iter:  89 loss: 0.16620 training acc: 0.95132 testing acc: 0.9468\n",
      "epoch: 5 iter:  90 loss: 0.15697 training acc: 0.95408 testing acc: 0.9454\n",
      "epoch: 5 iter:  91 loss: 0.15615 training acc: 0.95490 testing acc: 0.9458\n",
      "epoch: 5 iter:  92 loss: 0.15431 training acc: 0.95440 testing acc: 0.947\n",
      "epoch: 5 iter:  93 loss: 0.18078 training acc: 0.94536 testing acc: 0.9414\n",
      "epoch: 5 iter:  94 loss: 0.16059 training acc: 0.95310 testing acc: 0.9456\n",
      "epoch: 5 iter:  95 loss: 0.15965 training acc: 0.95336 testing acc: 0.947\n",
      "epoch: 5 iter:  96 loss: 0.15508 training acc: 0.95476 testing acc: 0.9472\n",
      "epoch: 5 iter:  97 loss: 0.15354 training acc: 0.95532 testing acc: 0.9488\n",
      "epoch: 5 iter:  98 loss: 0.15118 training acc: 0.95556 testing acc: 0.9464\n",
      "epoch: 5 iter:  99 loss: 0.15487 training acc: 0.95430 testing acc: 0.9474\n",
      "epoch: 5 iter: 100 loss: 0.15679 training acc: 0.95400 testing acc: 0.95\n",
      "epoch: 5 iter: 101 loss: 0.15207 training acc: 0.95556 testing acc: 0.9498\n",
      "epoch: 5 iter: 102 loss: 0.15246 training acc: 0.95536 testing acc: 0.948\n",
      "epoch: 5 iter: 103 loss: 0.15782 training acc: 0.95262 testing acc: 0.9448\n",
      "epoch: 5 iter: 104 loss: 0.14909 training acc: 0.95646 testing acc: 0.95\n",
      "epoch: 5 iter: 105 loss: 0.16158 training acc: 0.95268 testing acc: 0.946\n",
      "epoch: 5 iter: 106 loss: 0.15093 training acc: 0.95602 testing acc: 0.9492\n",
      "epoch: 5 iter: 107 loss: 0.15280 training acc: 0.95380 testing acc: 0.9472\n",
      "epoch: 5 iter: 108 loss: 0.15176 training acc: 0.95496 testing acc: 0.947\n",
      "epoch: 5 iter: 109 loss: 0.15562 training acc: 0.95296 testing acc: 0.9456\n",
      "epoch: 5 iter: 110 loss: 0.17509 training acc: 0.94524 testing acc: 0.9358\n",
      "epoch: 5 iter: 111 loss: 0.17256 training acc: 0.94704 testing acc: 0.9376\n",
      "epoch: 5 iter: 112 loss: 0.16338 training acc: 0.94972 testing acc: 0.9422\n",
      "epoch: 5 iter: 113 loss: 0.15808 training acc: 0.95322 testing acc: 0.944\n",
      "epoch: 5 iter: 114 loss: 0.15141 training acc: 0.95572 testing acc: 0.948\n",
      "epoch: 5 iter: 115 loss: 0.15582 training acc: 0.95532 testing acc: 0.9458\n",
      "epoch: 5 iter: 116 loss: 0.16964 training acc: 0.94914 testing acc: 0.9444\n",
      "epoch: 5 iter: 117 loss: 0.15760 training acc: 0.95384 testing acc: 0.947\n",
      "epoch: 5 iter: 118 loss: 0.15597 training acc: 0.95450 testing acc: 0.948\n",
      "epoch: 5 iter: 119 loss: 0.15317 training acc: 0.95544 testing acc: 0.9466\n",
      "epoch: 5 iter: 120 loss: 0.15350 training acc: 0.95556 testing acc: 0.9478\n",
      "epoch: 5 iter: 121 loss: 0.15897 training acc: 0.95374 testing acc: 0.9472\n",
      "epoch: 5 iter: 122 loss: 0.15200 training acc: 0.95690 testing acc: 0.9498\n",
      "epoch: 5 iter: 123 loss: 0.15040 training acc: 0.95728 testing acc: 0.949\n",
      "epoch: 5 iter: 124 loss: 0.15663 training acc: 0.95522 testing acc: 0.9454\n",
      "epoch: 5 iter: 125 loss: 0.16303 training acc: 0.95266 testing acc: 0.9418\n",
      "epoch: 5 iter: 126 loss: 0.16882 training acc: 0.94976 testing acc: 0.9422\n",
      "epoch: 5 iter: 127 loss: 0.15098 training acc: 0.95644 testing acc: 0.9486\n",
      "epoch: 5 iter: 128 loss: 0.15051 training acc: 0.95710 testing acc: 0.9506\n",
      "epoch: 5 iter: 129 loss: 0.14968 training acc: 0.95690 testing acc: 0.9514\n",
      "epoch: 5 iter: 130 loss: 0.15305 training acc: 0.95638 testing acc: 0.95\n",
      "epoch: 5 iter: 131 loss: 0.14690 training acc: 0.95848 testing acc: 0.9542\n",
      "epoch: 5 iter: 132 loss: 0.17671 training acc: 0.94792 testing acc: 0.9408\n",
      "epoch: 5 iter: 133 loss: 0.14840 training acc: 0.95794 testing acc: 0.9532\n",
      "epoch: 5 iter: 134 loss: 0.15371 training acc: 0.95586 testing acc: 0.9518\n",
      "epoch: 5 iter: 135 loss: 0.15225 training acc: 0.95644 testing acc: 0.9512\n",
      "epoch: 5 iter: 136 loss: 0.14714 training acc: 0.95864 testing acc: 0.9534\n",
      "epoch: 5 iter: 137 loss: 0.14508 training acc: 0.95858 testing acc: 0.9512\n",
      "epoch: 5 iter: 138 loss: 0.15866 training acc: 0.95300 testing acc: 0.9482\n",
      "epoch: 5 iter: 139 loss: 0.15277 training acc: 0.95580 testing acc: 0.9524\n",
      "epoch: 5 iter: 140 loss: 0.14729 training acc: 0.95816 testing acc: 0.9544\n",
      "epoch: 5 iter: 141 loss: 0.20859 training acc: 0.93950 testing acc: 0.9378\n",
      "epoch: 5 iter: 142 loss: 0.17067 training acc: 0.95158 testing acc: 0.9464\n",
      "epoch: 5 iter: 143 loss: 0.17710 training acc: 0.94832 testing acc: 0.9456\n",
      "epoch: 5 iter: 144 loss: 0.19526 training acc: 0.94212 testing acc: 0.9358\n",
      "epoch: 5 iter: 145 loss: 0.17027 training acc: 0.94974 testing acc: 0.944\n",
      "epoch: 5 iter: 146 loss: 0.16238 training acc: 0.95200 testing acc: 0.944\n",
      "epoch: 5 iter: 147 loss: 0.15400 training acc: 0.95498 testing acc: 0.9462\n",
      "epoch: 5 iter: 148 loss: 0.14789 training acc: 0.95774 testing acc: 0.952\n",
      "epoch: 5 iter: 149 loss: 0.15631 training acc: 0.95432 testing acc: 0.9488\n",
      "epoch: 5 iter: 150 loss: 0.14809 training acc: 0.95724 testing acc: 0.9508\n",
      "epoch: 5 iter: 151 loss: 0.15593 training acc: 0.95484 testing acc: 0.9518\n",
      "epoch: 5 iter: 152 loss: 0.15924 training acc: 0.95392 testing acc: 0.9496\n",
      "epoch: 5 iter: 153 loss: 0.15739 training acc: 0.95474 testing acc: 0.9496\n",
      "epoch: 5 iter: 154 loss: 0.14963 training acc: 0.95672 testing acc: 0.9506\n",
      "epoch: 5 iter: 155 loss: 0.14999 training acc: 0.95638 testing acc: 0.9516\n",
      "epoch: 5 iter: 156 loss: 0.15714 training acc: 0.95526 testing acc: 0.9486\n",
      "epoch: 5 iter: 157 loss: 0.16087 training acc: 0.95424 testing acc: 0.9498\n",
      "epoch: 5 iter: 158 loss: 0.15797 training acc: 0.95562 testing acc: 0.95\n",
      "epoch: 5 iter: 159 loss: 0.15171 training acc: 0.95694 testing acc: 0.9514\n",
      "epoch: 5 iter: 160 loss: 0.15275 training acc: 0.95710 testing acc: 0.9488\n",
      "epoch: 5 iter: 161 loss: 0.16592 training acc: 0.95316 testing acc: 0.9466\n",
      "epoch: 5 iter: 162 loss: 0.16069 training acc: 0.95488 testing acc: 0.948\n",
      "epoch: 5 iter: 163 loss: 0.15291 training acc: 0.95714 testing acc: 0.9482\n",
      "epoch: 5 iter: 164 loss: 0.16101 training acc: 0.95438 testing acc: 0.9466\n",
      "epoch: 5 iter: 165 loss: 0.16044 training acc: 0.95190 testing acc: 0.9472\n",
      "epoch: 5 iter: 166 loss: 0.15009 training acc: 0.95650 testing acc: 0.9522\n",
      "epoch: 5 iter: 167 loss: 0.14630 training acc: 0.95686 testing acc: 0.9528\n",
      "epoch: 5 iter: 168 loss: 0.15370 training acc: 0.95496 testing acc: 0.9492\n",
      "epoch: 5 iter: 169 loss: 0.15015 training acc: 0.95606 testing acc: 0.951\n",
      "epoch: 5 iter: 170 loss: 0.15858 training acc: 0.95454 testing acc: 0.9534\n",
      "epoch: 5 iter: 171 loss: 0.15623 training acc: 0.95522 testing acc: 0.953\n",
      "epoch: 5 iter: 172 loss: 0.15545 training acc: 0.95524 testing acc: 0.953\n",
      "epoch: 5 iter: 173 loss: 0.15735 training acc: 0.95432 testing acc: 0.9538\n",
      "epoch: 5 iter: 174 loss: 0.16191 training acc: 0.95380 testing acc: 0.9502\n",
      "epoch: 5 iter: 175 loss: 0.15130 training acc: 0.95718 testing acc: 0.9546\n",
      "epoch: 5 iter: 176 loss: 0.15325 training acc: 0.95626 testing acc: 0.952\n",
      "epoch: 5 iter: 177 loss: 0.14596 training acc: 0.95776 testing acc: 0.9534\n",
      "epoch: 5 iter: 178 loss: 0.14464 training acc: 0.95822 testing acc: 0.9528\n",
      "epoch: 5 iter: 179 loss: 0.14924 training acc: 0.95708 testing acc: 0.9516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 iter: 180 loss: 0.15604 training acc: 0.95440 testing acc: 0.947\n",
      "epoch: 5 iter: 181 loss: 0.15173 training acc: 0.95600 testing acc: 0.9474\n",
      "epoch: 5 iter: 182 loss: 0.15438 training acc: 0.95474 testing acc: 0.9464\n",
      "epoch: 5 iter: 183 loss: 0.15337 training acc: 0.95562 testing acc: 0.9482\n",
      "epoch: 5 iter: 184 loss: 0.14833 training acc: 0.95710 testing acc: 0.9514\n",
      "epoch: 5 iter: 185 loss: 0.14440 training acc: 0.95856 testing acc: 0.9522\n",
      "epoch: 5 iter: 186 loss: 0.14750 training acc: 0.95628 testing acc: 0.9506\n",
      "epoch: 5 iter: 187 loss: 0.14680 training acc: 0.95790 testing acc: 0.9512\n",
      "epoch: 5 iter: 188 loss: 0.15287 training acc: 0.95534 testing acc: 0.9504\n",
      "epoch: 5 iter: 189 loss: 0.15943 training acc: 0.95258 testing acc: 0.9484\n",
      "epoch: 5 iter: 190 loss: 0.17857 training acc: 0.94642 testing acc: 0.9438\n",
      "epoch: 5 iter: 191 loss: 0.16418 training acc: 0.95116 testing acc: 0.948\n",
      "epoch: 5 iter: 192 loss: 0.14860 training acc: 0.95652 testing acc: 0.95\n",
      "epoch: 5 iter: 193 loss: 0.15310 training acc: 0.95492 testing acc: 0.947\n",
      "epoch: 5 iter: 194 loss: 0.16238 training acc: 0.95078 testing acc: 0.9432\n",
      "epoch: 5 iter: 195 loss: 0.15100 training acc: 0.95698 testing acc: 0.9494\n",
      "epoch: 5 iter: 196 loss: 0.14670 training acc: 0.95796 testing acc: 0.9498\n",
      "epoch: 5 iter: 197 loss: 0.14553 training acc: 0.95850 testing acc: 0.9524\n",
      "epoch: 5 iter: 198 loss: 0.14879 training acc: 0.95692 testing acc: 0.9492\n",
      "epoch: 5 iter: 199 loss: 0.14903 training acc: 0.95680 testing acc: 0.9498\n",
      "epoch: 5 iter: 200 loss: 0.14727 training acc: 0.95718 testing acc: 0.9488\n",
      "epoch: 5 iter: 201 loss: 0.15791 training acc: 0.95248 testing acc: 0.9468\n",
      "epoch: 5 iter: 202 loss: 0.15910 training acc: 0.95242 testing acc: 0.9464\n",
      "epoch: 5 iter: 203 loss: 0.15615 training acc: 0.95300 testing acc: 0.947\n",
      "epoch: 5 iter: 204 loss: 0.16103 training acc: 0.95228 testing acc: 0.9476\n",
      "epoch: 5 iter: 205 loss: 0.15535 training acc: 0.95380 testing acc: 0.9446\n",
      "epoch: 5 iter: 206 loss: 0.14617 training acc: 0.95766 testing acc: 0.9504\n",
      "epoch: 5 iter: 207 loss: 0.14393 training acc: 0.95940 testing acc: 0.9524\n",
      "epoch: 5 iter: 208 loss: 0.14352 training acc: 0.95976 testing acc: 0.953\n",
      "epoch: 5 iter: 209 loss: 0.14951 training acc: 0.95708 testing acc: 0.9518\n",
      "epoch: 5 iter: 210 loss: 0.14468 training acc: 0.95808 testing acc: 0.9524\n",
      "epoch: 5 iter: 211 loss: 0.15645 training acc: 0.95344 testing acc: 0.9446\n",
      "epoch: 5 iter: 212 loss: 0.15648 training acc: 0.95470 testing acc: 0.9494\n",
      "epoch: 5 iter: 213 loss: 0.15341 training acc: 0.95590 testing acc: 0.9506\n",
      "epoch: 5 iter: 214 loss: 0.15287 training acc: 0.95554 testing acc: 0.9476\n",
      "epoch: 5 iter: 215 loss: 0.14863 training acc: 0.95738 testing acc: 0.9492\n",
      "epoch: 5 iter: 216 loss: 0.15355 training acc: 0.95582 testing acc: 0.948\n",
      "epoch: 5 iter: 217 loss: 0.15615 training acc: 0.95480 testing acc: 0.9462\n",
      "epoch: 5 iter: 218 loss: 0.17833 training acc: 0.94750 testing acc: 0.9418\n",
      "epoch: 5 iter: 219 loss: 0.18114 training acc: 0.94738 testing acc: 0.9426\n",
      "epoch: 5 iter: 220 loss: 0.16089 training acc: 0.95272 testing acc: 0.9452\n",
      "epoch: 5 iter: 221 loss: 0.15799 training acc: 0.95310 testing acc: 0.9444\n",
      "epoch: 5 iter: 222 loss: 0.15299 training acc: 0.95660 testing acc: 0.9464\n",
      "epoch: 5 iter: 223 loss: 0.15331 training acc: 0.95644 testing acc: 0.951\n",
      "epoch: 5 iter: 224 loss: 0.15930 training acc: 0.95498 testing acc: 0.9508\n",
      "epoch: 5 iter: 225 loss: 0.15822 training acc: 0.95512 testing acc: 0.9522\n",
      "epoch: 5 iter: 226 loss: 0.14972 training acc: 0.95766 testing acc: 0.9532\n",
      "epoch: 5 iter: 227 loss: 0.14555 training acc: 0.95924 testing acc: 0.9512\n",
      "epoch: 5 iter: 228 loss: 0.14573 training acc: 0.95854 testing acc: 0.9528\n",
      "epoch: 5 iter: 229 loss: 0.14418 training acc: 0.95914 testing acc: 0.9534\n",
      "epoch: 5 iter: 230 loss: 0.14460 training acc: 0.95874 testing acc: 0.9526\n",
      "epoch: 5 iter: 231 loss: 0.14468 training acc: 0.95846 testing acc: 0.9536\n",
      "epoch: 5 iter: 232 loss: 0.14863 training acc: 0.95802 testing acc: 0.9524\n",
      "epoch: 5 iter: 233 loss: 0.15692 training acc: 0.95492 testing acc: 0.9518\n",
      "epoch: 5 iter: 234 loss: 0.17493 training acc: 0.94984 testing acc: 0.9474\n",
      "epoch: 5 iter: 235 loss: 0.16595 training acc: 0.95332 testing acc: 0.9466\n",
      "epoch: 5 iter: 236 loss: 0.14446 training acc: 0.95926 testing acc: 0.9518\n",
      "epoch: 5 iter: 237 loss: 0.14489 training acc: 0.95934 testing acc: 0.9506\n",
      "epoch: 5 iter: 238 loss: 0.14397 training acc: 0.96020 testing acc: 0.9504\n",
      "epoch: 5 iter: 239 loss: 0.14575 training acc: 0.95914 testing acc: 0.951\n",
      "epoch: 5 iter: 240 loss: 0.14734 training acc: 0.95904 testing acc: 0.9504\n",
      "epoch: 5 iter: 241 loss: 0.14880 training acc: 0.95828 testing acc: 0.951\n",
      "epoch: 5 iter: 242 loss: 0.17263 training acc: 0.94868 testing acc: 0.9378\n",
      "epoch: 5 iter: 243 loss: 0.16006 training acc: 0.95308 testing acc: 0.943\n",
      "epoch: 5 iter: 244 loss: 0.15569 training acc: 0.95538 testing acc: 0.9494\n",
      "epoch: 5 iter: 245 loss: 0.15906 training acc: 0.95298 testing acc: 0.9488\n",
      "epoch: 5 iter: 246 loss: 0.15677 training acc: 0.95368 testing acc: 0.9486\n",
      "epoch: 5 iter: 247 loss: 0.18204 training acc: 0.94500 testing acc: 0.9426\n",
      "epoch: 5 iter: 248 loss: 0.17079 training acc: 0.95030 testing acc: 0.945\n",
      "epoch: 5 iter: 249 loss: 0.14946 training acc: 0.95784 testing acc: 0.9528\n",
      "epoch: 5 iter: 250 loss: 0.14789 training acc: 0.95818 testing acc: 0.9518\n",
      "epoch: 5 iter: 251 loss: 0.14982 training acc: 0.95738 testing acc: 0.9524\n",
      "epoch: 5 iter: 252 loss: 0.17826 training acc: 0.94682 testing acc: 0.941\n",
      "epoch: 5 iter: 253 loss: 0.15908 training acc: 0.95380 testing acc: 0.9474\n",
      "epoch: 5 iter: 254 loss: 0.15137 training acc: 0.95670 testing acc: 0.952\n",
      "epoch: 5 iter: 255 loss: 0.15834 training acc: 0.95374 testing acc: 0.9502\n",
      "epoch: 5 iter: 256 loss: 0.14972 training acc: 0.95680 testing acc: 0.9542\n",
      "epoch: 5 iter: 257 loss: 0.15530 training acc: 0.95490 testing acc: 0.953\n",
      "epoch: 5 iter: 258 loss: 0.16148 training acc: 0.95368 testing acc: 0.9502\n",
      "epoch: 5 iter: 259 loss: 0.15257 training acc: 0.95630 testing acc: 0.949\n",
      "epoch: 5 iter: 260 loss: 0.14039 training acc: 0.96076 testing acc: 0.9558\n",
      "epoch: 5 iter: 261 loss: 0.15749 training acc: 0.95506 testing acc: 0.9492\n",
      "epoch: 5 iter: 262 loss: 0.14587 training acc: 0.95894 testing acc: 0.9558\n",
      "epoch: 5 iter: 263 loss: 0.15474 training acc: 0.95620 testing acc: 0.9512\n",
      "epoch: 5 iter: 264 loss: 0.14915 training acc: 0.95554 testing acc: 0.9512\n",
      "epoch: 5 iter: 265 loss: 0.14513 training acc: 0.95710 testing acc: 0.9542\n",
      "epoch: 5 iter: 266 loss: 0.14570 training acc: 0.95720 testing acc: 0.9526\n",
      "epoch: 5 iter: 267 loss: 0.16017 training acc: 0.95318 testing acc: 0.9478\n",
      "epoch: 5 iter: 268 loss: 0.14951 training acc: 0.95734 testing acc: 0.9506\n",
      "epoch: 5 iter: 269 loss: 0.14259 training acc: 0.95910 testing acc: 0.954\n",
      "epoch: 5 iter: 270 loss: 0.14329 training acc: 0.95820 testing acc: 0.952\n",
      "epoch: 5 iter: 271 loss: 0.13725 training acc: 0.96038 testing acc: 0.9548\n",
      "epoch: 5 iter: 272 loss: 0.15946 training acc: 0.95234 testing acc: 0.9454\n",
      "epoch: 5 iter: 273 loss: 0.14352 training acc: 0.95798 testing acc: 0.952\n",
      "epoch: 5 iter: 274 loss: 0.15023 training acc: 0.95580 testing acc: 0.948\n",
      "epoch: 5 iter: 275 loss: 0.14660 training acc: 0.95718 testing acc: 0.9506\n",
      "epoch: 5 iter: 276 loss: 0.16806 training acc: 0.95142 testing acc: 0.9458\n",
      "epoch: 5 iter: 277 loss: 0.15651 training acc: 0.95420 testing acc: 0.9494\n",
      "epoch: 5 iter: 278 loss: 0.16223 training acc: 0.95234 testing acc: 0.9466\n",
      "epoch: 5 iter: 279 loss: 0.15963 training acc: 0.95302 testing acc: 0.9478\n",
      "epoch: 5 iter: 280 loss: 0.15225 training acc: 0.95580 testing acc: 0.953\n",
      "epoch: 5 iter: 281 loss: 0.14827 training acc: 0.95654 testing acc: 0.9524\n",
      "epoch: 5 iter: 282 loss: 0.14955 training acc: 0.95574 testing acc: 0.9524\n",
      "epoch: 5 iter: 283 loss: 0.15192 training acc: 0.95606 testing acc: 0.9506\n",
      "epoch: 5 iter: 284 loss: 0.13989 training acc: 0.95910 testing acc: 0.9544\n",
      "epoch: 5 iter: 285 loss: 0.13787 training acc: 0.96056 testing acc: 0.9554\n",
      "epoch: 5 iter: 286 loss: 0.15067 training acc: 0.95704 testing acc: 0.952\n",
      "epoch: 5 iter: 287 loss: 0.15073 training acc: 0.95664 testing acc: 0.9502\n",
      "epoch: 5 iter: 288 loss: 0.14762 training acc: 0.95782 testing acc: 0.955\n",
      "epoch: 5 iter: 289 loss: 0.15525 training acc: 0.95536 testing acc: 0.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 iter: 290 loss: 0.14969 training acc: 0.95820 testing acc: 0.9542\n",
      "epoch: 5 iter: 291 loss: 0.15740 training acc: 0.95368 testing acc: 0.947\n",
      "epoch: 5 iter: 292 loss: 0.18351 training acc: 0.94526 testing acc: 0.9372\n",
      "epoch: 5 iter: 293 loss: 0.15243 training acc: 0.95560 testing acc: 0.9496\n",
      "epoch: 5 iter: 294 loss: 0.15113 training acc: 0.95458 testing acc: 0.95\n",
      "epoch: 5 iter: 295 loss: 0.15417 training acc: 0.95412 testing acc: 0.9482\n",
      "epoch: 5 iter: 296 loss: 0.14553 training acc: 0.95708 testing acc: 0.9522\n",
      "epoch: 5 iter: 297 loss: 0.14859 training acc: 0.95626 testing acc: 0.9508\n",
      "epoch: 5 iter: 298 loss: 0.15007 training acc: 0.95598 testing acc: 0.9492\n",
      "epoch: 5 iter: 299 loss: 0.14649 training acc: 0.95774 testing acc: 0.9508\n",
      "epoch: 5 iter: 300 loss: 0.14136 training acc: 0.95868 testing acc: 0.9538\n",
      "epoch: 5 iter: 301 loss: 0.14312 training acc: 0.95788 testing acc: 0.9504\n",
      "epoch: 5 iter: 302 loss: 0.15245 training acc: 0.95514 testing acc: 0.9502\n",
      "epoch: 5 iter: 303 loss: 0.16662 training acc: 0.95136 testing acc: 0.9466\n",
      "epoch: 5 iter: 304 loss: 0.15774 training acc: 0.95282 testing acc: 0.9492\n",
      "epoch: 5 iter: 305 loss: 0.15436 training acc: 0.95452 testing acc: 0.949\n",
      "epoch: 5 iter: 306 loss: 0.15483 training acc: 0.95454 testing acc: 0.949\n",
      "epoch: 5 iter: 307 loss: 0.14971 training acc: 0.95490 testing acc: 0.9516\n",
      "epoch: 5 iter: 308 loss: 0.16098 training acc: 0.95188 testing acc: 0.9496\n",
      "epoch: 5 iter: 309 loss: 0.16172 training acc: 0.95148 testing acc: 0.9484\n",
      "epoch: 5 iter: 310 loss: 0.15188 training acc: 0.95558 testing acc: 0.9492\n",
      "epoch: 5 iter: 311 loss: 0.15194 training acc: 0.95546 testing acc: 0.952\n",
      "epoch: 5 iter: 312 loss: 0.15481 training acc: 0.95510 testing acc: 0.9476\n",
      "epoch: 5 iter: 313 loss: 0.15053 training acc: 0.95614 testing acc: 0.9494\n",
      "epoch: 5 iter: 314 loss: 0.14176 training acc: 0.95786 testing acc: 0.955\n",
      "epoch: 5 iter: 315 loss: 0.15722 training acc: 0.95370 testing acc: 0.9502\n",
      "epoch: 5 iter: 316 loss: 0.15954 training acc: 0.95298 testing acc: 0.9486\n",
      "epoch: 5 iter: 317 loss: 0.15728 training acc: 0.95290 testing acc: 0.9512\n",
      "epoch: 5 iter: 318 loss: 0.16136 training acc: 0.95246 testing acc: 0.9472\n",
      "epoch: 5 iter: 319 loss: 0.15399 training acc: 0.95492 testing acc: 0.9526\n",
      "epoch: 5 iter: 320 loss: 0.14557 training acc: 0.95804 testing acc: 0.9564\n",
      "epoch: 5 iter: 321 loss: 0.14602 training acc: 0.95828 testing acc: 0.954\n",
      "epoch: 5 iter: 322 loss: 0.14419 training acc: 0.95914 testing acc: 0.9562\n",
      "epoch: 5 iter: 323 loss: 0.14323 training acc: 0.95838 testing acc: 0.953\n",
      "epoch: 5 iter: 324 loss: 0.14645 training acc: 0.95726 testing acc: 0.9508\n",
      "epoch: 5 iter: 325 loss: 0.14790 training acc: 0.95628 testing acc: 0.9502\n",
      "epoch: 5 iter: 326 loss: 0.14712 training acc: 0.95740 testing acc: 0.9512\n",
      "epoch: 5 iter: 327 loss: 0.15049 training acc: 0.95618 testing acc: 0.9492\n",
      "epoch: 5 iter: 328 loss: 0.14706 training acc: 0.95778 testing acc: 0.9504\n",
      "epoch: 5 iter: 329 loss: 0.14159 training acc: 0.95956 testing acc: 0.953\n",
      "epoch: 5 iter: 330 loss: 0.15477 training acc: 0.95452 testing acc: 0.9514\n",
      "epoch: 5 iter: 331 loss: 0.15295 training acc: 0.95538 testing acc: 0.9524\n",
      "epoch: 5 iter: 332 loss: 0.15941 training acc: 0.95362 testing acc: 0.9508\n",
      "epoch: 5 iter: 333 loss: 0.15481 training acc: 0.95544 testing acc: 0.9488\n",
      "epoch: 5 iter: 334 loss: 0.16436 training acc: 0.95296 testing acc: 0.9516\n",
      "epoch: 5 iter: 335 loss: 0.14873 training acc: 0.95766 testing acc: 0.9534\n",
      "epoch: 5 iter: 336 loss: 0.14804 training acc: 0.95780 testing acc: 0.9532\n",
      "epoch: 5 iter: 337 loss: 0.15438 training acc: 0.95484 testing acc: 0.948\n",
      "epoch: 5 iter: 338 loss: 0.15027 training acc: 0.95618 testing acc: 0.9476\n",
      "epoch: 5 iter: 339 loss: 0.14939 training acc: 0.95590 testing acc: 0.9494\n",
      "epoch: 5 iter: 340 loss: 0.15585 training acc: 0.95308 testing acc: 0.9456\n",
      "epoch: 5 iter: 341 loss: 0.14907 training acc: 0.95580 testing acc: 0.9462\n",
      "epoch: 5 iter: 342 loss: 0.14987 training acc: 0.95496 testing acc: 0.9474\n",
      "epoch: 5 iter: 343 loss: 0.14449 training acc: 0.95756 testing acc: 0.948\n",
      "epoch: 5 iter: 344 loss: 0.13975 training acc: 0.95870 testing acc: 0.9514\n",
      "epoch: 5 iter: 345 loss: 0.14023 training acc: 0.95862 testing acc: 0.9498\n",
      "epoch: 5 iter: 346 loss: 0.14771 training acc: 0.95660 testing acc: 0.95\n",
      "epoch: 5 iter: 347 loss: 0.13823 training acc: 0.95974 testing acc: 0.9534\n",
      "epoch: 5 iter: 348 loss: 0.14023 training acc: 0.95880 testing acc: 0.9514\n",
      "epoch: 5 iter: 349 loss: 0.14529 training acc: 0.95740 testing acc: 0.9486\n",
      "epoch: 5 iter: 350 loss: 0.16149 training acc: 0.95240 testing acc: 0.9434\n",
      "epoch: 5 iter: 351 loss: 0.14643 training acc: 0.95818 testing acc: 0.9516\n",
      "epoch: 5 iter: 352 loss: 0.14160 training acc: 0.95950 testing acc: 0.9524\n",
      "epoch: 5 iter: 353 loss: 0.14087 training acc: 0.95980 testing acc: 0.954\n",
      "epoch: 5 iter: 354 loss: 0.14890 training acc: 0.95658 testing acc: 0.949\n",
      "epoch: 5 iter: 355 loss: 0.15397 training acc: 0.95436 testing acc: 0.946\n",
      "epoch: 5 iter: 356 loss: 0.14922 training acc: 0.95638 testing acc: 0.9484\n",
      "epoch: 5 iter: 357 loss: 0.14624 training acc: 0.95782 testing acc: 0.9496\n",
      "epoch: 5 iter: 358 loss: 0.14677 training acc: 0.95734 testing acc: 0.9474\n",
      "epoch: 5 iter: 359 loss: 0.15127 training acc: 0.95684 testing acc: 0.949\n",
      "epoch: 5 iter: 360 loss: 0.14987 training acc: 0.95552 testing acc: 0.9452\n",
      "epoch: 5 iter: 361 loss: 0.14545 training acc: 0.95786 testing acc: 0.9476\n",
      "epoch: 5 iter: 362 loss: 0.15090 training acc: 0.95552 testing acc: 0.9464\n",
      "epoch: 5 iter: 363 loss: 0.14536 training acc: 0.95700 testing acc: 0.9478\n",
      "epoch: 5 iter: 364 loss: 0.14722 training acc: 0.95638 testing acc: 0.9472\n",
      "epoch: 5 iter: 365 loss: 0.14720 training acc: 0.95764 testing acc: 0.9478\n",
      "epoch: 5 iter: 366 loss: 0.14130 training acc: 0.95948 testing acc: 0.95\n",
      "epoch: 5 iter: 367 loss: 0.13834 training acc: 0.96048 testing acc: 0.9532\n",
      "epoch: 5 iter: 368 loss: 0.14605 training acc: 0.95772 testing acc: 0.9504\n",
      "epoch: 5 iter: 369 loss: 0.15191 training acc: 0.95480 testing acc: 0.948\n",
      "epoch: 5 iter: 370 loss: 0.14855 training acc: 0.95622 testing acc: 0.9502\n",
      "epoch: 5 iter: 371 loss: 0.14905 training acc: 0.95614 testing acc: 0.9502\n",
      "epoch: 5 iter: 372 loss: 0.14758 training acc: 0.95668 testing acc: 0.9508\n",
      "epoch: 5 iter: 373 loss: 0.14594 training acc: 0.95744 testing acc: 0.9504\n",
      "epoch: 5 iter: 374 loss: 0.15468 training acc: 0.95494 testing acc: 0.9506\n",
      "epoch: 5 iter: 375 loss: 0.14693 training acc: 0.95760 testing acc: 0.9532\n",
      "epoch: 5 iter: 376 loss: 0.15141 training acc: 0.95556 testing acc: 0.9484\n",
      "epoch: 5 iter: 377 loss: 0.14992 training acc: 0.95670 testing acc: 0.9502\n",
      "epoch: 5 iter: 378 loss: 0.14996 training acc: 0.95678 testing acc: 0.9514\n",
      "epoch: 5 iter: 379 loss: 0.14047 training acc: 0.95910 testing acc: 0.954\n",
      "epoch: 5 iter: 380 loss: 0.15519 training acc: 0.95536 testing acc: 0.9524\n",
      "epoch: 5 iter: 381 loss: 0.14625 training acc: 0.95774 testing acc: 0.9498\n",
      "epoch: 5 iter: 382 loss: 0.14300 training acc: 0.95874 testing acc: 0.9518\n",
      "epoch: 5 iter: 383 loss: 0.16090 training acc: 0.95112 testing acc: 0.9456\n",
      "epoch: 5 iter: 384 loss: 0.18413 training acc: 0.94292 testing acc: 0.9368\n",
      "epoch: 5 iter: 385 loss: 0.15142 training acc: 0.95544 testing acc: 0.9514\n",
      "epoch: 5 iter: 386 loss: 0.15164 training acc: 0.95508 testing acc: 0.9492\n",
      "epoch: 5 iter: 387 loss: 0.14363 training acc: 0.95798 testing acc: 0.9524\n",
      "epoch: 5 iter: 388 loss: 0.14129 training acc: 0.95878 testing acc: 0.9546\n",
      "epoch: 5 iter: 389 loss: 0.14942 training acc: 0.95692 testing acc: 0.9546\n",
      "epoch: 5 iter: 390 loss: 0.14299 training acc: 0.95856 testing acc: 0.9566\n",
      "epoch: 5 iter: 391 loss: 0.14311 training acc: 0.95814 testing acc: 0.9556\n",
      "epoch: 5 iter: 392 loss: 0.15643 training acc: 0.95348 testing acc: 0.95\n",
      "epoch: 5 iter: 393 loss: 0.14302 training acc: 0.95828 testing acc: 0.9554\n",
      "epoch: 5 iter: 394 loss: 0.14446 training acc: 0.95828 testing acc: 0.9542\n",
      "epoch: 5 iter: 395 loss: 0.14595 training acc: 0.95758 testing acc: 0.9538\n",
      "epoch: 5 iter: 396 loss: 0.14653 training acc: 0.95726 testing acc: 0.9512\n",
      "epoch: 5 iter: 397 loss: 0.14107 training acc: 0.95876 testing acc: 0.954\n",
      "epoch: 5 iter: 398 loss: 0.14300 training acc: 0.95744 testing acc: 0.953\n",
      "epoch: 5 iter: 399 loss: 0.13870 training acc: 0.95958 testing acc: 0.9546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 iter: 400 loss: 0.16710 training acc: 0.94952 testing acc: 0.9452\n",
      "epoch: 5 iter: 401 loss: 0.14748 training acc: 0.95624 testing acc: 0.9522\n",
      "epoch: 5 iter: 402 loss: 0.13788 training acc: 0.95952 testing acc: 0.9534\n",
      "epoch: 5 iter: 403 loss: 0.13837 training acc: 0.95920 testing acc: 0.9544\n",
      "epoch: 5 iter: 404 loss: 0.14760 training acc: 0.95638 testing acc: 0.9502\n",
      "epoch: 5 iter: 405 loss: 0.14202 training acc: 0.95856 testing acc: 0.9536\n",
      "epoch: 5 iter: 406 loss: 0.14226 training acc: 0.95766 testing acc: 0.9542\n",
      "epoch: 5 iter: 407 loss: 0.14137 training acc: 0.95868 testing acc: 0.9548\n",
      "epoch: 5 iter: 408 loss: 0.13773 training acc: 0.95958 testing acc: 0.9568\n",
      "epoch: 5 iter: 409 loss: 0.13711 training acc: 0.95998 testing acc: 0.9576\n",
      "epoch: 5 iter: 410 loss: 0.14408 training acc: 0.95792 testing acc: 0.9552\n",
      "epoch: 5 iter: 411 loss: 0.13984 training acc: 0.95924 testing acc: 0.956\n",
      "epoch: 5 iter: 412 loss: 0.14531 training acc: 0.95712 testing acc: 0.9532\n",
      "epoch: 5 iter: 413 loss: 0.14154 training acc: 0.95832 testing acc: 0.957\n",
      "epoch: 5 iter: 414 loss: 0.15250 training acc: 0.95414 testing acc: 0.9482\n",
      "epoch: 5 iter: 415 loss: 0.14600 training acc: 0.95612 testing acc: 0.952\n",
      "epoch: 5 iter: 416 loss: 0.13849 training acc: 0.95912 testing acc: 0.9564\n",
      "epoch: 5 iter: 417 loss: 0.13992 training acc: 0.95916 testing acc: 0.9544\n",
      "epoch: 5 iter: 418 loss: 0.14003 training acc: 0.95902 testing acc: 0.9556\n",
      "epoch: 5 iter: 419 loss: 0.14176 training acc: 0.95812 testing acc: 0.9568\n",
      "epoch: 5 iter: 420 loss: 0.13703 training acc: 0.96016 testing acc: 0.9578\n",
      "epoch: 5 iter: 421 loss: 0.15666 training acc: 0.95422 testing acc: 0.951\n",
      "epoch: 5 iter: 422 loss: 0.13747 training acc: 0.95928 testing acc: 0.955\n",
      "epoch: 5 iter: 423 loss: 0.13527 training acc: 0.96076 testing acc: 0.958\n",
      "epoch: 5 iter: 424 loss: 0.13525 training acc: 0.96042 testing acc: 0.9566\n",
      "epoch: 5 iter: 425 loss: 0.13333 training acc: 0.96082 testing acc: 0.957\n",
      "epoch: 5 iter: 426 loss: 0.13209 training acc: 0.96102 testing acc: 0.9568\n",
      "epoch: 5 iter: 427 loss: 0.13440 training acc: 0.96046 testing acc: 0.957\n",
      "epoch: 5 iter: 428 loss: 0.15799 training acc: 0.95404 testing acc: 0.9504\n",
      "epoch: 5 iter: 429 loss: 0.14285 training acc: 0.95826 testing acc: 0.9542\n",
      "epoch: 5 iter: 430 loss: 0.13794 training acc: 0.95956 testing acc: 0.957\n",
      "epoch: 5 iter: 431 loss: 0.14070 training acc: 0.95970 testing acc: 0.9552\n",
      "epoch: 5 iter: 432 loss: 0.15686 training acc: 0.95464 testing acc: 0.9498\n",
      "epoch: 5 iter: 433 loss: 0.15664 training acc: 0.95178 testing acc: 0.9468\n",
      "epoch: 5 iter: 434 loss: 0.14692 training acc: 0.95704 testing acc: 0.9504\n",
      "epoch: 5 iter: 435 loss: 0.13860 training acc: 0.96006 testing acc: 0.9558\n",
      "epoch: 5 iter: 436 loss: 0.14527 training acc: 0.95838 testing acc: 0.9552\n",
      "epoch: 5 iter: 437 loss: 0.13789 training acc: 0.95996 testing acc: 0.955\n",
      "epoch: 5 iter: 438 loss: 0.13865 training acc: 0.95964 testing acc: 0.9546\n",
      "epoch: 5 iter: 439 loss: 0.13443 training acc: 0.96062 testing acc: 0.9566\n",
      "epoch: 5 iter: 440 loss: 0.14071 training acc: 0.95842 testing acc: 0.9536\n",
      "epoch: 5 iter: 441 loss: 0.14672 training acc: 0.95796 testing acc: 0.9544\n",
      "epoch: 5 iter: 442 loss: 0.14492 training acc: 0.95808 testing acc: 0.9544\n",
      "epoch: 5 iter: 443 loss: 0.13796 training acc: 0.96108 testing acc: 0.9568\n",
      "epoch: 5 iter: 444 loss: 0.15342 training acc: 0.95528 testing acc: 0.9518\n",
      "epoch: 5 iter: 445 loss: 0.13878 training acc: 0.96022 testing acc: 0.9566\n",
      "epoch: 5 iter: 446 loss: 0.14042 training acc: 0.95910 testing acc: 0.9562\n",
      "epoch: 5 iter: 447 loss: 0.13486 training acc: 0.96170 testing acc: 0.9558\n",
      "epoch: 5 iter: 448 loss: 0.14268 training acc: 0.95952 testing acc: 0.9542\n",
      "epoch: 5 iter: 449 loss: 0.14338 training acc: 0.95850 testing acc: 0.953\n",
      "epoch: 5 iter: 450 loss: 0.13230 training acc: 0.96238 testing acc: 0.9552\n",
      "epoch: 5 iter: 451 loss: 0.13164 training acc: 0.96248 testing acc: 0.9546\n",
      "epoch: 5 iter: 452 loss: 0.13585 training acc: 0.96168 testing acc: 0.9556\n",
      "epoch: 5 iter: 453 loss: 0.13801 training acc: 0.96120 testing acc: 0.9546\n",
      "epoch: 5 iter: 454 loss: 0.14525 training acc: 0.95870 testing acc: 0.9544\n",
      "epoch: 5 iter: 455 loss: 0.14712 training acc: 0.95828 testing acc: 0.9532\n",
      "epoch: 5 iter: 456 loss: 0.14211 training acc: 0.95952 testing acc: 0.9542\n",
      "epoch: 5 iter: 457 loss: 0.14909 training acc: 0.95524 testing acc: 0.9492\n",
      "epoch: 5 iter: 458 loss: 0.14349 training acc: 0.95792 testing acc: 0.9518\n",
      "epoch: 5 iter: 459 loss: 0.13934 training acc: 0.95954 testing acc: 0.9518\n",
      "epoch: 5 iter: 460 loss: 0.13934 training acc: 0.95930 testing acc: 0.953\n",
      "epoch: 5 iter: 461 loss: 0.14692 training acc: 0.95688 testing acc: 0.952\n",
      "epoch: 5 iter: 462 loss: 0.14112 training acc: 0.95904 testing acc: 0.9524\n",
      "epoch: 5 iter: 463 loss: 0.13907 training acc: 0.95934 testing acc: 0.9514\n",
      "epoch: 5 iter: 464 loss: 0.13317 training acc: 0.96206 testing acc: 0.955\n",
      "epoch: 5 iter: 465 loss: 0.14055 training acc: 0.95932 testing acc: 0.9526\n",
      "epoch: 5 iter: 466 loss: 0.14818 training acc: 0.95718 testing acc: 0.9538\n",
      "epoch: 5 iter: 467 loss: 0.15568 training acc: 0.95444 testing acc: 0.9506\n",
      "epoch: 5 iter: 468 loss: 0.14036 training acc: 0.95988 testing acc: 0.9544\n",
      "epoch: 5 iter: 469 loss: 0.13821 training acc: 0.96048 testing acc: 0.9548\n",
      "epoch: 5 iter: 470 loss: 0.14223 training acc: 0.95950 testing acc: 0.9536\n",
      "epoch: 5 iter: 471 loss: 0.13623 training acc: 0.96124 testing acc: 0.9536\n",
      "epoch: 5 iter: 472 loss: 0.13395 training acc: 0.96136 testing acc: 0.954\n",
      "epoch: 5 iter: 473 loss: 0.13358 training acc: 0.96208 testing acc: 0.9518\n",
      "epoch: 5 iter: 474 loss: 0.13736 training acc: 0.96076 testing acc: 0.9512\n",
      "epoch: 5 iter: 475 loss: 0.13398 training acc: 0.96210 testing acc: 0.9536\n",
      "epoch: 5 iter: 476 loss: 0.14102 training acc: 0.95968 testing acc: 0.9514\n",
      "epoch: 5 iter: 477 loss: 0.13290 training acc: 0.96288 testing acc: 0.9562\n",
      "epoch: 5 iter: 478 loss: 0.13382 training acc: 0.96258 testing acc: 0.9556\n",
      "epoch: 5 iter: 479 loss: 0.13491 training acc: 0.96194 testing acc: 0.9558\n",
      "epoch: 5 iter: 480 loss: 0.13677 training acc: 0.96098 testing acc: 0.9548\n",
      "epoch: 5 iter: 481 loss: 0.13757 training acc: 0.96114 testing acc: 0.9554\n",
      "epoch: 5 iter: 482 loss: 0.13966 training acc: 0.96052 testing acc: 0.954\n",
      "epoch: 5 iter: 483 loss: 0.13960 training acc: 0.96050 testing acc: 0.9536\n",
      "epoch: 5 iter: 484 loss: 0.15142 training acc: 0.95678 testing acc: 0.951\n",
      "epoch: 5 iter: 485 loss: 0.14549 training acc: 0.95900 testing acc: 0.9538\n",
      "epoch: 5 iter: 486 loss: 0.13746 training acc: 0.96204 testing acc: 0.954\n",
      "epoch: 5 iter: 487 loss: 0.13851 training acc: 0.96016 testing acc: 0.9536\n",
      "epoch: 5 iter: 488 loss: 0.14062 training acc: 0.95938 testing acc: 0.954\n",
      "epoch: 5 iter: 489 loss: 0.14623 training acc: 0.95862 testing acc: 0.9504\n",
      "epoch: 5 iter: 490 loss: 0.13790 training acc: 0.96098 testing acc: 0.9534\n",
      "epoch: 5 iter: 491 loss: 0.13568 training acc: 0.96158 testing acc: 0.9528\n",
      "epoch: 5 iter: 492 loss: 0.14060 training acc: 0.95860 testing acc: 0.9522\n",
      "epoch: 5 iter: 493 loss: 0.14692 training acc: 0.95718 testing acc: 0.9498\n",
      "epoch: 5 iter: 494 loss: 0.14284 training acc: 0.95826 testing acc: 0.9524\n",
      "epoch: 5 iter: 495 loss: 0.15345 training acc: 0.95534 testing acc: 0.95\n",
      "epoch: 5 iter: 496 loss: 0.16126 training acc: 0.95308 testing acc: 0.947\n",
      "epoch: 5 iter: 497 loss: 0.17432 training acc: 0.94790 testing acc: 0.9416\n",
      "epoch: 5 iter: 498 loss: 0.16787 training acc: 0.94992 testing acc: 0.944\n",
      "epoch: 5 iter: 499 loss: 0.15278 training acc: 0.95520 testing acc: 0.9498\n",
      "epoch: 5 iter: 500 loss: 0.14035 training acc: 0.95824 testing acc: 0.9522\n",
      "epoch: 5 iter: 501 loss: 0.13816 training acc: 0.96048 testing acc: 0.9502\n",
      "epoch: 5 iter: 502 loss: 0.12931 training acc: 0.96342 testing acc: 0.953\n",
      "epoch: 5 iter: 503 loss: 0.13157 training acc: 0.96278 testing acc: 0.9548\n",
      "epoch: 5 iter: 504 loss: 0.13808 training acc: 0.96074 testing acc: 0.9536\n",
      "epoch: 5 iter: 505 loss: 0.13871 training acc: 0.96070 testing acc: 0.9512\n",
      "epoch: 5 iter: 506 loss: 0.14069 training acc: 0.96044 testing acc: 0.9538\n",
      "epoch: 5 iter: 507 loss: 0.13671 training acc: 0.96100 testing acc: 0.9532\n",
      "epoch: 5 iter: 508 loss: 0.13892 training acc: 0.96070 testing acc: 0.9534\n",
      "epoch: 5 iter: 509 loss: 0.13652 training acc: 0.96200 testing acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 iter: 510 loss: 0.13571 training acc: 0.96202 testing acc: 0.9534\n",
      "epoch: 5 iter: 511 loss: 0.17299 training acc: 0.94878 testing acc: 0.9418\n",
      "epoch: 5 iter: 512 loss: 0.14587 training acc: 0.95766 testing acc: 0.9474\n",
      "epoch: 5 iter: 513 loss: 0.14171 training acc: 0.95870 testing acc: 0.951\n",
      "epoch: 5 iter: 514 loss: 0.14051 training acc: 0.95936 testing acc: 0.9524\n",
      "epoch: 5 iter: 515 loss: 0.13778 training acc: 0.96048 testing acc: 0.9548\n",
      "epoch: 5 iter: 516 loss: 0.14904 training acc: 0.95648 testing acc: 0.9486\n",
      "epoch: 5 iter: 517 loss: 0.13589 training acc: 0.96182 testing acc: 0.9528\n",
      "epoch: 5 iter: 518 loss: 0.13969 training acc: 0.96056 testing acc: 0.9546\n",
      "epoch: 5 iter: 519 loss: 0.13261 training acc: 0.96298 testing acc: 0.9562\n",
      "epoch: 5 iter: 520 loss: 0.14145 training acc: 0.96008 testing acc: 0.9526\n",
      "epoch: 5 iter: 521 loss: 0.14837 training acc: 0.95734 testing acc: 0.9476\n",
      "epoch: 5 iter: 522 loss: 0.14606 training acc: 0.95808 testing acc: 0.9532\n",
      "epoch: 5 iter: 523 loss: 0.14774 training acc: 0.95726 testing acc: 0.9516\n",
      "epoch: 5 iter: 524 loss: 0.14038 training acc: 0.95960 testing acc: 0.9534\n",
      "epoch: 5 iter: 525 loss: 0.14134 training acc: 0.95978 testing acc: 0.9532\n",
      "epoch: 5 iter: 526 loss: 0.13443 training acc: 0.96200 testing acc: 0.9558\n",
      "epoch: 5 iter: 527 loss: 0.13476 training acc: 0.96180 testing acc: 0.9552\n",
      "epoch: 5 iter: 528 loss: 0.13823 training acc: 0.96098 testing acc: 0.9542\n",
      "epoch: 5 iter: 529 loss: 0.15113 training acc: 0.95540 testing acc: 0.9498\n",
      "epoch: 5 iter: 530 loss: 0.13973 training acc: 0.95920 testing acc: 0.9516\n",
      "epoch: 5 iter: 531 loss: 0.13526 training acc: 0.96044 testing acc: 0.954\n",
      "epoch: 5 iter: 532 loss: 0.13656 training acc: 0.95954 testing acc: 0.955\n",
      "epoch: 5 iter: 533 loss: 0.13818 training acc: 0.95904 testing acc: 0.953\n",
      "epoch: 5 iter: 534 loss: 0.14448 training acc: 0.95732 testing acc: 0.9498\n",
      "epoch: 5 iter: 535 loss: 0.14903 training acc: 0.95518 testing acc: 0.9488\n",
      "epoch: 5 iter: 536 loss: 0.15010 training acc: 0.95548 testing acc: 0.9474\n",
      "epoch: 5 iter: 537 loss: 0.14940 training acc: 0.95598 testing acc: 0.9506\n",
      "epoch: 5 iter: 538 loss: 0.13523 training acc: 0.96126 testing acc: 0.9554\n",
      "epoch: 5 iter: 539 loss: 0.13662 training acc: 0.96034 testing acc: 0.9536\n",
      "epoch: 5 iter: 540 loss: 0.14215 training acc: 0.95882 testing acc: 0.952\n",
      "epoch: 5 iter: 541 loss: 0.13467 training acc: 0.96106 testing acc: 0.9562\n",
      "epoch: 5 iter: 542 loss: 0.13549 training acc: 0.96168 testing acc: 0.9552\n",
      "epoch: 5 iter: 543 loss: 0.13897 training acc: 0.95984 testing acc: 0.9526\n",
      "epoch: 5 iter: 544 loss: 0.13790 training acc: 0.96042 testing acc: 0.9556\n",
      "epoch: 5 iter: 545 loss: 0.13633 training acc: 0.96152 testing acc: 0.954\n",
      "epoch: 5 iter: 546 loss: 0.13321 training acc: 0.96234 testing acc: 0.9552\n",
      "epoch: 5 iter: 547 loss: 0.13671 training acc: 0.96088 testing acc: 0.9536\n",
      "epoch: 5 iter: 548 loss: 0.14004 training acc: 0.96014 testing acc: 0.9512\n",
      "epoch: 5 iter: 549 loss: 0.14041 training acc: 0.96034 testing acc: 0.9506\n",
      "epoch: 5 iter: 550 loss: 0.14875 training acc: 0.95768 testing acc: 0.9512\n",
      "epoch: 5 iter: 551 loss: 0.15775 training acc: 0.95480 testing acc: 0.946\n",
      "epoch: 5 iter: 552 loss: 0.14787 training acc: 0.95746 testing acc: 0.9508\n",
      "epoch: 5 iter: 553 loss: 0.14715 training acc: 0.95784 testing acc: 0.9508\n",
      "epoch: 5 iter: 554 loss: 0.14035 training acc: 0.96042 testing acc: 0.9536\n",
      "epoch: 5 iter: 555 loss: 0.14182 training acc: 0.95916 testing acc: 0.9558\n",
      "epoch: 5 iter: 556 loss: 0.13546 training acc: 0.96052 testing acc: 0.9532\n",
      "epoch: 5 iter: 557 loss: 0.13520 training acc: 0.96068 testing acc: 0.9546\n",
      "epoch: 5 iter: 558 loss: 0.13410 training acc: 0.96144 testing acc: 0.9538\n",
      "epoch: 5 iter: 559 loss: 0.13415 training acc: 0.96112 testing acc: 0.9528\n",
      "epoch: 5 iter: 560 loss: 0.14071 training acc: 0.95880 testing acc: 0.9508\n",
      "epoch: 5 iter: 561 loss: 0.13602 training acc: 0.96078 testing acc: 0.9526\n",
      "epoch: 5 iter: 562 loss: 0.14318 training acc: 0.95884 testing acc: 0.9518\n",
      "epoch: 5 iter: 563 loss: 0.13446 training acc: 0.96188 testing acc: 0.9522\n",
      "epoch: 5 iter: 564 loss: 0.13141 training acc: 0.96236 testing acc: 0.954\n",
      "epoch: 5 iter: 565 loss: 0.13840 training acc: 0.96004 testing acc: 0.9502\n",
      "epoch: 5 iter: 566 loss: 0.13902 training acc: 0.95980 testing acc: 0.9526\n",
      "epoch: 5 iter: 567 loss: 0.13330 training acc: 0.96128 testing acc: 0.9532\n",
      "epoch: 5 iter: 568 loss: 0.15283 training acc: 0.95614 testing acc: 0.9472\n",
      "epoch: 5 iter: 569 loss: 0.14444 training acc: 0.95862 testing acc: 0.9494\n",
      "epoch: 5 iter: 570 loss: 0.14505 training acc: 0.95906 testing acc: 0.9508\n",
      "epoch: 5 iter: 571 loss: 0.14793 training acc: 0.95748 testing acc: 0.9498\n",
      "epoch: 5 iter: 572 loss: 0.15907 training acc: 0.95244 testing acc: 0.9468\n",
      "epoch: 5 iter: 573 loss: 0.14952 training acc: 0.95622 testing acc: 0.951\n",
      "epoch: 5 iter: 574 loss: 0.13212 training acc: 0.96216 testing acc: 0.9546\n",
      "epoch: 5 iter: 575 loss: 0.13598 training acc: 0.96106 testing acc: 0.9524\n",
      "epoch: 5 iter: 576 loss: 0.13496 training acc: 0.96126 testing acc: 0.9524\n",
      "epoch: 5 iter: 577 loss: 0.13223 training acc: 0.96196 testing acc: 0.955\n",
      "epoch: 5 iter: 578 loss: 0.13918 training acc: 0.95970 testing acc: 0.952\n",
      "epoch: 5 iter: 579 loss: 0.13599 training acc: 0.96080 testing acc: 0.9538\n",
      "epoch: 5 iter: 580 loss: 0.14037 training acc: 0.95880 testing acc: 0.9504\n",
      "epoch: 5 iter: 581 loss: 0.14018 training acc: 0.95808 testing acc: 0.95\n",
      "epoch: 5 iter: 582 loss: 0.13543 training acc: 0.95968 testing acc: 0.9524\n",
      "epoch: 5 iter: 583 loss: 0.14006 training acc: 0.95884 testing acc: 0.9522\n",
      "epoch: 5 iter: 584 loss: 0.13610 training acc: 0.96052 testing acc: 0.9564\n",
      "epoch: 5 iter: 585 loss: 0.13453 training acc: 0.96136 testing acc: 0.9568\n",
      "epoch: 5 iter: 586 loss: 0.13649 training acc: 0.96064 testing acc: 0.9536\n",
      "epoch: 5 iter: 587 loss: 0.13485 training acc: 0.96276 testing acc: 0.9528\n",
      "epoch: 5 iter: 588 loss: 0.13586 training acc: 0.96100 testing acc: 0.9526\n",
      "epoch: 5 iter: 589 loss: 0.13194 training acc: 0.96278 testing acc: 0.9546\n",
      "epoch: 5 iter: 590 loss: 0.13284 training acc: 0.96188 testing acc: 0.9534\n",
      "epoch: 5 iter: 591 loss: 0.14300 training acc: 0.95886 testing acc: 0.9526\n",
      "epoch: 5 iter: 592 loss: 0.13235 training acc: 0.96242 testing acc: 0.954\n",
      "epoch: 5 iter: 593 loss: 0.13293 training acc: 0.96144 testing acc: 0.956\n",
      "epoch: 5 iter: 594 loss: 0.13367 training acc: 0.96140 testing acc: 0.956\n",
      "epoch: 5 iter: 595 loss: 0.14504 training acc: 0.95944 testing acc: 0.9518\n",
      "epoch: 5 iter: 596 loss: 0.13668 training acc: 0.96190 testing acc: 0.955\n",
      "epoch: 5 iter: 597 loss: 0.13583 training acc: 0.96184 testing acc: 0.9548\n",
      "epoch: 5 iter: 598 loss: 0.14072 training acc: 0.95902 testing acc: 0.954\n",
      "epoch: 5 iter: 599 loss: 0.13601 training acc: 0.96070 testing acc: 0.956\n",
      "epoch: 5 iter: 600 loss: 0.13385 training acc: 0.96140 testing acc: 0.9566\n",
      "epoch: 5 iter: 601 loss: 0.14407 training acc: 0.95756 testing acc: 0.9518\n",
      "epoch: 5 iter: 602 loss: 0.13741 training acc: 0.96058 testing acc: 0.9536\n",
      "epoch: 5 iter: 603 loss: 0.13190 training acc: 0.96212 testing acc: 0.954\n",
      "epoch: 5 iter: 604 loss: 0.13390 training acc: 0.96100 testing acc: 0.9558\n",
      "epoch: 5 iter: 605 loss: 0.13381 training acc: 0.96182 testing acc: 0.9566\n",
      "epoch: 5 iter: 606 loss: 0.13487 training acc: 0.96080 testing acc: 0.955\n",
      "epoch: 5 iter: 607 loss: 0.15133 training acc: 0.95566 testing acc: 0.9478\n",
      "epoch: 5 iter: 608 loss: 0.15042 training acc: 0.95448 testing acc: 0.95\n",
      "epoch: 5 iter: 609 loss: 0.14217 training acc: 0.95806 testing acc: 0.9522\n",
      "epoch: 5 iter: 610 loss: 0.13732 training acc: 0.96044 testing acc: 0.9536\n",
      "epoch: 5 iter: 611 loss: 0.13727 training acc: 0.96072 testing acc: 0.9536\n",
      "epoch: 5 iter: 612 loss: 0.13366 training acc: 0.96182 testing acc: 0.9564\n",
      "epoch: 5 iter: 613 loss: 0.13318 training acc: 0.96196 testing acc: 0.9574\n",
      "epoch: 5 iter: 614 loss: 0.13394 training acc: 0.96124 testing acc: 0.9554\n",
      "epoch: 5 iter: 615 loss: 0.13797 training acc: 0.95958 testing acc: 0.9558\n",
      "epoch: 5 iter: 616 loss: 0.14020 training acc: 0.95984 testing acc: 0.9554\n",
      "epoch: 5 iter: 617 loss: 0.13860 training acc: 0.95938 testing acc: 0.9554\n",
      "epoch: 5 iter: 618 loss: 0.13745 training acc: 0.95974 testing acc: 0.9538\n",
      "epoch: 5 iter: 619 loss: 0.13286 training acc: 0.96172 testing acc: 0.9572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 iter: 620 loss: 0.12958 training acc: 0.96268 testing acc: 0.9582\n",
      "epoch: 5 iter: 621 loss: 0.13359 training acc: 0.96122 testing acc: 0.9542\n",
      "epoch: 5 iter: 622 loss: 0.14573 training acc: 0.95628 testing acc: 0.9518\n",
      "epoch: 5 iter: 623 loss: 0.15042 training acc: 0.95442 testing acc: 0.9516\n",
      "epoch: 5 iter: 624 loss: 0.13563 training acc: 0.96040 testing acc: 0.9542\n",
      "epoch: 5 iter: 625 loss: 0.14958 training acc: 0.95358 testing acc: 0.9476\n",
      "epoch: 5 iter: 626 loss: 0.15649 training acc: 0.95212 testing acc: 0.9462\n",
      "epoch: 5 iter: 627 loss: 0.14670 training acc: 0.95586 testing acc: 0.9504\n",
      "epoch: 5 iter: 628 loss: 0.14875 training acc: 0.95574 testing acc: 0.95\n",
      "epoch: 5 iter: 629 loss: 0.15211 training acc: 0.95456 testing acc: 0.9488\n",
      "epoch: 5 iter: 630 loss: 0.13279 training acc: 0.96148 testing acc: 0.9562\n",
      "epoch: 5 iter: 631 loss: 0.13921 training acc: 0.95868 testing acc: 0.9556\n",
      "epoch: 5 iter: 632 loss: 0.13922 training acc: 0.95940 testing acc: 0.9534\n",
      "epoch: 5 iter: 633 loss: 0.14604 training acc: 0.95776 testing acc: 0.9524\n",
      "epoch: 5 iter: 634 loss: 0.13487 training acc: 0.96196 testing acc: 0.9578\n",
      "epoch: 5 iter: 635 loss: 0.14118 training acc: 0.96070 testing acc: 0.9558\n",
      "epoch: 5 iter: 636 loss: 0.13908 training acc: 0.96140 testing acc: 0.9558\n",
      "epoch: 5 iter: 637 loss: 0.13474 training acc: 0.96262 testing acc: 0.957\n",
      "epoch: 5 iter: 638 loss: 0.13441 training acc: 0.96144 testing acc: 0.9562\n",
      "epoch: 5 iter: 639 loss: 0.12857 training acc: 0.96348 testing acc: 0.9574\n",
      "epoch: 5 iter: 640 loss: 0.13078 training acc: 0.96326 testing acc: 0.9574\n",
      "epoch: 5 iter: 641 loss: 0.12801 training acc: 0.96424 testing acc: 0.9592\n",
      "epoch: 5 iter: 642 loss: 0.13267 training acc: 0.96250 testing acc: 0.9562\n",
      "epoch: 5 iter: 643 loss: 0.12936 training acc: 0.96348 testing acc: 0.9592\n",
      "epoch: 5 iter: 644 loss: 0.13458 training acc: 0.96176 testing acc: 0.9548\n",
      "epoch: 5 iter: 645 loss: 0.14566 training acc: 0.95700 testing acc: 0.9512\n",
      "epoch: 5 iter: 646 loss: 0.13409 training acc: 0.96078 testing acc: 0.9558\n",
      "epoch: 5 iter: 647 loss: 0.13741 training acc: 0.95994 testing acc: 0.9542\n",
      "epoch: 5 iter: 648 loss: 0.13618 training acc: 0.96042 testing acc: 0.9528\n",
      "epoch: 5 iter: 649 loss: 0.12983 training acc: 0.96268 testing acc: 0.9546\n",
      "epoch: 5 iter: 650 loss: 0.12754 training acc: 0.96378 testing acc: 0.9562\n",
      "epoch: 5 iter: 651 loss: 0.13606 training acc: 0.95998 testing acc: 0.953\n",
      "epoch: 5 iter: 652 loss: 0.13586 training acc: 0.96004 testing acc: 0.9546\n",
      "epoch: 5 iter: 653 loss: 0.13647 training acc: 0.95986 testing acc: 0.9534\n",
      "epoch: 5 iter: 654 loss: 0.13883 training acc: 0.95928 testing acc: 0.953\n",
      "epoch: 5 iter: 655 loss: 0.13311 training acc: 0.96128 testing acc: 0.9558\n",
      "epoch: 5 iter: 656 loss: 0.14681 training acc: 0.95666 testing acc: 0.9538\n",
      "epoch: 5 iter: 657 loss: 0.15167 training acc: 0.95532 testing acc: 0.9504\n",
      "epoch: 5 iter: 658 loss: 0.13398 training acc: 0.96124 testing acc: 0.9558\n",
      "epoch: 5 iter: 659 loss: 0.13945 training acc: 0.95952 testing acc: 0.9532\n",
      "epoch: 5 iter: 660 loss: 0.12944 training acc: 0.96270 testing acc: 0.9568\n",
      "epoch: 5 iter: 661 loss: 0.13357 training acc: 0.96170 testing acc: 0.957\n",
      "epoch: 5 iter: 662 loss: 0.16970 training acc: 0.94998 testing acc: 0.943\n",
      "epoch: 5 iter: 663 loss: 0.14126 training acc: 0.95998 testing acc: 0.9514\n",
      "epoch: 5 iter: 664 loss: 0.13929 training acc: 0.96006 testing acc: 0.9534\n",
      "epoch: 5 iter: 665 loss: 0.13506 training acc: 0.96090 testing acc: 0.956\n",
      "epoch: 5 iter: 666 loss: 0.13044 training acc: 0.96298 testing acc: 0.9566\n",
      "epoch: 5 iter: 667 loss: 0.13169 training acc: 0.96242 testing acc: 0.9558\n",
      "epoch: 5 iter: 668 loss: 0.13012 training acc: 0.96260 testing acc: 0.9576\n",
      "epoch: 5 iter: 669 loss: 0.12893 training acc: 0.96298 testing acc: 0.9558\n",
      "epoch: 5 iter: 670 loss: 0.14020 training acc: 0.95950 testing acc: 0.9518\n",
      "epoch: 5 iter: 671 loss: 0.14830 training acc: 0.95658 testing acc: 0.9502\n",
      "epoch: 5 iter: 672 loss: 0.13681 training acc: 0.96080 testing acc: 0.9534\n",
      "epoch: 5 iter: 673 loss: 0.13937 training acc: 0.95970 testing acc: 0.9556\n",
      "epoch: 5 iter: 674 loss: 0.16140 training acc: 0.95114 testing acc: 0.9468\n",
      "epoch: 5 iter: 675 loss: 0.14915 training acc: 0.95586 testing acc: 0.9488\n",
      "epoch: 5 iter: 676 loss: 0.14074 training acc: 0.95872 testing acc: 0.9516\n",
      "epoch: 5 iter: 677 loss: 0.14823 training acc: 0.95644 testing acc: 0.9516\n",
      "epoch: 5 iter: 678 loss: 0.13768 training acc: 0.96014 testing acc: 0.957\n",
      "epoch: 5 iter: 679 loss: 0.13750 training acc: 0.95970 testing acc: 0.9528\n",
      "epoch: 5 iter: 680 loss: 0.13134 training acc: 0.96188 testing acc: 0.9552\n",
      "epoch: 5 iter: 681 loss: 0.14307 training acc: 0.95766 testing acc: 0.9508\n",
      "epoch: 5 iter: 682 loss: 0.13900 training acc: 0.95900 testing acc: 0.9538\n",
      "epoch: 5 iter: 683 loss: 0.13200 training acc: 0.96164 testing acc: 0.9576\n",
      "epoch: 5 iter: 684 loss: 0.13461 training acc: 0.95990 testing acc: 0.9566\n",
      "epoch: 5 iter: 685 loss: 0.12977 training acc: 0.96192 testing acc: 0.9568\n",
      "epoch: 5 iter: 686 loss: 0.13941 training acc: 0.95776 testing acc: 0.951\n",
      "epoch: 5 iter: 687 loss: 0.14748 training acc: 0.95500 testing acc: 0.9482\n",
      "epoch: 5 iter: 688 loss: 0.14696 training acc: 0.95460 testing acc: 0.9482\n",
      "epoch: 5 iter: 689 loss: 0.13542 training acc: 0.95934 testing acc: 0.9522\n",
      "epoch: 5 iter: 690 loss: 0.13474 training acc: 0.96056 testing acc: 0.9536\n",
      "epoch: 5 iter: 691 loss: 0.13094 training acc: 0.96234 testing acc: 0.9574\n",
      "epoch: 5 iter: 692 loss: 0.13504 training acc: 0.96070 testing acc: 0.9552\n",
      "epoch: 5 iter: 693 loss: 0.13553 training acc: 0.96048 testing acc: 0.9562\n",
      "epoch: 5 iter: 694 loss: 0.13263 training acc: 0.96134 testing acc: 0.955\n",
      "epoch: 5 iter: 695 loss: 0.13590 training acc: 0.96090 testing acc: 0.9544\n",
      "epoch: 5 iter: 696 loss: 0.14834 training acc: 0.95632 testing acc: 0.953\n",
      "epoch: 5 iter: 697 loss: 0.12950 training acc: 0.96274 testing acc: 0.9576\n",
      "epoch: 5 iter: 698 loss: 0.12865 training acc: 0.96390 testing acc: 0.958\n",
      "epoch: 5 iter: 699 loss: 0.12823 training acc: 0.96382 testing acc: 0.9588\n",
      "epoch: 5 iter: 700 loss: 0.13375 training acc: 0.96186 testing acc: 0.9564\n",
      "epoch: 5 iter: 701 loss: 0.12345 training acc: 0.96560 testing acc: 0.9604\n",
      "epoch: 5 iter: 702 loss: 0.12416 training acc: 0.96496 testing acc: 0.9622\n",
      "epoch: 5 iter: 703 loss: 0.13218 training acc: 0.96106 testing acc: 0.9578\n",
      "epoch: 5 iter: 704 loss: 0.13018 training acc: 0.96152 testing acc: 0.96\n",
      "epoch: 5 iter: 705 loss: 0.14129 training acc: 0.95940 testing acc: 0.9566\n",
      "epoch: 5 iter: 706 loss: 0.12542 training acc: 0.96404 testing acc: 0.963\n",
      "epoch: 5 iter: 707 loss: 0.13154 training acc: 0.96234 testing acc: 0.9568\n",
      "epoch: 5 iter: 708 loss: 0.12709 training acc: 0.96404 testing acc: 0.9584\n",
      "epoch: 5 iter: 709 loss: 0.12711 training acc: 0.96406 testing acc: 0.959\n",
      "epoch: 5 iter: 710 loss: 0.13203 training acc: 0.96300 testing acc: 0.9548\n",
      "epoch: 5 iter: 711 loss: 0.13008 training acc: 0.96322 testing acc: 0.9572\n",
      "epoch: 5 iter: 712 loss: 0.12950 training acc: 0.96216 testing acc: 0.9586\n",
      "epoch: 5 iter: 713 loss: 0.15305 training acc: 0.95564 testing acc: 0.9498\n",
      "epoch: 5 iter: 714 loss: 0.13191 training acc: 0.96152 testing acc: 0.9568\n",
      "epoch: 5 iter: 715 loss: 0.13396 training acc: 0.96060 testing acc: 0.9566\n",
      "epoch: 5 iter: 716 loss: 0.13864 training acc: 0.96040 testing acc: 0.9532\n",
      "epoch: 5 iter: 717 loss: 0.13391 training acc: 0.96200 testing acc: 0.9566\n",
      "epoch: 5 iter: 718 loss: 0.13758 training acc: 0.96030 testing acc: 0.9532\n",
      "epoch: 5 iter: 719 loss: 0.13308 training acc: 0.96130 testing acc: 0.9548\n",
      "epoch: 5 iter: 720 loss: 0.14384 training acc: 0.95750 testing acc: 0.947\n",
      "epoch: 5 iter: 721 loss: 0.14367 training acc: 0.95732 testing acc: 0.9464\n",
      "epoch: 5 iter: 722 loss: 0.14263 training acc: 0.95804 testing acc: 0.9486\n",
      "epoch: 5 iter: 723 loss: 0.13414 training acc: 0.96174 testing acc: 0.9518\n",
      "epoch: 5 iter: 724 loss: 0.12912 training acc: 0.96338 testing acc: 0.9546\n",
      "epoch: 5 iter: 725 loss: 0.13704 training acc: 0.96092 testing acc: 0.9526\n",
      "epoch: 5 iter: 726 loss: 0.12356 training acc: 0.96546 testing acc: 0.9566\n",
      "epoch: 5 iter: 727 loss: 0.12876 training acc: 0.96368 testing acc: 0.9564\n",
      "epoch: 5 iter: 728 loss: 0.13913 training acc: 0.96012 testing acc: 0.9514\n",
      "epoch: 5 iter: 729 loss: 0.13312 training acc: 0.96212 testing acc: 0.9566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 iter: 730 loss: 0.12625 training acc: 0.96370 testing acc: 0.9588\n",
      "epoch: 5 iter: 731 loss: 0.13124 training acc: 0.96238 testing acc: 0.9556\n",
      "epoch: 5 iter: 732 loss: 0.13358 training acc: 0.96206 testing acc: 0.9556\n",
      "epoch: 5 iter: 733 loss: 0.13024 training acc: 0.96256 testing acc: 0.956\n",
      "epoch: 5 iter: 734 loss: 0.13461 training acc: 0.96094 testing acc: 0.9558\n",
      "epoch: 5 iter: 735 loss: 0.12791 training acc: 0.96304 testing acc: 0.9576\n",
      "epoch: 5 iter: 736 loss: 0.12537 training acc: 0.96382 testing acc: 0.9588\n",
      "epoch: 5 iter: 737 loss: 0.12568 training acc: 0.96388 testing acc: 0.9578\n",
      "epoch: 5 iter: 738 loss: 0.12348 training acc: 0.96500 testing acc: 0.9606\n",
      "epoch: 5 iter: 739 loss: 0.13008 training acc: 0.96250 testing acc: 0.9574\n",
      "epoch: 5 iter: 740 loss: 0.13125 training acc: 0.96100 testing acc: 0.9562\n",
      "epoch: 5 iter: 741 loss: 0.13253 training acc: 0.96018 testing acc: 0.9552\n",
      "epoch: 5 iter: 742 loss: 0.12935 training acc: 0.96182 testing acc: 0.9552\n",
      "epoch: 5 iter: 743 loss: 0.12951 training acc: 0.96326 testing acc: 0.9584\n",
      "epoch: 5 iter: 744 loss: 0.12860 training acc: 0.96314 testing acc: 0.9584\n",
      "epoch: 5 iter: 745 loss: 0.12797 training acc: 0.96384 testing acc: 0.9596\n",
      "epoch: 5 iter: 746 loss: 0.13905 training acc: 0.96064 testing acc: 0.9568\n",
      "epoch: 5 iter: 747 loss: 0.12459 training acc: 0.96420 testing acc: 0.9624\n",
      "epoch: 5 iter: 748 loss: 0.12958 training acc: 0.96216 testing acc: 0.9572\n",
      "epoch: 5 iter: 749 loss: 0.12356 training acc: 0.96456 testing acc: 0.9614\n",
      "epoch: 5 iter: 750 loss: 0.12283 training acc: 0.96478 testing acc: 0.9604\n",
      "epoch: 5 iter: 751 loss: 0.12248 training acc: 0.96478 testing acc: 0.9602\n",
      "epoch: 5 iter: 752 loss: 0.12698 training acc: 0.96314 testing acc: 0.9586\n",
      "epoch: 5 iter: 753 loss: 0.13197 training acc: 0.96220 testing acc: 0.9576\n",
      "epoch: 5 iter: 754 loss: 0.13053 training acc: 0.96222 testing acc: 0.9568\n",
      "epoch: 5 iter: 755 loss: 0.12564 training acc: 0.96398 testing acc: 0.9586\n",
      "epoch: 5 iter: 756 loss: 0.12469 training acc: 0.96312 testing acc: 0.9556\n",
      "epoch: 5 iter: 757 loss: 0.12751 training acc: 0.96292 testing acc: 0.9536\n",
      "epoch: 5 iter: 758 loss: 0.12859 training acc: 0.96332 testing acc: 0.9554\n",
      "epoch: 5 iter: 759 loss: 0.13180 training acc: 0.96144 testing acc: 0.9556\n",
      "epoch: 5 iter: 760 loss: 0.12721 training acc: 0.96308 testing acc: 0.9538\n",
      "epoch: 5 iter: 761 loss: 0.12616 training acc: 0.96346 testing acc: 0.953\n",
      "epoch: 5 iter: 762 loss: 0.13249 training acc: 0.96184 testing acc: 0.953\n",
      "epoch: 5 iter: 763 loss: 0.12680 training acc: 0.96418 testing acc: 0.9556\n",
      "epoch: 5 iter: 764 loss: 0.13015 training acc: 0.96348 testing acc: 0.9554\n",
      "epoch: 5 iter: 765 loss: 0.12765 training acc: 0.96422 testing acc: 0.9556\n",
      "epoch: 5 iter: 766 loss: 0.12827 training acc: 0.96320 testing acc: 0.9534\n",
      "epoch: 5 iter: 767 loss: 0.12948 training acc: 0.96292 testing acc: 0.9542\n",
      "epoch: 5 iter: 768 loss: 0.12572 training acc: 0.96460 testing acc: 0.9566\n",
      "epoch: 5 iter: 769 loss: 0.13169 training acc: 0.96238 testing acc: 0.9558\n",
      "epoch: 5 iter: 770 loss: 0.12279 training acc: 0.96524 testing acc: 0.959\n",
      "epoch: 5 iter: 771 loss: 0.14632 training acc: 0.95592 testing acc: 0.951\n",
      "epoch: 5 iter: 772 loss: 0.13765 training acc: 0.95968 testing acc: 0.9514\n",
      "epoch: 5 iter: 773 loss: 0.12988 training acc: 0.96242 testing acc: 0.9532\n",
      "epoch: 5 iter: 774 loss: 0.13255 training acc: 0.96130 testing acc: 0.9552\n",
      "epoch: 5 iter: 775 loss: 0.12849 training acc: 0.96190 testing acc: 0.956\n",
      "epoch: 5 iter: 776 loss: 0.12128 training acc: 0.96546 testing acc: 0.9578\n",
      "epoch: 5 iter: 777 loss: 0.12164 training acc: 0.96522 testing acc: 0.9586\n",
      "epoch: 5 iter: 778 loss: 0.12315 training acc: 0.96476 testing acc: 0.9598\n",
      "epoch: 5 iter: 779 loss: 0.14712 training acc: 0.95638 testing acc: 0.9522\n",
      "epoch: 5 iter: 780 loss: 0.15275 training acc: 0.95454 testing acc: 0.9474\n",
      "epoch: 5 iter: 781 loss: 0.14097 training acc: 0.95760 testing acc: 0.952\n",
      "epoch: 5 iter: 782 loss: 0.12682 training acc: 0.96304 testing acc: 0.9566\n",
      "epoch: 5 iter: 783 loss: 0.11986 training acc: 0.96584 testing acc: 0.9596\n",
      "epoch: 5 iter: 784 loss: 0.12912 training acc: 0.96268 testing acc: 0.9554\n",
      "epoch: 5 iter: 785 loss: 0.12255 training acc: 0.96478 testing acc: 0.959\n",
      "epoch: 5 iter: 786 loss: 0.12542 training acc: 0.96370 testing acc: 0.9598\n",
      "epoch: 5 iter: 787 loss: 0.13582 training acc: 0.95934 testing acc: 0.954\n",
      "epoch: 5 iter: 788 loss: 0.14460 training acc: 0.95552 testing acc: 0.951\n",
      "epoch: 5 iter: 789 loss: 0.13306 training acc: 0.96046 testing acc: 0.9556\n",
      "epoch: 5 iter: 790 loss: 0.13907 training acc: 0.95814 testing acc: 0.95\n",
      "epoch: 5 iter: 791 loss: 0.14425 training acc: 0.95648 testing acc: 0.9492\n",
      "epoch: 5 iter: 792 loss: 0.14397 training acc: 0.95652 testing acc: 0.9502\n",
      "epoch: 5 iter: 793 loss: 0.14200 training acc: 0.95680 testing acc: 0.9504\n",
      "epoch: 5 iter: 794 loss: 0.13537 training acc: 0.95960 testing acc: 0.9532\n",
      "epoch: 5 iter: 795 loss: 0.13375 training acc: 0.96078 testing acc: 0.9532\n",
      "epoch: 5 iter: 796 loss: 0.14226 training acc: 0.95730 testing acc: 0.9502\n",
      "epoch: 5 iter: 797 loss: 0.15730 training acc: 0.95234 testing acc: 0.9472\n",
      "epoch: 5 iter: 798 loss: 0.13876 training acc: 0.95918 testing acc: 0.952\n",
      "epoch: 5 iter: 799 loss: 0.12426 training acc: 0.96372 testing acc: 0.957\n",
      "epoch: 5 iter: 800 loss: 0.12994 training acc: 0.96292 testing acc: 0.9556\n",
      "epoch: 5 iter: 801 loss: 0.13216 training acc: 0.96182 testing acc: 0.954\n",
      "epoch: 5 iter: 802 loss: 0.13823 training acc: 0.95982 testing acc: 0.952\n",
      "epoch: 5 iter: 803 loss: 0.13318 training acc: 0.96146 testing acc: 0.953\n",
      "epoch: 5 iter: 804 loss: 0.12946 training acc: 0.96262 testing acc: 0.9554\n",
      "epoch: 5 iter: 805 loss: 0.12817 training acc: 0.96346 testing acc: 0.9554\n",
      "epoch: 5 iter: 806 loss: 0.14341 training acc: 0.95814 testing acc: 0.9532\n",
      "epoch: 5 iter: 807 loss: 0.13476 training acc: 0.96144 testing acc: 0.9546\n",
      "epoch: 5 iter: 808 loss: 0.14118 training acc: 0.95896 testing acc: 0.9512\n",
      "epoch: 5 iter: 809 loss: 0.13412 training acc: 0.96190 testing acc: 0.955\n",
      "epoch: 5 iter: 810 loss: 0.13027 training acc: 0.96252 testing acc: 0.9562\n",
      "epoch: 5 iter: 811 loss: 0.14460 training acc: 0.95758 testing acc: 0.9508\n",
      "epoch: 5 iter: 812 loss: 0.14328 training acc: 0.95658 testing acc: 0.9528\n",
      "epoch: 5 iter: 813 loss: 0.13493 training acc: 0.96050 testing acc: 0.9544\n",
      "epoch: 5 iter: 814 loss: 0.13385 training acc: 0.96124 testing acc: 0.9546\n",
      "epoch: 5 iter: 815 loss: 0.12778 training acc: 0.96356 testing acc: 0.9548\n",
      "epoch: 5 iter: 816 loss: 0.12701 training acc: 0.96362 testing acc: 0.9564\n",
      "epoch: 5 iter: 817 loss: 0.12902 training acc: 0.96292 testing acc: 0.9548\n",
      "epoch: 5 iter: 818 loss: 0.12545 training acc: 0.96412 testing acc: 0.956\n",
      "epoch: 5 iter: 819 loss: 0.14916 training acc: 0.95552 testing acc: 0.952\n",
      "epoch: 5 iter: 820 loss: 0.12654 training acc: 0.96322 testing acc: 0.9578\n",
      "epoch: 5 iter: 821 loss: 0.12650 training acc: 0.96322 testing acc: 0.9588\n",
      "epoch: 5 iter: 822 loss: 0.12260 training acc: 0.96446 testing acc: 0.9608\n",
      "epoch: 5 iter: 823 loss: 0.12198 training acc: 0.96440 testing acc: 0.9616\n",
      "epoch: 5 iter: 824 loss: 0.13171 training acc: 0.96176 testing acc: 0.958\n",
      "epoch: 5 iter: 825 loss: 0.12749 training acc: 0.96272 testing acc: 0.9574\n",
      "epoch: 5 iter: 826 loss: 0.12591 training acc: 0.96308 testing acc: 0.958\n",
      "epoch: 5 iter: 827 loss: 0.12639 training acc: 0.96340 testing acc: 0.9582\n",
      "epoch: 5 iter: 828 loss: 0.12654 training acc: 0.96276 testing acc: 0.9568\n",
      "epoch: 5 iter: 829 loss: 0.13636 training acc: 0.95874 testing acc: 0.955\n",
      "epoch: 5 iter: 830 loss: 0.12521 training acc: 0.96262 testing acc: 0.9586\n",
      "epoch: 5 iter: 831 loss: 0.12033 training acc: 0.96494 testing acc: 0.9602\n",
      "epoch: 5 iter: 832 loss: 0.12253 training acc: 0.96434 testing acc: 0.9616\n",
      "epoch: 5 iter: 833 loss: 0.12769 training acc: 0.96260 testing acc: 0.9598\n",
      "epoch: 5 iter: 834 loss: 0.12090 training acc: 0.96580 testing acc: 0.9612\n",
      "epoch: 5 iter: 835 loss: 0.12863 training acc: 0.96338 testing acc: 0.959\n",
      "epoch: 5 iter: 836 loss: 0.12712 training acc: 0.96358 testing acc: 0.9568\n",
      "epoch: 5 iter: 837 loss: 0.12508 training acc: 0.96378 testing acc: 0.9584\n",
      "epoch: 5 iter: 838 loss: 0.13908 training acc: 0.95962 testing acc: 0.9522\n",
      "epoch: 5 iter: 839 loss: 0.13105 training acc: 0.96158 testing acc: 0.9548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 iter: 840 loss: 0.12424 training acc: 0.96318 testing acc: 0.9568\n",
      "epoch: 5 iter: 841 loss: 0.12918 training acc: 0.96224 testing acc: 0.9568\n",
      "epoch: 5 iter: 842 loss: 0.13751 training acc: 0.95994 testing acc: 0.954\n",
      "epoch: 5 iter: 843 loss: 0.14914 training acc: 0.95762 testing acc: 0.9492\n",
      "epoch: 5 iter: 844 loss: 0.13226 training acc: 0.96122 testing acc: 0.9552\n",
      "epoch: 5 iter: 845 loss: 0.13274 training acc: 0.96072 testing acc: 0.954\n",
      "epoch: 5 iter: 846 loss: 0.12952 training acc: 0.96304 testing acc: 0.9596\n",
      "epoch: 5 iter: 847 loss: 0.15725 training acc: 0.95390 testing acc: 0.9488\n",
      "epoch: 5 iter: 848 loss: 0.15184 training acc: 0.95460 testing acc: 0.9472\n",
      "epoch: 5 iter: 849 loss: 0.13481 training acc: 0.95990 testing acc: 0.9536\n",
      "epoch: 5 iter: 850 loss: 0.13764 training acc: 0.95864 testing acc: 0.952\n",
      "epoch: 5 iter: 851 loss: 0.12515 training acc: 0.96314 testing acc: 0.9592\n",
      "epoch: 5 iter: 852 loss: 0.13147 training acc: 0.96192 testing acc: 0.9558\n",
      "epoch: 5 iter: 853 loss: 0.13295 training acc: 0.96120 testing acc: 0.9532\n",
      "epoch: 5 iter: 854 loss: 0.12904 training acc: 0.96198 testing acc: 0.9572\n",
      "epoch: 5 iter: 855 loss: 0.15091 training acc: 0.95418 testing acc: 0.9502\n",
      "epoch: 5 iter: 856 loss: 0.12642 training acc: 0.96264 testing acc: 0.9576\n",
      "epoch: 5 iter: 857 loss: 0.13168 training acc: 0.96134 testing acc: 0.9548\n",
      "epoch: 5 iter: 858 loss: 0.12666 training acc: 0.96324 testing acc: 0.96\n",
      "epoch: 5 iter: 859 loss: 0.12566 training acc: 0.96382 testing acc: 0.9576\n",
      "epoch: 5 iter: 860 loss: 0.12782 training acc: 0.96346 testing acc: 0.958\n",
      "epoch: 5 iter: 861 loss: 0.14060 training acc: 0.95866 testing acc: 0.9532\n",
      "epoch: 5 iter: 862 loss: 0.13042 training acc: 0.96248 testing acc: 0.9574\n",
      "epoch: 5 iter: 863 loss: 0.13180 training acc: 0.96166 testing acc: 0.9574\n",
      "epoch: 5 iter: 864 loss: 0.12184 training acc: 0.96492 testing acc: 0.9608\n",
      "epoch: 5 iter: 865 loss: 0.12144 training acc: 0.96494 testing acc: 0.9616\n",
      "epoch: 5 iter: 866 loss: 0.12289 training acc: 0.96434 testing acc: 0.9604\n",
      "epoch: 5 iter: 867 loss: 0.12791 training acc: 0.96270 testing acc: 0.9582\n",
      "epoch: 5 iter: 868 loss: 0.12154 training acc: 0.96464 testing acc: 0.9622\n",
      "epoch: 5 iter: 869 loss: 0.12592 training acc: 0.96328 testing acc: 0.9602\n",
      "epoch: 5 iter: 870 loss: 0.12214 training acc: 0.96472 testing acc: 0.9628\n",
      "epoch: 5 iter: 871 loss: 0.12663 training acc: 0.96404 testing acc: 0.9608\n",
      "epoch: 5 iter: 872 loss: 0.13110 training acc: 0.96200 testing acc: 0.9588\n",
      "epoch: 5 iter: 873 loss: 0.12473 training acc: 0.96410 testing acc: 0.9624\n",
      "epoch: 5 iter: 874 loss: 0.13679 training acc: 0.96010 testing acc: 0.9544\n",
      "epoch: 5 iter: 875 loss: 0.12041 training acc: 0.96602 testing acc: 0.9632\n",
      "epoch: 5 iter: 876 loss: 0.11747 training acc: 0.96684 testing acc: 0.9636\n",
      "epoch: 5 iter: 877 loss: 0.11543 training acc: 0.96692 testing acc: 0.9642\n",
      "epoch: 5 iter: 878 loss: 0.12403 training acc: 0.96418 testing acc: 0.9596\n",
      "epoch: 5 iter: 879 loss: 0.12362 training acc: 0.96408 testing acc: 0.9578\n",
      "epoch: 5 iter: 880 loss: 0.12068 training acc: 0.96552 testing acc: 0.96\n",
      "epoch: 5 iter: 881 loss: 0.11850 training acc: 0.96630 testing acc: 0.9618\n",
      "epoch: 5 iter: 882 loss: 0.11858 training acc: 0.96642 testing acc: 0.961\n",
      "epoch: 5 iter: 883 loss: 0.11856 training acc: 0.96638 testing acc: 0.9594\n",
      "epoch: 5 iter: 884 loss: 0.11909 training acc: 0.96578 testing acc: 0.9598\n",
      "epoch: 5 iter: 885 loss: 0.11992 training acc: 0.96554 testing acc: 0.9592\n",
      "epoch: 5 iter: 886 loss: 0.12077 training acc: 0.96504 testing acc: 0.9576\n",
      "epoch: 5 iter: 887 loss: 0.12149 training acc: 0.96406 testing acc: 0.958\n",
      "epoch: 5 iter: 888 loss: 0.12341 training acc: 0.96334 testing acc: 0.958\n",
      "epoch: 5 iter: 889 loss: 0.12367 training acc: 0.96462 testing acc: 0.9592\n",
      "epoch: 5 iter: 890 loss: 0.12296 training acc: 0.96518 testing acc: 0.9594\n",
      "epoch: 5 iter: 891 loss: 0.12360 training acc: 0.96522 testing acc: 0.9626\n",
      "epoch: 5 iter: 892 loss: 0.12335 training acc: 0.96514 testing acc: 0.961\n",
      "epoch: 5 iter: 893 loss: 0.12878 training acc: 0.96384 testing acc: 0.9588\n",
      "epoch: 5 iter: 894 loss: 0.11895 training acc: 0.96612 testing acc: 0.9592\n",
      "epoch: 5 iter: 895 loss: 0.11550 training acc: 0.96698 testing acc: 0.9634\n",
      "epoch: 5 iter: 896 loss: 0.11970 training acc: 0.96592 testing acc: 0.961\n",
      "epoch: 5 iter: 897 loss: 0.11767 training acc: 0.96670 testing acc: 0.9616\n",
      "epoch: 5 iter: 898 loss: 0.12666 training acc: 0.96238 testing acc: 0.958\n",
      "epoch: 5 iter: 899 loss: 0.12726 training acc: 0.96280 testing acc: 0.9586\n",
      "epoch: 5 iter: 900 loss: 0.12511 training acc: 0.96430 testing acc: 0.9594\n",
      "epoch: 5 iter: 901 loss: 0.12313 training acc: 0.96444 testing acc: 0.96\n",
      "epoch: 5 iter: 902 loss: 0.12353 training acc: 0.96394 testing acc: 0.9592\n",
      "epoch: 5 iter: 903 loss: 0.12721 training acc: 0.96270 testing acc: 0.9574\n",
      "epoch: 5 iter: 904 loss: 0.12644 training acc: 0.96306 testing acc: 0.9574\n",
      "epoch: 5 iter: 905 loss: 0.12449 training acc: 0.96420 testing acc: 0.9572\n",
      "epoch: 5 iter: 906 loss: 0.12385 training acc: 0.96394 testing acc: 0.9578\n",
      "epoch: 5 iter: 907 loss: 0.12371 training acc: 0.96438 testing acc: 0.9562\n",
      "epoch: 5 iter: 908 loss: 0.12146 training acc: 0.96504 testing acc: 0.9576\n",
      "epoch: 5 iter: 909 loss: 0.11855 training acc: 0.96618 testing acc: 0.9606\n",
      "epoch: 5 iter: 910 loss: 0.12004 training acc: 0.96570 testing acc: 0.96\n",
      "epoch: 5 iter: 911 loss: 0.12127 training acc: 0.96514 testing acc: 0.9586\n",
      "epoch: 5 iter: 912 loss: 0.13337 training acc: 0.96120 testing acc: 0.9542\n",
      "epoch: 5 iter: 913 loss: 0.12476 training acc: 0.96482 testing acc: 0.9576\n",
      "epoch: 5 iter: 914 loss: 0.12241 training acc: 0.96584 testing acc: 0.9602\n",
      "epoch: 5 iter: 915 loss: 0.12731 training acc: 0.96322 testing acc: 0.959\n",
      "epoch: 5 iter: 916 loss: 0.12345 training acc: 0.96448 testing acc: 0.9606\n",
      "epoch: 5 iter: 917 loss: 0.12206 training acc: 0.96538 testing acc: 0.9608\n",
      "epoch: 5 iter: 918 loss: 0.12578 training acc: 0.96452 testing acc: 0.9584\n",
      "epoch: 5 iter: 919 loss: 0.12452 training acc: 0.96410 testing acc: 0.9576\n",
      "epoch: 5 iter: 920 loss: 0.12872 training acc: 0.96324 testing acc: 0.9578\n",
      "epoch: 5 iter: 921 loss: 0.13182 training acc: 0.96248 testing acc: 0.9578\n",
      "epoch: 5 iter: 922 loss: 0.12756 training acc: 0.96378 testing acc: 0.9588\n",
      "epoch: 5 iter: 923 loss: 0.12412 training acc: 0.96438 testing acc: 0.9576\n",
      "epoch: 5 iter: 924 loss: 0.12462 training acc: 0.96390 testing acc: 0.9574\n",
      "epoch: 5 iter: 925 loss: 0.12528 training acc: 0.96372 testing acc: 0.9576\n",
      "epoch: 5 iter: 926 loss: 0.12506 training acc: 0.96424 testing acc: 0.9592\n",
      "epoch: 5 iter: 927 loss: 0.12277 training acc: 0.96464 testing acc: 0.958\n",
      "epoch: 5 iter: 928 loss: 0.17887 training acc: 0.94552 testing acc: 0.9402\n",
      "epoch: 5 iter: 929 loss: 0.12743 training acc: 0.96344 testing acc: 0.9568\n",
      "epoch: 5 iter: 930 loss: 0.12065 training acc: 0.96528 testing acc: 0.9602\n",
      "epoch: 5 iter: 931 loss: 0.11882 training acc: 0.96598 testing acc: 0.962\n",
      "epoch: 5 iter: 932 loss: 0.12033 training acc: 0.96604 testing acc: 0.9608\n",
      "epoch: 5 iter: 933 loss: 0.11935 training acc: 0.96614 testing acc: 0.9602\n",
      "epoch: 5 iter: 934 loss: 0.11785 training acc: 0.96678 testing acc: 0.9598\n",
      "epoch: 5 iter: 935 loss: 0.12514 training acc: 0.96464 testing acc: 0.9576\n",
      "epoch: 5 iter: 936 loss: 0.13065 training acc: 0.96304 testing acc: 0.957\n",
      "epoch: 5 iter: 937 loss: 0.12546 training acc: 0.96490 testing acc: 0.9592\n",
      "epoch: 5 iter: 938 loss: 0.12544 training acc: 0.96478 testing acc: 0.9582\n",
      "epoch: 5 iter: 939 loss: 0.12429 training acc: 0.96512 testing acc: 0.9566\n",
      "epoch: 5 iter: 940 loss: 0.12427 training acc: 0.96488 testing acc: 0.9584\n",
      "epoch: 5 iter: 941 loss: 0.14676 training acc: 0.95542 testing acc: 0.9506\n",
      "epoch: 5 iter: 942 loss: 0.14259 training acc: 0.95704 testing acc: 0.9524\n",
      "epoch: 5 iter: 943 loss: 0.13258 training acc: 0.96182 testing acc: 0.9524\n",
      "epoch: 5 iter: 944 loss: 0.11459 training acc: 0.96728 testing acc: 0.9596\n",
      "epoch: 5 iter: 945 loss: 0.12223 training acc: 0.96434 testing acc: 0.9578\n",
      "epoch: 5 iter: 946 loss: 0.12775 training acc: 0.96270 testing acc: 0.9562\n",
      "epoch: 5 iter: 947 loss: 0.12336 training acc: 0.96460 testing acc: 0.9592\n",
      "epoch: 5 iter: 948 loss: 0.12089 training acc: 0.96492 testing acc: 0.9596\n",
      "epoch: 5 iter: 949 loss: 0.12228 training acc: 0.96586 testing acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 iter: 950 loss: 0.11841 training acc: 0.96602 testing acc: 0.9598\n",
      "epoch: 5 iter: 951 loss: 0.12619 training acc: 0.96310 testing acc: 0.9572\n",
      "epoch: 5 iter: 952 loss: 0.12980 training acc: 0.96230 testing acc: 0.9556\n",
      "epoch: 5 iter: 953 loss: 0.12391 training acc: 0.96430 testing acc: 0.9574\n",
      "epoch: 5 iter: 954 loss: 0.12020 training acc: 0.96564 testing acc: 0.9612\n",
      "epoch: 5 iter: 955 loss: 0.11830 training acc: 0.96668 testing acc: 0.9638\n",
      "epoch: 5 iter: 956 loss: 0.11782 training acc: 0.96702 testing acc: 0.9636\n",
      "epoch: 5 iter: 957 loss: 0.12041 training acc: 0.96522 testing acc: 0.961\n",
      "epoch: 5 iter: 958 loss: 0.11928 training acc: 0.96572 testing acc: 0.962\n",
      "epoch: 5 iter: 959 loss: 0.12686 training acc: 0.96340 testing acc: 0.9566\n",
      "epoch: 5 iter: 960 loss: 0.12813 training acc: 0.96312 testing acc: 0.955\n",
      "epoch: 5 iter: 961 loss: 0.11890 training acc: 0.96566 testing acc: 0.9592\n",
      "epoch: 5 iter: 962 loss: 0.11589 training acc: 0.96688 testing acc: 0.9624\n",
      "epoch: 5 iter: 963 loss: 0.12077 training acc: 0.96536 testing acc: 0.9618\n",
      "epoch: 5 iter: 964 loss: 0.11809 training acc: 0.96568 testing acc: 0.9618\n",
      "epoch: 5 iter: 965 loss: 0.11804 training acc: 0.96610 testing acc: 0.9608\n",
      "epoch: 5 iter: 966 loss: 0.12365 training acc: 0.96426 testing acc: 0.9602\n",
      "epoch: 5 iter: 967 loss: 0.11750 training acc: 0.96642 testing acc: 0.963\n",
      "epoch: 5 iter: 968 loss: 0.11719 training acc: 0.96632 testing acc: 0.962\n",
      "epoch: 5 iter: 969 loss: 0.11715 training acc: 0.96594 testing acc: 0.961\n",
      "epoch: 5 iter: 970 loss: 0.11769 training acc: 0.96588 testing acc: 0.9596\n",
      "epoch: 5 iter: 971 loss: 0.11585 training acc: 0.96640 testing acc: 0.9618\n",
      "epoch: 5 iter: 972 loss: 0.11848 training acc: 0.96576 testing acc: 0.959\n",
      "epoch: 5 iter: 973 loss: 0.12088 training acc: 0.96574 testing acc: 0.9586\n",
      "epoch: 5 iter: 974 loss: 0.12767 training acc: 0.96298 testing acc: 0.957\n",
      "epoch: 5 iter: 975 loss: 0.11778 training acc: 0.96636 testing acc: 0.9614\n",
      "epoch: 5 iter: 976 loss: 0.11656 training acc: 0.96636 testing acc: 0.9614\n",
      "epoch: 5 iter: 977 loss: 0.11576 training acc: 0.96652 testing acc: 0.9618\n",
      "epoch: 5 iter: 978 loss: 0.11592 training acc: 0.96670 testing acc: 0.9616\n",
      "epoch: 5 iter: 979 loss: 0.11540 training acc: 0.96706 testing acc: 0.9618\n",
      "epoch: 5 iter: 980 loss: 0.11486 training acc: 0.96724 testing acc: 0.9616\n",
      "epoch: 5 iter: 981 loss: 0.12009 training acc: 0.96564 testing acc: 0.9604\n",
      "epoch: 5 iter: 982 loss: 0.11526 training acc: 0.96708 testing acc: 0.9622\n",
      "epoch: 5 iter: 983 loss: 0.11613 training acc: 0.96688 testing acc: 0.9622\n",
      "epoch: 5 iter: 984 loss: 0.13733 training acc: 0.96006 testing acc: 0.9546\n",
      "epoch: 5 iter: 985 loss: 0.12534 training acc: 0.96436 testing acc: 0.9594\n",
      "epoch: 5 iter: 986 loss: 0.12691 training acc: 0.96356 testing acc: 0.9604\n",
      "epoch: 5 iter: 987 loss: 0.13012 training acc: 0.96324 testing acc: 0.959\n",
      "epoch: 5 iter: 988 loss: 0.12946 training acc: 0.96240 testing acc: 0.9562\n",
      "epoch: 5 iter: 989 loss: 0.12306 training acc: 0.96558 testing acc: 0.9612\n",
      "epoch: 5 iter: 990 loss: 0.11544 training acc: 0.96730 testing acc: 0.9612\n",
      "epoch: 5 iter: 991 loss: 0.11651 training acc: 0.96668 testing acc: 0.9592\n",
      "epoch: 5 iter: 992 loss: 0.11950 training acc: 0.96596 testing acc: 0.9578\n",
      "epoch: 5 iter: 993 loss: 0.11609 training acc: 0.96694 testing acc: 0.959\n",
      "epoch: 5 iter: 994 loss: 0.12111 training acc: 0.96482 testing acc: 0.9576\n",
      "epoch: 5 iter: 995 loss: 0.12310 training acc: 0.96442 testing acc: 0.9574\n",
      "epoch: 5 iter: 996 loss: 0.13141 training acc: 0.96194 testing acc: 0.9536\n",
      "epoch: 5 iter: 997 loss: 0.12745 training acc: 0.96182 testing acc: 0.9556\n",
      "epoch: 5 iter: 998 loss: 0.13149 training acc: 0.96028 testing acc: 0.9524\n",
      "epoch: 5 iter: 999 loss: 0.13691 training acc: 0.95896 testing acc: 0.9504\n",
      "epoch: 6 iter:   0 loss: 0.12434 training acc: 0.96310 testing acc: 0.9556\n",
      "epoch: 6 iter:   1 loss: 0.13057 training acc: 0.96094 testing acc: 0.9522\n",
      "epoch: 6 iter:   2 loss: 0.12571 training acc: 0.96294 testing acc: 0.9546\n",
      "epoch: 6 iter:   3 loss: 0.12255 training acc: 0.96452 testing acc: 0.956\n",
      "epoch: 6 iter:   4 loss: 0.12499 training acc: 0.96368 testing acc: 0.9548\n",
      "epoch: 6 iter:   5 loss: 0.11848 training acc: 0.96656 testing acc: 0.9582\n",
      "epoch: 6 iter:   6 loss: 0.12236 training acc: 0.96536 testing acc: 0.9566\n",
      "epoch: 6 iter:   7 loss: 0.12466 training acc: 0.96458 testing acc: 0.9564\n",
      "epoch: 6 iter:   8 loss: 0.12055 training acc: 0.96588 testing acc: 0.959\n",
      "epoch: 6 iter:   9 loss: 0.12351 training acc: 0.96462 testing acc: 0.9562\n",
      "epoch: 6 iter:  10 loss: 0.11866 training acc: 0.96616 testing acc: 0.9584\n",
      "epoch: 6 iter:  11 loss: 0.12514 training acc: 0.96398 testing acc: 0.955\n",
      "epoch: 6 iter:  12 loss: 0.12134 training acc: 0.96532 testing acc: 0.9572\n",
      "epoch: 6 iter:  13 loss: 0.12084 training acc: 0.96534 testing acc: 0.9582\n",
      "epoch: 6 iter:  14 loss: 0.11815 training acc: 0.96608 testing acc: 0.9588\n",
      "epoch: 6 iter:  15 loss: 0.13161 training acc: 0.96144 testing acc: 0.952\n",
      "epoch: 6 iter:  16 loss: 0.12180 training acc: 0.96456 testing acc: 0.9566\n",
      "epoch: 6 iter:  17 loss: 0.12286 training acc: 0.96462 testing acc: 0.9576\n",
      "epoch: 6 iter:  18 loss: 0.11912 training acc: 0.96540 testing acc: 0.9576\n",
      "epoch: 6 iter:  19 loss: 0.12215 training acc: 0.96438 testing acc: 0.9578\n",
      "epoch: 6 iter:  20 loss: 0.11817 training acc: 0.96600 testing acc: 0.959\n",
      "epoch: 6 iter:  21 loss: 0.12775 training acc: 0.96226 testing acc: 0.955\n",
      "epoch: 6 iter:  22 loss: 0.12281 training acc: 0.96484 testing acc: 0.9562\n",
      "epoch: 6 iter:  23 loss: 0.11675 training acc: 0.96628 testing acc: 0.9608\n",
      "epoch: 6 iter:  24 loss: 0.11845 training acc: 0.96588 testing acc: 0.9616\n",
      "epoch: 6 iter:  25 loss: 0.12445 training acc: 0.96378 testing acc: 0.9594\n",
      "epoch: 6 iter:  26 loss: 0.12051 training acc: 0.96538 testing acc: 0.9614\n",
      "epoch: 6 iter:  27 loss: 0.12015 training acc: 0.96506 testing acc: 0.9602\n",
      "epoch: 6 iter:  28 loss: 0.12168 training acc: 0.96460 testing acc: 0.9588\n",
      "epoch: 6 iter:  29 loss: 0.13971 training acc: 0.95858 testing acc: 0.9522\n",
      "epoch: 6 iter:  30 loss: 0.12714 training acc: 0.96132 testing acc: 0.9556\n",
      "epoch: 6 iter:  31 loss: 0.12215 training acc: 0.96370 testing acc: 0.9592\n",
      "epoch: 6 iter:  32 loss: 0.11855 training acc: 0.96538 testing acc: 0.9614\n",
      "epoch: 6 iter:  33 loss: 0.11642 training acc: 0.96556 testing acc: 0.961\n",
      "epoch: 6 iter:  34 loss: 0.11746 training acc: 0.96556 testing acc: 0.9608\n",
      "epoch: 6 iter:  35 loss: 0.11898 training acc: 0.96500 testing acc: 0.9598\n",
      "epoch: 6 iter:  36 loss: 0.11778 training acc: 0.96532 testing acc: 0.9612\n",
      "epoch: 6 iter:  37 loss: 0.12289 training acc: 0.96376 testing acc: 0.9578\n",
      "epoch: 6 iter:  38 loss: 0.13379 training acc: 0.96048 testing acc: 0.9526\n",
      "epoch: 6 iter:  39 loss: 0.12492 training acc: 0.96334 testing acc: 0.9566\n",
      "epoch: 6 iter:  40 loss: 0.12299 training acc: 0.96354 testing acc: 0.9574\n",
      "epoch: 6 iter:  41 loss: 0.12846 training acc: 0.96248 testing acc: 0.9562\n",
      "epoch: 6 iter:  42 loss: 0.13037 training acc: 0.96190 testing acc: 0.9564\n",
      "epoch: 6 iter:  43 loss: 0.12914 training acc: 0.96272 testing acc: 0.9566\n",
      "epoch: 6 iter:  44 loss: 0.12631 training acc: 0.96350 testing acc: 0.9574\n",
      "epoch: 6 iter:  45 loss: 0.12215 training acc: 0.96510 testing acc: 0.9586\n",
      "epoch: 6 iter:  46 loss: 0.11919 training acc: 0.96560 testing acc: 0.9594\n",
      "epoch: 6 iter:  47 loss: 0.11796 training acc: 0.96560 testing acc: 0.9596\n",
      "epoch: 6 iter:  48 loss: 0.11478 training acc: 0.96692 testing acc: 0.9626\n",
      "epoch: 6 iter:  49 loss: 0.12143 training acc: 0.96406 testing acc: 0.957\n",
      "epoch: 6 iter:  50 loss: 0.12105 training acc: 0.96436 testing acc: 0.957\n",
      "epoch: 6 iter:  51 loss: 0.11791 training acc: 0.96592 testing acc: 0.9592\n",
      "epoch: 6 iter:  52 loss: 0.11537 training acc: 0.96660 testing acc: 0.9606\n",
      "epoch: 6 iter:  53 loss: 0.11360 training acc: 0.96730 testing acc: 0.9608\n",
      "epoch: 6 iter:  54 loss: 0.11971 training acc: 0.96494 testing acc: 0.959\n",
      "epoch: 6 iter:  55 loss: 0.12365 training acc: 0.96504 testing acc: 0.9598\n",
      "epoch: 6 iter:  56 loss: 0.12307 training acc: 0.96534 testing acc: 0.9592\n",
      "epoch: 6 iter:  57 loss: 0.11944 training acc: 0.96668 testing acc: 0.9612\n",
      "epoch: 6 iter:  58 loss: 0.12010 training acc: 0.96664 testing acc: 0.961\n",
      "epoch: 6 iter:  59 loss: 0.11476 training acc: 0.96790 testing acc: 0.964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 iter:  60 loss: 0.12030 training acc: 0.96616 testing acc: 0.9616\n",
      "epoch: 6 iter:  61 loss: 0.16456 training acc: 0.94884 testing acc: 0.9434\n",
      "epoch: 6 iter:  62 loss: 0.15987 training acc: 0.95166 testing acc: 0.9458\n",
      "epoch: 6 iter:  63 loss: 0.11295 training acc: 0.96812 testing acc: 0.961\n",
      "epoch: 6 iter:  64 loss: 0.11799 training acc: 0.96610 testing acc: 0.96\n",
      "epoch: 6 iter:  65 loss: 0.11677 training acc: 0.96604 testing acc: 0.9602\n",
      "epoch: 6 iter:  66 loss: 0.11290 training acc: 0.96776 testing acc: 0.9602\n",
      "epoch: 6 iter:  67 loss: 0.11235 training acc: 0.96812 testing acc: 0.961\n",
      "epoch: 6 iter:  68 loss: 0.11145 training acc: 0.96802 testing acc: 0.961\n",
      "epoch: 6 iter:  69 loss: 0.13507 training acc: 0.96004 testing acc: 0.9516\n",
      "epoch: 6 iter:  70 loss: 0.12915 training acc: 0.96132 testing acc: 0.9538\n",
      "epoch: 6 iter:  71 loss: 0.14171 training acc: 0.95652 testing acc: 0.9486\n",
      "epoch: 6 iter:  72 loss: 0.12381 training acc: 0.96326 testing acc: 0.9584\n",
      "epoch: 6 iter:  73 loss: 0.12292 training acc: 0.96462 testing acc: 0.9558\n",
      "epoch: 6 iter:  74 loss: 0.12018 training acc: 0.96542 testing acc: 0.9582\n",
      "epoch: 6 iter:  75 loss: 0.12292 training acc: 0.96448 testing acc: 0.9576\n",
      "epoch: 6 iter:  76 loss: 0.11917 training acc: 0.96540 testing acc: 0.9558\n",
      "epoch: 6 iter:  77 loss: 0.12499 training acc: 0.96388 testing acc: 0.96\n",
      "epoch: 6 iter:  78 loss: 0.11885 training acc: 0.96586 testing acc: 0.959\n",
      "epoch: 6 iter:  79 loss: 0.11632 training acc: 0.96608 testing acc: 0.9594\n",
      "epoch: 6 iter:  80 loss: 0.11453 training acc: 0.96664 testing acc: 0.9602\n",
      "epoch: 6 iter:  81 loss: 0.11317 training acc: 0.96688 testing acc: 0.9618\n",
      "epoch: 6 iter:  82 loss: 0.11545 training acc: 0.96618 testing acc: 0.9576\n",
      "epoch: 6 iter:  83 loss: 0.11631 training acc: 0.96558 testing acc: 0.9584\n",
      "epoch: 6 iter:  84 loss: 0.11326 training acc: 0.96722 testing acc: 0.9588\n",
      "epoch: 6 iter:  85 loss: 0.11509 training acc: 0.96672 testing acc: 0.957\n",
      "epoch: 6 iter:  86 loss: 0.11636 training acc: 0.96636 testing acc: 0.9576\n",
      "epoch: 6 iter:  87 loss: 0.12816 training acc: 0.96166 testing acc: 0.9508\n",
      "epoch: 6 iter:  88 loss: 0.12032 training acc: 0.96484 testing acc: 0.9538\n",
      "epoch: 6 iter:  89 loss: 0.12242 training acc: 0.96372 testing acc: 0.9556\n",
      "epoch: 6 iter:  90 loss: 0.13049 training acc: 0.96226 testing acc: 0.9552\n",
      "epoch: 6 iter:  91 loss: 0.12674 training acc: 0.96330 testing acc: 0.9564\n",
      "epoch: 6 iter:  92 loss: 0.11483 training acc: 0.96710 testing acc: 0.9592\n",
      "epoch: 6 iter:  93 loss: 0.12575 training acc: 0.96438 testing acc: 0.9534\n",
      "epoch: 6 iter:  94 loss: 0.12714 training acc: 0.96292 testing acc: 0.9544\n",
      "epoch: 6 iter:  95 loss: 0.12571 training acc: 0.96330 testing acc: 0.955\n",
      "epoch: 6 iter:  96 loss: 0.13447 training acc: 0.96022 testing acc: 0.9526\n",
      "epoch: 6 iter:  97 loss: 0.13337 training acc: 0.95938 testing acc: 0.9492\n",
      "epoch: 6 iter:  98 loss: 0.12922 training acc: 0.96142 testing acc: 0.9502\n",
      "epoch: 6 iter:  99 loss: 0.14309 training acc: 0.95746 testing acc: 0.9466\n",
      "epoch: 6 iter: 100 loss: 0.13240 training acc: 0.96112 testing acc: 0.95\n",
      "epoch: 6 iter: 101 loss: 0.13098 training acc: 0.96154 testing acc: 0.9546\n",
      "epoch: 6 iter: 102 loss: 0.12447 training acc: 0.96402 testing acc: 0.9576\n",
      "epoch: 6 iter: 103 loss: 0.12105 training acc: 0.96526 testing acc: 0.9554\n",
      "epoch: 6 iter: 104 loss: 0.12463 training acc: 0.96420 testing acc: 0.9548\n",
      "epoch: 6 iter: 105 loss: 0.12675 training acc: 0.96246 testing acc: 0.9528\n",
      "epoch: 6 iter: 106 loss: 0.12243 training acc: 0.96362 testing acc: 0.9538\n",
      "epoch: 6 iter: 107 loss: 0.12198 training acc: 0.96414 testing acc: 0.9542\n",
      "epoch: 6 iter: 108 loss: 0.12075 training acc: 0.96432 testing acc: 0.9568\n",
      "epoch: 6 iter: 109 loss: 0.12406 training acc: 0.96312 testing acc: 0.955\n",
      "epoch: 6 iter: 110 loss: 0.12099 training acc: 0.96382 testing acc: 0.957\n",
      "epoch: 6 iter: 111 loss: 0.12016 training acc: 0.96352 testing acc: 0.9578\n",
      "epoch: 6 iter: 112 loss: 0.12162 training acc: 0.96310 testing acc: 0.9538\n",
      "epoch: 6 iter: 113 loss: 0.12377 training acc: 0.96216 testing acc: 0.9554\n",
      "epoch: 6 iter: 114 loss: 0.12617 training acc: 0.96120 testing acc: 0.954\n",
      "epoch: 6 iter: 115 loss: 0.12891 training acc: 0.96038 testing acc: 0.953\n",
      "epoch: 6 iter: 116 loss: 0.13600 training acc: 0.95778 testing acc: 0.9468\n",
      "epoch: 6 iter: 117 loss: 0.12352 training acc: 0.96228 testing acc: 0.955\n",
      "epoch: 6 iter: 118 loss: 0.12966 training acc: 0.95964 testing acc: 0.95\n",
      "epoch: 6 iter: 119 loss: 0.13868 training acc: 0.95770 testing acc: 0.9482\n",
      "epoch: 6 iter: 120 loss: 0.11418 training acc: 0.96718 testing acc: 0.9564\n",
      "epoch: 6 iter: 121 loss: 0.12890 training acc: 0.96100 testing acc: 0.9508\n",
      "epoch: 6 iter: 122 loss: 0.11852 training acc: 0.96394 testing acc: 0.9574\n",
      "epoch: 6 iter: 123 loss: 0.13483 training acc: 0.95866 testing acc: 0.951\n",
      "epoch: 6 iter: 124 loss: 0.12731 training acc: 0.96090 testing acc: 0.9542\n",
      "epoch: 6 iter: 125 loss: 0.11385 training acc: 0.96714 testing acc: 0.9606\n",
      "epoch: 6 iter: 126 loss: 0.12307 training acc: 0.96472 testing acc: 0.9564\n",
      "epoch: 6 iter: 127 loss: 0.12044 training acc: 0.96576 testing acc: 0.9578\n",
      "epoch: 6 iter: 128 loss: 0.11689 training acc: 0.96672 testing acc: 0.9614\n",
      "epoch: 6 iter: 129 loss: 0.11707 training acc: 0.96622 testing acc: 0.9604\n",
      "epoch: 6 iter: 130 loss: 0.11292 training acc: 0.96772 testing acc: 0.963\n",
      "epoch: 6 iter: 131 loss: 0.11424 training acc: 0.96636 testing acc: 0.9616\n",
      "epoch: 6 iter: 132 loss: 0.11693 training acc: 0.96476 testing acc: 0.9604\n",
      "epoch: 6 iter: 133 loss: 0.12097 training acc: 0.96352 testing acc: 0.9598\n",
      "epoch: 6 iter: 134 loss: 0.13420 training acc: 0.95838 testing acc: 0.9518\n",
      "epoch: 6 iter: 135 loss: 0.11711 training acc: 0.96488 testing acc: 0.957\n",
      "epoch: 6 iter: 136 loss: 0.15616 training acc: 0.95186 testing acc: 0.9456\n",
      "epoch: 6 iter: 137 loss: 0.13286 training acc: 0.95996 testing acc: 0.953\n",
      "epoch: 6 iter: 138 loss: 0.12161 training acc: 0.96340 testing acc: 0.9576\n",
      "epoch: 6 iter: 139 loss: 0.13307 training acc: 0.96126 testing acc: 0.957\n",
      "epoch: 6 iter: 140 loss: 0.12359 training acc: 0.96486 testing acc: 0.9602\n",
      "epoch: 6 iter: 141 loss: 0.11645 training acc: 0.96660 testing acc: 0.9614\n",
      "epoch: 6 iter: 142 loss: 0.11395 training acc: 0.96710 testing acc: 0.9614\n",
      "epoch: 6 iter: 143 loss: 0.11641 training acc: 0.96618 testing acc: 0.9576\n",
      "epoch: 6 iter: 144 loss: 0.11421 training acc: 0.96704 testing acc: 0.9604\n",
      "epoch: 6 iter: 145 loss: 0.11339 training acc: 0.96716 testing acc: 0.9604\n",
      "epoch: 6 iter: 146 loss: 0.11578 training acc: 0.96658 testing acc: 0.9604\n",
      "epoch: 6 iter: 147 loss: 0.11419 training acc: 0.96698 testing acc: 0.961\n",
      "epoch: 6 iter: 148 loss: 0.11734 training acc: 0.96554 testing acc: 0.9614\n",
      "epoch: 6 iter: 149 loss: 0.13392 training acc: 0.96092 testing acc: 0.9536\n",
      "epoch: 6 iter: 150 loss: 0.13222 training acc: 0.96140 testing acc: 0.9566\n",
      "epoch: 6 iter: 151 loss: 0.13029 training acc: 0.96232 testing acc: 0.9572\n",
      "epoch: 6 iter: 152 loss: 0.11869 training acc: 0.96656 testing acc: 0.9618\n",
      "epoch: 6 iter: 153 loss: 0.11732 training acc: 0.96650 testing acc: 0.9626\n",
      "epoch: 6 iter: 154 loss: 0.11665 training acc: 0.96642 testing acc: 0.9606\n",
      "epoch: 6 iter: 155 loss: 0.12218 training acc: 0.96608 testing acc: 0.9602\n",
      "epoch: 6 iter: 156 loss: 0.13847 training acc: 0.95914 testing acc: 0.953\n",
      "epoch: 6 iter: 157 loss: 0.13161 training acc: 0.96132 testing acc: 0.9574\n",
      "epoch: 6 iter: 158 loss: 0.13257 training acc: 0.96118 testing acc: 0.9568\n",
      "epoch: 6 iter: 159 loss: 0.13427 training acc: 0.96006 testing acc: 0.955\n",
      "epoch: 6 iter: 160 loss: 0.13738 training acc: 0.95846 testing acc: 0.9518\n",
      "epoch: 6 iter: 161 loss: 0.13401 training acc: 0.96002 testing acc: 0.953\n",
      "epoch: 6 iter: 162 loss: 0.12231 training acc: 0.96446 testing acc: 0.9564\n",
      "epoch: 6 iter: 163 loss: 0.11538 training acc: 0.96708 testing acc: 0.9618\n",
      "epoch: 6 iter: 164 loss: 0.11248 training acc: 0.96796 testing acc: 0.963\n",
      "epoch: 6 iter: 165 loss: 0.11362 training acc: 0.96718 testing acc: 0.9616\n",
      "epoch: 6 iter: 166 loss: 0.11389 training acc: 0.96700 testing acc: 0.9616\n",
      "epoch: 6 iter: 167 loss: 0.11897 training acc: 0.96600 testing acc: 0.9596\n",
      "epoch: 6 iter: 168 loss: 0.15002 training acc: 0.95694 testing acc: 0.9516\n",
      "epoch: 6 iter: 169 loss: 0.12509 training acc: 0.96396 testing acc: 0.9594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 iter: 170 loss: 0.12070 training acc: 0.96456 testing acc: 0.9574\n",
      "epoch: 6 iter: 171 loss: 0.11905 training acc: 0.96440 testing acc: 0.9584\n",
      "epoch: 6 iter: 172 loss: 0.12349 training acc: 0.96332 testing acc: 0.958\n",
      "epoch: 6 iter: 173 loss: 0.11496 training acc: 0.96634 testing acc: 0.9614\n",
      "epoch: 6 iter: 174 loss: 0.11224 training acc: 0.96722 testing acc: 0.961\n",
      "epoch: 6 iter: 175 loss: 0.11743 training acc: 0.96542 testing acc: 0.9584\n",
      "epoch: 6 iter: 176 loss: 0.11966 training acc: 0.96524 testing acc: 0.958\n",
      "epoch: 6 iter: 177 loss: 0.11188 training acc: 0.96790 testing acc: 0.9648\n",
      "epoch: 6 iter: 178 loss: 0.11394 training acc: 0.96712 testing acc: 0.9622\n",
      "epoch: 6 iter: 179 loss: 0.12375 training acc: 0.96382 testing acc: 0.9582\n",
      "epoch: 6 iter: 180 loss: 0.12273 training acc: 0.96406 testing acc: 0.9586\n",
      "epoch: 6 iter: 181 loss: 0.12354 training acc: 0.96334 testing acc: 0.9584\n",
      "epoch: 6 iter: 182 loss: 0.11734 training acc: 0.96594 testing acc: 0.9608\n",
      "epoch: 6 iter: 183 loss: 0.11993 training acc: 0.96500 testing acc: 0.9606\n",
      "epoch: 6 iter: 184 loss: 0.11422 training acc: 0.96738 testing acc: 0.9616\n",
      "epoch: 6 iter: 185 loss: 0.11444 training acc: 0.96740 testing acc: 0.9622\n",
      "epoch: 6 iter: 186 loss: 0.11948 training acc: 0.96496 testing acc: 0.961\n",
      "epoch: 6 iter: 187 loss: 0.11727 training acc: 0.96602 testing acc: 0.9612\n",
      "epoch: 6 iter: 188 loss: 0.12368 training acc: 0.96460 testing acc: 0.9584\n",
      "epoch: 6 iter: 189 loss: 0.12164 training acc: 0.96538 testing acc: 0.9572\n",
      "epoch: 6 iter: 190 loss: 0.12269 training acc: 0.96524 testing acc: 0.958\n",
      "epoch: 6 iter: 191 loss: 0.13056 training acc: 0.96158 testing acc: 0.9548\n",
      "epoch: 6 iter: 192 loss: 0.12778 training acc: 0.96230 testing acc: 0.9548\n",
      "epoch: 6 iter: 193 loss: 0.12547 training acc: 0.96310 testing acc: 0.9564\n",
      "epoch: 6 iter: 194 loss: 0.12148 training acc: 0.96440 testing acc: 0.9576\n",
      "epoch: 6 iter: 195 loss: 0.13552 training acc: 0.95948 testing acc: 0.954\n",
      "epoch: 6 iter: 196 loss: 0.12448 training acc: 0.96438 testing acc: 0.9574\n",
      "epoch: 6 iter: 197 loss: 0.12268 training acc: 0.96424 testing acc: 0.9584\n",
      "epoch: 6 iter: 198 loss: 0.12390 training acc: 0.96416 testing acc: 0.9582\n",
      "epoch: 6 iter: 199 loss: 0.11472 training acc: 0.96660 testing acc: 0.96\n",
      "epoch: 6 iter: 200 loss: 0.11878 training acc: 0.96536 testing acc: 0.9592\n",
      "epoch: 6 iter: 201 loss: 0.11939 training acc: 0.96502 testing acc: 0.9576\n",
      "epoch: 6 iter: 202 loss: 0.12851 training acc: 0.96216 testing acc: 0.9568\n",
      "epoch: 6 iter: 203 loss: 0.11703 training acc: 0.96604 testing acc: 0.9608\n",
      "epoch: 6 iter: 204 loss: 0.12596 training acc: 0.96332 testing acc: 0.957\n",
      "epoch: 6 iter: 205 loss: 0.15693 training acc: 0.95320 testing acc: 0.9492\n",
      "epoch: 6 iter: 206 loss: 0.12377 training acc: 0.96492 testing acc: 0.9586\n",
      "epoch: 6 iter: 207 loss: 0.12291 training acc: 0.96456 testing acc: 0.9574\n",
      "epoch: 6 iter: 208 loss: 0.11968 training acc: 0.96552 testing acc: 0.9572\n",
      "epoch: 6 iter: 209 loss: 0.11846 training acc: 0.96642 testing acc: 0.9566\n",
      "epoch: 6 iter: 210 loss: 0.12183 training acc: 0.96402 testing acc: 0.9562\n",
      "epoch: 6 iter: 211 loss: 0.12102 training acc: 0.96516 testing acc: 0.956\n",
      "epoch: 6 iter: 212 loss: 0.13255 training acc: 0.96180 testing acc: 0.9544\n",
      "epoch: 6 iter: 213 loss: 0.13298 training acc: 0.96114 testing acc: 0.9552\n",
      "epoch: 6 iter: 214 loss: 0.12761 training acc: 0.96246 testing acc: 0.955\n",
      "epoch: 6 iter: 215 loss: 0.12751 training acc: 0.96232 testing acc: 0.955\n",
      "epoch: 6 iter: 216 loss: 0.12988 training acc: 0.96184 testing acc: 0.9532\n",
      "epoch: 6 iter: 217 loss: 0.12727 training acc: 0.96282 testing acc: 0.956\n",
      "epoch: 6 iter: 218 loss: 0.12139 training acc: 0.96472 testing acc: 0.9592\n",
      "epoch: 6 iter: 219 loss: 0.11312 training acc: 0.96750 testing acc: 0.9592\n",
      "epoch: 6 iter: 220 loss: 0.11770 training acc: 0.96566 testing acc: 0.9566\n",
      "epoch: 6 iter: 221 loss: 0.11502 training acc: 0.96694 testing acc: 0.9602\n",
      "epoch: 6 iter: 222 loss: 0.11299 training acc: 0.96752 testing acc: 0.9604\n",
      "epoch: 6 iter: 223 loss: 0.11127 training acc: 0.96772 testing acc: 0.96\n",
      "epoch: 6 iter: 224 loss: 0.11572 training acc: 0.96612 testing acc: 0.9582\n",
      "epoch: 6 iter: 225 loss: 0.11317 training acc: 0.96718 testing acc: 0.9596\n",
      "epoch: 6 iter: 226 loss: 0.11370 training acc: 0.96700 testing acc: 0.9592\n",
      "epoch: 6 iter: 227 loss: 0.11718 training acc: 0.96534 testing acc: 0.9592\n",
      "epoch: 6 iter: 228 loss: 0.11644 training acc: 0.96592 testing acc: 0.9594\n",
      "epoch: 6 iter: 229 loss: 0.12640 training acc: 0.96264 testing acc: 0.9552\n",
      "epoch: 6 iter: 230 loss: 0.11616 training acc: 0.96664 testing acc: 0.9594\n",
      "epoch: 6 iter: 231 loss: 0.12889 training acc: 0.96262 testing acc: 0.953\n",
      "epoch: 6 iter: 232 loss: 0.12215 training acc: 0.96330 testing acc: 0.9562\n",
      "epoch: 6 iter: 233 loss: 0.12708 training acc: 0.96170 testing acc: 0.9528\n",
      "epoch: 6 iter: 234 loss: 0.13702 training acc: 0.95718 testing acc: 0.9516\n",
      "epoch: 6 iter: 235 loss: 0.11448 training acc: 0.96688 testing acc: 0.96\n",
      "epoch: 6 iter: 236 loss: 0.11241 training acc: 0.96788 testing acc: 0.9618\n",
      "epoch: 6 iter: 237 loss: 0.11337 training acc: 0.96718 testing acc: 0.9614\n",
      "epoch: 6 iter: 238 loss: 0.11095 training acc: 0.96814 testing acc: 0.963\n",
      "epoch: 6 iter: 239 loss: 0.11249 training acc: 0.96814 testing acc: 0.9614\n",
      "epoch: 6 iter: 240 loss: 0.11197 training acc: 0.96800 testing acc: 0.9616\n",
      "epoch: 6 iter: 241 loss: 0.11862 training acc: 0.96562 testing acc: 0.959\n",
      "epoch: 6 iter: 242 loss: 0.12079 training acc: 0.96536 testing acc: 0.959\n",
      "epoch: 6 iter: 243 loss: 0.11587 training acc: 0.96738 testing acc: 0.9622\n",
      "epoch: 6 iter: 244 loss: 0.13450 training acc: 0.96024 testing acc: 0.956\n",
      "epoch: 6 iter: 245 loss: 0.11961 training acc: 0.96578 testing acc: 0.9604\n",
      "epoch: 6 iter: 246 loss: 0.12333 training acc: 0.96446 testing acc: 0.958\n",
      "epoch: 6 iter: 247 loss: 0.11970 training acc: 0.96570 testing acc: 0.957\n",
      "epoch: 6 iter: 248 loss: 0.12935 training acc: 0.96234 testing acc: 0.9548\n",
      "epoch: 6 iter: 249 loss: 0.12221 training acc: 0.96434 testing acc: 0.9586\n",
      "epoch: 6 iter: 250 loss: 0.12059 training acc: 0.96540 testing acc: 0.9604\n",
      "epoch: 6 iter: 251 loss: 0.11892 training acc: 0.96562 testing acc: 0.96\n",
      "epoch: 6 iter: 252 loss: 0.11296 training acc: 0.96742 testing acc: 0.9602\n",
      "epoch: 6 iter: 253 loss: 0.12008 training acc: 0.96510 testing acc: 0.9616\n",
      "epoch: 6 iter: 254 loss: 0.11674 training acc: 0.96606 testing acc: 0.9598\n",
      "epoch: 6 iter: 255 loss: 0.11026 training acc: 0.96860 testing acc: 0.9614\n",
      "epoch: 6 iter: 256 loss: 0.11407 training acc: 0.96748 testing acc: 0.9596\n",
      "epoch: 6 iter: 257 loss: 0.11031 training acc: 0.96948 testing acc: 0.9608\n",
      "epoch: 6 iter: 258 loss: 0.10949 training acc: 0.96898 testing acc: 0.9622\n",
      "epoch: 6 iter: 259 loss: 0.10818 training acc: 0.96904 testing acc: 0.9624\n",
      "epoch: 6 iter: 260 loss: 0.10910 training acc: 0.96814 testing acc: 0.9606\n",
      "epoch: 6 iter: 261 loss: 0.11261 training acc: 0.96770 testing acc: 0.9588\n",
      "epoch: 6 iter: 262 loss: 0.11370 training acc: 0.96678 testing acc: 0.9562\n",
      "epoch: 6 iter: 263 loss: 0.11399 training acc: 0.96700 testing acc: 0.9576\n",
      "epoch: 6 iter: 264 loss: 0.11362 training acc: 0.96716 testing acc: 0.9586\n",
      "epoch: 6 iter: 265 loss: 0.12002 training acc: 0.96456 testing acc: 0.9568\n",
      "epoch: 6 iter: 266 loss: 0.12353 training acc: 0.96456 testing acc: 0.9554\n",
      "epoch: 6 iter: 267 loss: 0.11708 training acc: 0.96588 testing acc: 0.9574\n",
      "epoch: 6 iter: 268 loss: 0.13756 training acc: 0.95684 testing acc: 0.9494\n",
      "epoch: 6 iter: 269 loss: 0.12912 training acc: 0.96004 testing acc: 0.9534\n",
      "epoch: 6 iter: 270 loss: 0.12091 training acc: 0.96340 testing acc: 0.9574\n",
      "epoch: 6 iter: 271 loss: 0.11492 training acc: 0.96650 testing acc: 0.9584\n",
      "epoch: 6 iter: 272 loss: 0.11351 training acc: 0.96704 testing acc: 0.9588\n",
      "epoch: 6 iter: 273 loss: 0.12743 training acc: 0.96266 testing acc: 0.953\n",
      "epoch: 6 iter: 274 loss: 0.12273 training acc: 0.96420 testing acc: 0.956\n",
      "epoch: 6 iter: 275 loss: 0.11449 training acc: 0.96668 testing acc: 0.9598\n",
      "epoch: 6 iter: 276 loss: 0.11018 training acc: 0.96814 testing acc: 0.9622\n",
      "epoch: 6 iter: 277 loss: 0.11210 training acc: 0.96810 testing acc: 0.9624\n",
      "epoch: 6 iter: 278 loss: 0.11569 training acc: 0.96670 testing acc: 0.9596\n",
      "epoch: 6 iter: 279 loss: 0.11105 training acc: 0.96832 testing acc: 0.9614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 iter: 280 loss: 0.11772 training acc: 0.96598 testing acc: 0.959\n",
      "epoch: 6 iter: 281 loss: 0.11390 training acc: 0.96788 testing acc: 0.9624\n",
      "epoch: 6 iter: 282 loss: 0.11857 training acc: 0.96642 testing acc: 0.9582\n",
      "epoch: 6 iter: 283 loss: 0.11247 training acc: 0.96836 testing acc: 0.9624\n",
      "epoch: 6 iter: 284 loss: 0.11552 training acc: 0.96700 testing acc: 0.9602\n",
      "epoch: 6 iter: 285 loss: 0.11435 training acc: 0.96734 testing acc: 0.9582\n",
      "epoch: 6 iter: 286 loss: 0.11205 training acc: 0.96694 testing acc: 0.9608\n",
      "epoch: 6 iter: 287 loss: 0.11276 training acc: 0.96724 testing acc: 0.961\n",
      "epoch: 6 iter: 288 loss: 0.11141 training acc: 0.96740 testing acc: 0.9604\n",
      "epoch: 6 iter: 289 loss: 0.11545 training acc: 0.96608 testing acc: 0.9598\n",
      "epoch: 6 iter: 290 loss: 0.11661 training acc: 0.96602 testing acc: 0.9584\n",
      "epoch: 6 iter: 291 loss: 0.12276 training acc: 0.96378 testing acc: 0.9588\n",
      "epoch: 6 iter: 292 loss: 0.12061 training acc: 0.96500 testing acc: 0.9564\n",
      "epoch: 6 iter: 293 loss: 0.11299 training acc: 0.96754 testing acc: 0.9626\n",
      "epoch: 6 iter: 294 loss: 0.11180 training acc: 0.96776 testing acc: 0.963\n",
      "epoch: 6 iter: 295 loss: 0.11321 training acc: 0.96726 testing acc: 0.961\n",
      "epoch: 6 iter: 296 loss: 0.11378 training acc: 0.96692 testing acc: 0.961\n",
      "epoch: 6 iter: 297 loss: 0.11672 training acc: 0.96602 testing acc: 0.9612\n",
      "epoch: 6 iter: 298 loss: 0.11882 training acc: 0.96536 testing acc: 0.9602\n",
      "epoch: 6 iter: 299 loss: 0.10920 training acc: 0.96896 testing acc: 0.9626\n",
      "epoch: 6 iter: 300 loss: 0.12025 training acc: 0.96580 testing acc: 0.9608\n",
      "epoch: 6 iter: 301 loss: 0.10818 training acc: 0.96990 testing acc: 0.9618\n",
      "epoch: 6 iter: 302 loss: 0.11054 training acc: 0.96874 testing acc: 0.9614\n",
      "epoch: 6 iter: 303 loss: 0.10886 training acc: 0.96864 testing acc: 0.9602\n",
      "epoch: 6 iter: 304 loss: 0.11488 training acc: 0.96634 testing acc: 0.958\n",
      "epoch: 6 iter: 305 loss: 0.11077 training acc: 0.96776 testing acc: 0.96\n",
      "epoch: 6 iter: 306 loss: 0.11043 training acc: 0.96740 testing acc: 0.9604\n",
      "epoch: 6 iter: 307 loss: 0.11318 training acc: 0.96676 testing acc: 0.959\n",
      "epoch: 6 iter: 308 loss: 0.11563 training acc: 0.96554 testing acc: 0.9578\n",
      "epoch: 6 iter: 309 loss: 0.11199 training acc: 0.96678 testing acc: 0.9588\n",
      "epoch: 6 iter: 310 loss: 0.11502 training acc: 0.96620 testing acc: 0.9586\n",
      "epoch: 6 iter: 311 loss: 0.10958 training acc: 0.96838 testing acc: 0.9594\n",
      "epoch: 6 iter: 312 loss: 0.11023 training acc: 0.96860 testing acc: 0.9584\n",
      "epoch: 6 iter: 313 loss: 0.11007 training acc: 0.96828 testing acc: 0.9582\n",
      "epoch: 6 iter: 314 loss: 0.11543 training acc: 0.96604 testing acc: 0.9604\n",
      "epoch: 6 iter: 315 loss: 0.11416 training acc: 0.96582 testing acc: 0.96\n",
      "epoch: 6 iter: 316 loss: 0.12900 training acc: 0.96162 testing acc: 0.954\n",
      "epoch: 6 iter: 317 loss: 0.11410 training acc: 0.96730 testing acc: 0.9576\n",
      "epoch: 6 iter: 318 loss: 0.10478 training acc: 0.97036 testing acc: 0.9628\n",
      "epoch: 6 iter: 319 loss: 0.10752 training acc: 0.96888 testing acc: 0.964\n",
      "epoch: 6 iter: 320 loss: 0.10719 training acc: 0.96866 testing acc: 0.9642\n",
      "epoch: 6 iter: 321 loss: 0.10892 training acc: 0.96858 testing acc: 0.9654\n",
      "epoch: 6 iter: 322 loss: 0.11340 training acc: 0.96698 testing acc: 0.9628\n",
      "epoch: 6 iter: 323 loss: 0.10782 training acc: 0.96896 testing acc: 0.9652\n",
      "epoch: 6 iter: 324 loss: 0.11057 training acc: 0.96792 testing acc: 0.9632\n",
      "epoch: 6 iter: 325 loss: 0.11122 training acc: 0.96828 testing acc: 0.9614\n",
      "epoch: 6 iter: 326 loss: 0.11065 training acc: 0.96808 testing acc: 0.9624\n",
      "epoch: 6 iter: 327 loss: 0.11156 training acc: 0.96794 testing acc: 0.963\n",
      "epoch: 6 iter: 328 loss: 0.11166 training acc: 0.96766 testing acc: 0.9626\n",
      "epoch: 6 iter: 329 loss: 0.11030 training acc: 0.96812 testing acc: 0.963\n",
      "epoch: 6 iter: 330 loss: 0.10883 training acc: 0.96868 testing acc: 0.9636\n",
      "epoch: 6 iter: 331 loss: 0.10767 training acc: 0.96922 testing acc: 0.9628\n",
      "epoch: 6 iter: 332 loss: 0.11052 training acc: 0.96830 testing acc: 0.9628\n",
      "epoch: 6 iter: 333 loss: 0.11159 training acc: 0.96722 testing acc: 0.962\n",
      "epoch: 6 iter: 334 loss: 0.10932 training acc: 0.96818 testing acc: 0.9624\n",
      "epoch: 6 iter: 335 loss: 0.11014 training acc: 0.96790 testing acc: 0.9612\n",
      "epoch: 6 iter: 336 loss: 0.11275 training acc: 0.96672 testing acc: 0.9628\n",
      "epoch: 6 iter: 337 loss: 0.12296 training acc: 0.96364 testing acc: 0.9574\n",
      "epoch: 6 iter: 338 loss: 0.11554 training acc: 0.96674 testing acc: 0.961\n",
      "epoch: 6 iter: 339 loss: 0.12543 training acc: 0.96278 testing acc: 0.9532\n",
      "epoch: 6 iter: 340 loss: 0.11644 training acc: 0.96420 testing acc: 0.9574\n",
      "epoch: 6 iter: 341 loss: 0.11461 training acc: 0.96542 testing acc: 0.9568\n",
      "epoch: 6 iter: 342 loss: 0.11293 training acc: 0.96568 testing acc: 0.958\n",
      "epoch: 6 iter: 343 loss: 0.10582 training acc: 0.96856 testing acc: 0.9618\n",
      "epoch: 6 iter: 344 loss: 0.10996 training acc: 0.96688 testing acc: 0.9612\n",
      "epoch: 6 iter: 345 loss: 0.12693 training acc: 0.96150 testing acc: 0.9554\n",
      "epoch: 6 iter: 346 loss: 0.11117 training acc: 0.96716 testing acc: 0.9588\n",
      "epoch: 6 iter: 347 loss: 0.10807 training acc: 0.96800 testing acc: 0.9628\n",
      "epoch: 6 iter: 348 loss: 0.11742 training acc: 0.96526 testing acc: 0.958\n",
      "epoch: 6 iter: 349 loss: 0.12353 training acc: 0.96316 testing acc: 0.9578\n",
      "epoch: 6 iter: 350 loss: 0.12197 training acc: 0.96354 testing acc: 0.9574\n",
      "epoch: 6 iter: 351 loss: 0.11775 training acc: 0.96508 testing acc: 0.958\n",
      "epoch: 6 iter: 352 loss: 0.11588 training acc: 0.96618 testing acc: 0.961\n",
      "epoch: 6 iter: 353 loss: 0.11796 training acc: 0.96522 testing acc: 0.9594\n",
      "epoch: 6 iter: 354 loss: 0.12318 training acc: 0.96320 testing acc: 0.9568\n",
      "epoch: 6 iter: 355 loss: 0.11314 training acc: 0.96738 testing acc: 0.9604\n",
      "epoch: 6 iter: 356 loss: 0.11294 training acc: 0.96748 testing acc: 0.9608\n",
      "epoch: 6 iter: 357 loss: 0.10907 training acc: 0.96848 testing acc: 0.961\n",
      "epoch: 6 iter: 358 loss: 0.11151 training acc: 0.96728 testing acc: 0.963\n",
      "epoch: 6 iter: 359 loss: 0.11385 training acc: 0.96662 testing acc: 0.9638\n",
      "epoch: 6 iter: 360 loss: 0.11987 training acc: 0.96384 testing acc: 0.9602\n",
      "epoch: 6 iter: 361 loss: 0.11766 training acc: 0.96466 testing acc: 0.9588\n",
      "epoch: 6 iter: 362 loss: 0.11694 training acc: 0.96618 testing acc: 0.9602\n",
      "epoch: 6 iter: 363 loss: 0.11530 training acc: 0.96674 testing acc: 0.9602\n",
      "epoch: 6 iter: 364 loss: 0.11400 training acc: 0.96692 testing acc: 0.96\n",
      "epoch: 6 iter: 365 loss: 0.11951 training acc: 0.96512 testing acc: 0.958\n",
      "epoch: 6 iter: 366 loss: 0.11183 training acc: 0.96718 testing acc: 0.9614\n",
      "epoch: 6 iter: 367 loss: 0.10924 training acc: 0.96822 testing acc: 0.9616\n",
      "epoch: 6 iter: 368 loss: 0.11024 training acc: 0.96760 testing acc: 0.9636\n",
      "epoch: 6 iter: 369 loss: 0.12747 training acc: 0.96172 testing acc: 0.9564\n",
      "epoch: 6 iter: 370 loss: 0.11654 training acc: 0.96464 testing acc: 0.959\n",
      "epoch: 6 iter: 371 loss: 0.11110 training acc: 0.96784 testing acc: 0.9632\n",
      "epoch: 6 iter: 372 loss: 0.11626 training acc: 0.96614 testing acc: 0.9604\n",
      "epoch: 6 iter: 373 loss: 0.11223 training acc: 0.96634 testing acc: 0.9622\n",
      "epoch: 6 iter: 374 loss: 0.11609 training acc: 0.96540 testing acc: 0.9594\n",
      "epoch: 6 iter: 375 loss: 0.11425 training acc: 0.96676 testing acc: 0.9582\n",
      "epoch: 6 iter: 376 loss: 0.10951 training acc: 0.96880 testing acc: 0.9596\n",
      "epoch: 6 iter: 377 loss: 0.10980 training acc: 0.96832 testing acc: 0.9612\n",
      "epoch: 6 iter: 378 loss: 0.10801 training acc: 0.97014 testing acc: 0.9628\n",
      "epoch: 6 iter: 379 loss: 0.10989 training acc: 0.96956 testing acc: 0.9604\n",
      "epoch: 6 iter: 380 loss: 0.11145 training acc: 0.96786 testing acc: 0.9612\n",
      "epoch: 6 iter: 381 loss: 0.11671 training acc: 0.96606 testing acc: 0.9594\n",
      "epoch: 6 iter: 382 loss: 0.11519 training acc: 0.96686 testing acc: 0.9622\n",
      "epoch: 6 iter: 383 loss: 0.11314 training acc: 0.96736 testing acc: 0.963\n",
      "epoch: 6 iter: 384 loss: 0.11528 training acc: 0.96604 testing acc: 0.961\n",
      "epoch: 6 iter: 385 loss: 0.11025 training acc: 0.96834 testing acc: 0.9616\n",
      "epoch: 6 iter: 386 loss: 0.10724 training acc: 0.96872 testing acc: 0.9638\n",
      "epoch: 6 iter: 387 loss: 0.10660 training acc: 0.96902 testing acc: 0.9638\n",
      "epoch: 6 iter: 388 loss: 0.10733 training acc: 0.96938 testing acc: 0.9624\n",
      "epoch: 6 iter: 389 loss: 0.11289 training acc: 0.96700 testing acc: 0.9616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 iter: 390 loss: 0.12520 training acc: 0.96324 testing acc: 0.9544\n",
      "epoch: 6 iter: 391 loss: 0.11170 training acc: 0.96778 testing acc: 0.958\n",
      "epoch: 6 iter: 392 loss: 0.11898 training acc: 0.96482 testing acc: 0.957\n",
      "epoch: 6 iter: 393 loss: 0.11935 training acc: 0.96442 testing acc: 0.9568\n",
      "epoch: 6 iter: 394 loss: 0.11046 training acc: 0.96802 testing acc: 0.96\n",
      "epoch: 6 iter: 395 loss: 0.11726 training acc: 0.96508 testing acc: 0.954\n",
      "epoch: 6 iter: 396 loss: 0.12011 training acc: 0.96424 testing acc: 0.951\n",
      "epoch: 6 iter: 397 loss: 0.11476 training acc: 0.96634 testing acc: 0.9574\n",
      "epoch: 6 iter: 398 loss: 0.11119 training acc: 0.96786 testing acc: 0.9586\n",
      "epoch: 6 iter: 399 loss: 0.11343 training acc: 0.96668 testing acc: 0.9582\n",
      "epoch: 6 iter: 400 loss: 0.11951 training acc: 0.96474 testing acc: 0.9556\n",
      "epoch: 6 iter: 401 loss: 0.10633 training acc: 0.96892 testing acc: 0.9604\n",
      "epoch: 6 iter: 402 loss: 0.12032 training acc: 0.96410 testing acc: 0.9536\n",
      "epoch: 6 iter: 403 loss: 0.12265 training acc: 0.96362 testing acc: 0.958\n",
      "epoch: 6 iter: 404 loss: 0.11378 training acc: 0.96670 testing acc: 0.9596\n",
      "epoch: 6 iter: 405 loss: 0.10697 training acc: 0.96914 testing acc: 0.9616\n",
      "epoch: 6 iter: 406 loss: 0.11064 training acc: 0.96744 testing acc: 0.9608\n",
      "epoch: 6 iter: 407 loss: 0.11258 training acc: 0.96660 testing acc: 0.9562\n",
      "epoch: 6 iter: 408 loss: 0.11360 training acc: 0.96610 testing acc: 0.9548\n",
      "epoch: 6 iter: 409 loss: 0.11641 training acc: 0.96454 testing acc: 0.9556\n",
      "epoch: 6 iter: 410 loss: 0.12754 training acc: 0.96064 testing acc: 0.9538\n",
      "epoch: 6 iter: 411 loss: 0.11853 training acc: 0.96394 testing acc: 0.9572\n",
      "epoch: 6 iter: 412 loss: 0.11534 training acc: 0.96512 testing acc: 0.9588\n",
      "epoch: 6 iter: 413 loss: 0.11658 training acc: 0.96496 testing acc: 0.9592\n",
      "epoch: 6 iter: 414 loss: 0.11797 training acc: 0.96480 testing acc: 0.9588\n",
      "epoch: 6 iter: 415 loss: 0.11040 training acc: 0.96706 testing acc: 0.9612\n",
      "epoch: 6 iter: 416 loss: 0.10867 training acc: 0.96834 testing acc: 0.964\n",
      "epoch: 6 iter: 417 loss: 0.11169 training acc: 0.96714 testing acc: 0.9608\n",
      "epoch: 6 iter: 418 loss: 0.11080 training acc: 0.96756 testing acc: 0.959\n",
      "epoch: 6 iter: 419 loss: 0.13766 training acc: 0.95904 testing acc: 0.9504\n",
      "epoch: 6 iter: 420 loss: 0.11655 training acc: 0.96570 testing acc: 0.9584\n",
      "epoch: 6 iter: 421 loss: 0.11366 training acc: 0.96692 testing acc: 0.9602\n",
      "epoch: 6 iter: 422 loss: 0.11316 training acc: 0.96790 testing acc: 0.9592\n",
      "epoch: 6 iter: 423 loss: 0.11122 training acc: 0.96806 testing acc: 0.9618\n",
      "epoch: 6 iter: 424 loss: 0.11318 training acc: 0.96752 testing acc: 0.9602\n",
      "epoch: 6 iter: 425 loss: 0.11108 training acc: 0.96810 testing acc: 0.9608\n",
      "epoch: 6 iter: 426 loss: 0.10841 training acc: 0.96798 testing acc: 0.9608\n",
      "epoch: 6 iter: 427 loss: 0.12462 training acc: 0.96186 testing acc: 0.9526\n",
      "epoch: 6 iter: 428 loss: 0.11832 training acc: 0.96444 testing acc: 0.9554\n",
      "epoch: 6 iter: 429 loss: 0.11970 training acc: 0.96492 testing acc: 0.9566\n",
      "epoch: 6 iter: 430 loss: 0.11625 training acc: 0.96530 testing acc: 0.9576\n",
      "epoch: 6 iter: 431 loss: 0.10934 training acc: 0.96742 testing acc: 0.9606\n",
      "epoch: 6 iter: 432 loss: 0.11366 training acc: 0.96646 testing acc: 0.9594\n",
      "epoch: 6 iter: 433 loss: 0.12560 training acc: 0.96228 testing acc: 0.9542\n",
      "epoch: 6 iter: 434 loss: 0.11930 training acc: 0.96436 testing acc: 0.959\n",
      "epoch: 6 iter: 435 loss: 0.11282 training acc: 0.96632 testing acc: 0.9604\n",
      "epoch: 6 iter: 436 loss: 0.11917 training acc: 0.96498 testing acc: 0.9558\n",
      "epoch: 6 iter: 437 loss: 0.11614 training acc: 0.96658 testing acc: 0.9582\n",
      "epoch: 6 iter: 438 loss: 0.11185 training acc: 0.96832 testing acc: 0.9586\n",
      "epoch: 6 iter: 439 loss: 0.11972 training acc: 0.96546 testing acc: 0.9554\n",
      "epoch: 6 iter: 440 loss: 0.12083 training acc: 0.96468 testing acc: 0.9556\n",
      "epoch: 6 iter: 441 loss: 0.11710 training acc: 0.96536 testing acc: 0.9552\n",
      "epoch: 6 iter: 442 loss: 0.11243 training acc: 0.96724 testing acc: 0.958\n",
      "epoch: 6 iter: 443 loss: 0.10951 training acc: 0.96830 testing acc: 0.958\n",
      "epoch: 6 iter: 444 loss: 0.11469 training acc: 0.96706 testing acc: 0.958\n",
      "epoch: 6 iter: 445 loss: 0.11621 training acc: 0.96616 testing acc: 0.956\n",
      "epoch: 6 iter: 446 loss: 0.11696 training acc: 0.96612 testing acc: 0.9556\n",
      "epoch: 6 iter: 447 loss: 0.11735 training acc: 0.96606 testing acc: 0.956\n",
      "epoch: 6 iter: 448 loss: 0.11500 training acc: 0.96746 testing acc: 0.957\n",
      "epoch: 6 iter: 449 loss: 0.11971 training acc: 0.96558 testing acc: 0.9558\n",
      "epoch: 6 iter: 450 loss: 0.12398 training acc: 0.96414 testing acc: 0.9546\n",
      "epoch: 6 iter: 451 loss: 0.11922 training acc: 0.96554 testing acc: 0.9572\n",
      "epoch: 6 iter: 452 loss: 0.12302 training acc: 0.96450 testing acc: 0.9552\n",
      "epoch: 6 iter: 453 loss: 0.11638 training acc: 0.96648 testing acc: 0.9584\n",
      "epoch: 6 iter: 454 loss: 0.11230 training acc: 0.96748 testing acc: 0.9598\n",
      "epoch: 6 iter: 455 loss: 0.11113 training acc: 0.96786 testing acc: 0.961\n",
      "epoch: 6 iter: 456 loss: 0.10716 training acc: 0.96912 testing acc: 0.9638\n",
      "epoch: 6 iter: 457 loss: 0.10718 training acc: 0.96940 testing acc: 0.9618\n",
      "epoch: 6 iter: 458 loss: 0.10673 training acc: 0.96934 testing acc: 0.9612\n",
      "epoch: 6 iter: 459 loss: 0.10690 training acc: 0.96970 testing acc: 0.962\n",
      "epoch: 6 iter: 460 loss: 0.10593 training acc: 0.96974 testing acc: 0.9624\n",
      "epoch: 6 iter: 461 loss: 0.11683 training acc: 0.96562 testing acc: 0.956\n",
      "epoch: 6 iter: 462 loss: 0.11439 training acc: 0.96620 testing acc: 0.9564\n",
      "epoch: 6 iter: 463 loss: 0.11250 training acc: 0.96712 testing acc: 0.958\n",
      "epoch: 6 iter: 464 loss: 0.11115 training acc: 0.96762 testing acc: 0.9592\n",
      "epoch: 6 iter: 465 loss: 0.10990 training acc: 0.96780 testing acc: 0.9604\n",
      "epoch: 6 iter: 466 loss: 0.11291 training acc: 0.96636 testing acc: 0.9586\n",
      "epoch: 6 iter: 467 loss: 0.10872 training acc: 0.96792 testing acc: 0.961\n",
      "epoch: 6 iter: 468 loss: 0.12090 training acc: 0.96284 testing acc: 0.9558\n",
      "epoch: 6 iter: 469 loss: 0.14110 training acc: 0.95480 testing acc: 0.942\n",
      "epoch: 6 iter: 470 loss: 0.12459 training acc: 0.96144 testing acc: 0.9532\n",
      "epoch: 6 iter: 471 loss: 0.12750 training acc: 0.96058 testing acc: 0.9514\n",
      "epoch: 6 iter: 472 loss: 0.11091 training acc: 0.96738 testing acc: 0.9592\n",
      "epoch: 6 iter: 473 loss: 0.10929 training acc: 0.96878 testing acc: 0.9612\n",
      "epoch: 6 iter: 474 loss: 0.11130 training acc: 0.96778 testing acc: 0.9596\n",
      "epoch: 6 iter: 475 loss: 0.10982 training acc: 0.96866 testing acc: 0.9608\n",
      "epoch: 6 iter: 476 loss: 0.10798 training acc: 0.96930 testing acc: 0.9608\n",
      "epoch: 6 iter: 477 loss: 0.11186 training acc: 0.96756 testing acc: 0.9608\n",
      "epoch: 6 iter: 478 loss: 0.11557 training acc: 0.96634 testing acc: 0.9602\n",
      "epoch: 6 iter: 479 loss: 0.11566 training acc: 0.96590 testing acc: 0.9598\n",
      "epoch: 6 iter: 480 loss: 0.11395 training acc: 0.96654 testing acc: 0.9602\n",
      "epoch: 6 iter: 481 loss: 0.11828 training acc: 0.96486 testing acc: 0.9592\n",
      "epoch: 6 iter: 482 loss: 0.11382 training acc: 0.96646 testing acc: 0.9608\n",
      "epoch: 6 iter: 483 loss: 0.11709 training acc: 0.96656 testing acc: 0.9562\n",
      "epoch: 6 iter: 484 loss: 0.11003 training acc: 0.96820 testing acc: 0.9596\n",
      "epoch: 6 iter: 485 loss: 0.11302 training acc: 0.96696 testing acc: 0.9578\n",
      "epoch: 6 iter: 486 loss: 0.13072 training acc: 0.95998 testing acc: 0.9504\n",
      "epoch: 6 iter: 487 loss: 0.14543 training acc: 0.95622 testing acc: 0.947\n",
      "epoch: 6 iter: 488 loss: 0.12930 training acc: 0.96082 testing acc: 0.9512\n",
      "epoch: 6 iter: 489 loss: 0.12569 training acc: 0.96208 testing acc: 0.953\n",
      "epoch: 6 iter: 490 loss: 0.11366 training acc: 0.96624 testing acc: 0.9584\n",
      "epoch: 6 iter: 491 loss: 0.10715 training acc: 0.96904 testing acc: 0.9608\n",
      "epoch: 6 iter: 492 loss: 0.10923 training acc: 0.96820 testing acc: 0.9594\n",
      "epoch: 6 iter: 493 loss: 0.12535 training acc: 0.96302 testing acc: 0.9552\n",
      "epoch: 6 iter: 494 loss: 0.11336 training acc: 0.96672 testing acc: 0.9606\n",
      "epoch: 6 iter: 495 loss: 0.11279 training acc: 0.96658 testing acc: 0.96\n",
      "epoch: 6 iter: 496 loss: 0.12080 training acc: 0.96374 testing acc: 0.9562\n",
      "epoch: 6 iter: 497 loss: 0.11786 training acc: 0.96564 testing acc: 0.957\n",
      "epoch: 6 iter: 498 loss: 0.11927 training acc: 0.96474 testing acc: 0.9566\n",
      "epoch: 6 iter: 499 loss: 0.11199 training acc: 0.96816 testing acc: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 iter: 500 loss: 0.11284 training acc: 0.96754 testing acc: 0.9604\n",
      "epoch: 6 iter: 501 loss: 0.10525 training acc: 0.96964 testing acc: 0.9624\n",
      "epoch: 6 iter: 502 loss: 0.10788 training acc: 0.96900 testing acc: 0.9594\n",
      "epoch: 6 iter: 503 loss: 0.10805 training acc: 0.96922 testing acc: 0.96\n",
      "epoch: 6 iter: 504 loss: 0.11348 training acc: 0.96666 testing acc: 0.9592\n",
      "epoch: 6 iter: 505 loss: 0.11511 training acc: 0.96606 testing acc: 0.9594\n",
      "epoch: 6 iter: 506 loss: 0.10821 training acc: 0.96896 testing acc: 0.9624\n",
      "epoch: 6 iter: 507 loss: 0.10557 training acc: 0.96986 testing acc: 0.9628\n",
      "epoch: 6 iter: 508 loss: 0.10450 training acc: 0.97008 testing acc: 0.9634\n",
      "epoch: 6 iter: 509 loss: 0.11135 training acc: 0.96796 testing acc: 0.9628\n",
      "epoch: 6 iter: 510 loss: 0.11141 training acc: 0.96782 testing acc: 0.9598\n",
      "epoch: 6 iter: 511 loss: 0.11419 training acc: 0.96676 testing acc: 0.9586\n",
      "epoch: 6 iter: 512 loss: 0.11312 training acc: 0.96690 testing acc: 0.9614\n",
      "epoch: 6 iter: 513 loss: 0.11955 training acc: 0.96538 testing acc: 0.9576\n",
      "epoch: 6 iter: 514 loss: 0.11195 training acc: 0.96776 testing acc: 0.9602\n",
      "epoch: 6 iter: 515 loss: 0.11304 training acc: 0.96750 testing acc: 0.96\n",
      "epoch: 6 iter: 516 loss: 0.10967 training acc: 0.96880 testing acc: 0.9608\n",
      "epoch: 6 iter: 517 loss: 0.11270 training acc: 0.96772 testing acc: 0.961\n",
      "epoch: 6 iter: 518 loss: 0.10984 training acc: 0.96784 testing acc: 0.9614\n",
      "epoch: 6 iter: 519 loss: 0.11278 training acc: 0.96760 testing acc: 0.96\n",
      "epoch: 6 iter: 520 loss: 0.14134 training acc: 0.95752 testing acc: 0.9482\n",
      "epoch: 6 iter: 521 loss: 0.11497 training acc: 0.96624 testing acc: 0.9562\n",
      "epoch: 6 iter: 522 loss: 0.11050 training acc: 0.96772 testing acc: 0.9574\n",
      "epoch: 6 iter: 523 loss: 0.10962 training acc: 0.96776 testing acc: 0.959\n",
      "epoch: 6 iter: 524 loss: 0.10998 training acc: 0.96774 testing acc: 0.9606\n",
      "epoch: 6 iter: 525 loss: 0.11357 training acc: 0.96680 testing acc: 0.9604\n",
      "epoch: 6 iter: 526 loss: 0.13189 training acc: 0.96098 testing acc: 0.952\n",
      "epoch: 6 iter: 527 loss: 0.11437 training acc: 0.96654 testing acc: 0.9584\n",
      "epoch: 6 iter: 528 loss: 0.11192 training acc: 0.96750 testing acc: 0.9624\n",
      "epoch: 6 iter: 529 loss: 0.12289 training acc: 0.96414 testing acc: 0.9582\n",
      "epoch: 6 iter: 530 loss: 0.10587 training acc: 0.96920 testing acc: 0.9636\n",
      "epoch: 6 iter: 531 loss: 0.10868 training acc: 0.96814 testing acc: 0.9594\n",
      "epoch: 6 iter: 532 loss: 0.10876 training acc: 0.96774 testing acc: 0.9594\n",
      "epoch: 6 iter: 533 loss: 0.10971 training acc: 0.96740 testing acc: 0.9578\n",
      "epoch: 6 iter: 534 loss: 0.10436 training acc: 0.96970 testing acc: 0.9614\n",
      "epoch: 6 iter: 535 loss: 0.10616 training acc: 0.96894 testing acc: 0.9606\n",
      "epoch: 6 iter: 536 loss: 0.11307 training acc: 0.96644 testing acc: 0.958\n",
      "epoch: 6 iter: 537 loss: 0.10438 training acc: 0.96988 testing acc: 0.9616\n",
      "epoch: 6 iter: 538 loss: 0.10371 training acc: 0.97014 testing acc: 0.9616\n",
      "epoch: 6 iter: 539 loss: 0.10441 training acc: 0.96980 testing acc: 0.9608\n",
      "epoch: 6 iter: 540 loss: 0.11123 training acc: 0.96668 testing acc: 0.96\n",
      "epoch: 6 iter: 541 loss: 0.11073 training acc: 0.96746 testing acc: 0.9586\n",
      "epoch: 6 iter: 542 loss: 0.11009 training acc: 0.96814 testing acc: 0.9586\n",
      "epoch: 6 iter: 543 loss: 0.10418 training acc: 0.96952 testing acc: 0.9604\n",
      "epoch: 6 iter: 544 loss: 0.10699 training acc: 0.96870 testing acc: 0.9602\n",
      "epoch: 6 iter: 545 loss: 0.11169 training acc: 0.96680 testing acc: 0.9576\n",
      "epoch: 6 iter: 546 loss: 0.11123 training acc: 0.96724 testing acc: 0.961\n",
      "epoch: 6 iter: 547 loss: 0.13460 training acc: 0.95912 testing acc: 0.952\n",
      "epoch: 6 iter: 548 loss: 0.12930 training acc: 0.96094 testing acc: 0.954\n",
      "epoch: 6 iter: 549 loss: 0.12861 training acc: 0.96112 testing acc: 0.955\n",
      "epoch: 6 iter: 550 loss: 0.13822 training acc: 0.95780 testing acc: 0.9492\n",
      "epoch: 6 iter: 551 loss: 0.11477 training acc: 0.96634 testing acc: 0.9562\n",
      "epoch: 6 iter: 552 loss: 0.11016 training acc: 0.96866 testing acc: 0.9598\n",
      "epoch: 6 iter: 553 loss: 0.10614 training acc: 0.97050 testing acc: 0.9624\n",
      "epoch: 6 iter: 554 loss: 0.10262 training acc: 0.97044 testing acc: 0.9628\n",
      "epoch: 6 iter: 555 loss: 0.10258 training acc: 0.97048 testing acc: 0.9622\n",
      "epoch: 6 iter: 556 loss: 0.10262 training acc: 0.97070 testing acc: 0.9618\n",
      "epoch: 6 iter: 557 loss: 0.10494 training acc: 0.96978 testing acc: 0.9606\n",
      "epoch: 6 iter: 558 loss: 0.10737 training acc: 0.96872 testing acc: 0.9604\n",
      "epoch: 6 iter: 559 loss: 0.10415 training acc: 0.96986 testing acc: 0.9622\n",
      "epoch: 6 iter: 560 loss: 0.10324 training acc: 0.96980 testing acc: 0.962\n",
      "epoch: 6 iter: 561 loss: 0.11032 training acc: 0.96784 testing acc: 0.9604\n",
      "epoch: 6 iter: 562 loss: 0.10728 training acc: 0.96874 testing acc: 0.96\n",
      "epoch: 6 iter: 563 loss: 0.10644 training acc: 0.96990 testing acc: 0.9632\n",
      "epoch: 6 iter: 564 loss: 0.10684 training acc: 0.96938 testing acc: 0.9612\n",
      "epoch: 6 iter: 565 loss: 0.10834 training acc: 0.96954 testing acc: 0.9626\n",
      "epoch: 6 iter: 566 loss: 0.10796 training acc: 0.96944 testing acc: 0.9626\n",
      "epoch: 6 iter: 567 loss: 0.10524 training acc: 0.96928 testing acc: 0.9614\n",
      "epoch: 6 iter: 568 loss: 0.10664 training acc: 0.96858 testing acc: 0.9608\n",
      "epoch: 6 iter: 569 loss: 0.10539 training acc: 0.96900 testing acc: 0.9612\n",
      "epoch: 6 iter: 570 loss: 0.10882 training acc: 0.96774 testing acc: 0.9586\n",
      "epoch: 6 iter: 571 loss: 0.10688 training acc: 0.96904 testing acc: 0.9604\n",
      "epoch: 6 iter: 572 loss: 0.11886 training acc: 0.96576 testing acc: 0.959\n",
      "epoch: 6 iter: 573 loss: 0.10771 training acc: 0.96910 testing acc: 0.9622\n",
      "epoch: 6 iter: 574 loss: 0.10328 training acc: 0.97044 testing acc: 0.9628\n",
      "epoch: 6 iter: 575 loss: 0.10436 training acc: 0.96910 testing acc: 0.9624\n",
      "epoch: 6 iter: 576 loss: 0.10142 training acc: 0.97042 testing acc: 0.9628\n",
      "epoch: 6 iter: 577 loss: 0.10242 training acc: 0.96976 testing acc: 0.9628\n",
      "epoch: 6 iter: 578 loss: 0.10098 training acc: 0.97108 testing acc: 0.9628\n",
      "epoch: 6 iter: 579 loss: 0.10394 training acc: 0.96984 testing acc: 0.9618\n",
      "epoch: 6 iter: 580 loss: 0.10721 training acc: 0.96888 testing acc: 0.961\n",
      "epoch: 6 iter: 581 loss: 0.10600 training acc: 0.96932 testing acc: 0.9646\n",
      "epoch: 6 iter: 582 loss: 0.10525 training acc: 0.97008 testing acc: 0.9648\n",
      "epoch: 6 iter: 583 loss: 0.12084 training acc: 0.96524 testing acc: 0.9594\n",
      "epoch: 6 iter: 584 loss: 0.11171 training acc: 0.96794 testing acc: 0.9616\n",
      "epoch: 6 iter: 585 loss: 0.10528 training acc: 0.96962 testing acc: 0.9612\n",
      "epoch: 6 iter: 586 loss: 0.10922 training acc: 0.96834 testing acc: 0.9606\n",
      "epoch: 6 iter: 587 loss: 0.10884 training acc: 0.96786 testing acc: 0.96\n",
      "epoch: 6 iter: 588 loss: 0.12034 training acc: 0.96460 testing acc: 0.9564\n",
      "epoch: 6 iter: 589 loss: 0.11807 training acc: 0.96472 testing acc: 0.957\n",
      "epoch: 6 iter: 590 loss: 0.10908 training acc: 0.96728 testing acc: 0.9594\n",
      "epoch: 6 iter: 591 loss: 0.11297 training acc: 0.96526 testing acc: 0.9584\n",
      "epoch: 6 iter: 592 loss: 0.11078 training acc: 0.96628 testing acc: 0.958\n",
      "epoch: 6 iter: 593 loss: 0.11326 training acc: 0.96628 testing acc: 0.9572\n",
      "epoch: 6 iter: 594 loss: 0.10691 training acc: 0.96892 testing acc: 0.9612\n",
      "epoch: 6 iter: 595 loss: 0.13909 training acc: 0.95608 testing acc: 0.9478\n",
      "epoch: 6 iter: 596 loss: 0.11125 training acc: 0.96724 testing acc: 0.9586\n",
      "epoch: 6 iter: 597 loss: 0.12160 training acc: 0.96330 testing acc: 0.9548\n",
      "epoch: 6 iter: 598 loss: 0.12745 training acc: 0.96214 testing acc: 0.9536\n",
      "epoch: 6 iter: 599 loss: 0.10495 training acc: 0.96956 testing acc: 0.9602\n",
      "epoch: 6 iter: 600 loss: 0.10447 training acc: 0.96976 testing acc: 0.9608\n",
      "epoch: 6 iter: 601 loss: 0.10426 training acc: 0.96982 testing acc: 0.9602\n",
      "epoch: 6 iter: 602 loss: 0.10335 training acc: 0.97032 testing acc: 0.9612\n",
      "epoch: 6 iter: 603 loss: 0.10314 training acc: 0.97030 testing acc: 0.962\n",
      "epoch: 6 iter: 604 loss: 0.10244 training acc: 0.97048 testing acc: 0.962\n",
      "epoch: 6 iter: 605 loss: 0.10245 training acc: 0.97040 testing acc: 0.9624\n",
      "epoch: 6 iter: 606 loss: 0.10201 training acc: 0.97024 testing acc: 0.9634\n",
      "epoch: 6 iter: 607 loss: 0.10895 training acc: 0.96792 testing acc: 0.9614\n",
      "epoch: 6 iter: 608 loss: 0.11519 training acc: 0.96540 testing acc: 0.9582\n",
      "epoch: 6 iter: 609 loss: 0.11038 training acc: 0.96730 testing acc: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 iter: 610 loss: 0.10711 training acc: 0.96904 testing acc: 0.9606\n",
      "epoch: 6 iter: 611 loss: 0.10399 training acc: 0.97010 testing acc: 0.9616\n",
      "epoch: 6 iter: 612 loss: 0.11607 training acc: 0.96624 testing acc: 0.9586\n",
      "epoch: 6 iter: 613 loss: 0.13038 training acc: 0.96110 testing acc: 0.9538\n",
      "epoch: 6 iter: 614 loss: 0.13001 training acc: 0.96072 testing acc: 0.9532\n",
      "epoch: 6 iter: 615 loss: 0.12226 training acc: 0.96402 testing acc: 0.956\n",
      "epoch: 6 iter: 616 loss: 0.11916 training acc: 0.96494 testing acc: 0.9576\n",
      "epoch: 6 iter: 617 loss: 0.10893 training acc: 0.96892 testing acc: 0.9618\n",
      "epoch: 6 iter: 618 loss: 0.10753 training acc: 0.96894 testing acc: 0.9618\n",
      "epoch: 6 iter: 619 loss: 0.11001 training acc: 0.96824 testing acc: 0.9606\n",
      "epoch: 6 iter: 620 loss: 0.10458 training acc: 0.97024 testing acc: 0.9608\n",
      "epoch: 6 iter: 621 loss: 0.10497 training acc: 0.96978 testing acc: 0.9618\n",
      "epoch: 6 iter: 622 loss: 0.10562 training acc: 0.96928 testing acc: 0.959\n",
      "epoch: 6 iter: 623 loss: 0.10891 training acc: 0.96844 testing acc: 0.9576\n",
      "epoch: 6 iter: 624 loss: 0.11280 training acc: 0.96722 testing acc: 0.959\n",
      "epoch: 6 iter: 625 loss: 0.10769 training acc: 0.96870 testing acc: 0.9572\n",
      "epoch: 6 iter: 626 loss: 0.10741 training acc: 0.96904 testing acc: 0.9594\n",
      "epoch: 6 iter: 627 loss: 0.10861 training acc: 0.96836 testing acc: 0.9604\n",
      "epoch: 6 iter: 628 loss: 0.10458 training acc: 0.96950 testing acc: 0.9598\n",
      "epoch: 6 iter: 629 loss: 0.11157 training acc: 0.96672 testing acc: 0.9572\n",
      "epoch: 6 iter: 630 loss: 0.11652 training acc: 0.96490 testing acc: 0.9556\n",
      "epoch: 6 iter: 631 loss: 0.11436 training acc: 0.96540 testing acc: 0.9566\n",
      "epoch: 6 iter: 632 loss: 0.11494 training acc: 0.96554 testing acc: 0.957\n",
      "epoch: 6 iter: 633 loss: 0.11567 training acc: 0.96540 testing acc: 0.9562\n",
      "epoch: 6 iter: 634 loss: 0.11203 training acc: 0.96644 testing acc: 0.9582\n",
      "epoch: 6 iter: 635 loss: 0.12358 training acc: 0.96294 testing acc: 0.9566\n",
      "epoch: 6 iter: 636 loss: 0.11856 training acc: 0.96456 testing acc: 0.957\n",
      "epoch: 6 iter: 637 loss: 0.10549 training acc: 0.96938 testing acc: 0.9612\n",
      "epoch: 6 iter: 638 loss: 0.10505 training acc: 0.96970 testing acc: 0.9628\n",
      "epoch: 6 iter: 639 loss: 0.12051 training acc: 0.96398 testing acc: 0.9578\n",
      "epoch: 6 iter: 640 loss: 0.11115 training acc: 0.96714 testing acc: 0.9594\n",
      "epoch: 6 iter: 641 loss: 0.12965 training acc: 0.96216 testing acc: 0.9556\n",
      "epoch: 6 iter: 642 loss: 0.11968 training acc: 0.96508 testing acc: 0.957\n",
      "epoch: 6 iter: 643 loss: 0.11611 training acc: 0.96574 testing acc: 0.9538\n",
      "epoch: 6 iter: 644 loss: 0.11191 training acc: 0.96732 testing acc: 0.9556\n",
      "epoch: 6 iter: 645 loss: 0.11277 training acc: 0.96676 testing acc: 0.9564\n",
      "epoch: 6 iter: 646 loss: 0.10722 training acc: 0.96766 testing acc: 0.9608\n",
      "epoch: 6 iter: 647 loss: 0.10745 training acc: 0.96774 testing acc: 0.958\n",
      "epoch: 6 iter: 648 loss: 0.10512 training acc: 0.96946 testing acc: 0.9596\n",
      "epoch: 6 iter: 649 loss: 0.10366 training acc: 0.96952 testing acc: 0.961\n",
      "epoch: 6 iter: 650 loss: 0.10866 training acc: 0.96694 testing acc: 0.9588\n",
      "epoch: 6 iter: 651 loss: 0.10729 training acc: 0.96728 testing acc: 0.9602\n",
      "epoch: 6 iter: 652 loss: 0.10539 training acc: 0.96804 testing acc: 0.9608\n",
      "epoch: 6 iter: 653 loss: 0.10555 training acc: 0.96786 testing acc: 0.9614\n",
      "epoch: 6 iter: 654 loss: 0.10898 training acc: 0.96728 testing acc: 0.9574\n",
      "epoch: 6 iter: 655 loss: 0.10831 training acc: 0.96660 testing acc: 0.9576\n",
      "epoch: 6 iter: 656 loss: 0.10568 training acc: 0.96848 testing acc: 0.9612\n",
      "epoch: 6 iter: 657 loss: 0.10603 training acc: 0.96844 testing acc: 0.9606\n",
      "epoch: 6 iter: 658 loss: 0.10837 training acc: 0.96778 testing acc: 0.9588\n",
      "epoch: 6 iter: 659 loss: 0.13781 training acc: 0.95706 testing acc: 0.947\n",
      "epoch: 6 iter: 660 loss: 0.13484 training acc: 0.95818 testing acc: 0.948\n",
      "epoch: 6 iter: 661 loss: 0.11009 training acc: 0.96652 testing acc: 0.9606\n",
      "epoch: 6 iter: 662 loss: 0.10564 training acc: 0.96850 testing acc: 0.9612\n",
      "epoch: 6 iter: 663 loss: 0.10866 training acc: 0.96692 testing acc: 0.96\n",
      "epoch: 6 iter: 664 loss: 0.10817 training acc: 0.96720 testing acc: 0.9602\n",
      "epoch: 6 iter: 665 loss: 0.10262 training acc: 0.96914 testing acc: 0.964\n",
      "epoch: 6 iter: 666 loss: 0.10054 training acc: 0.97088 testing acc: 0.9644\n",
      "epoch: 6 iter: 667 loss: 0.09978 training acc: 0.97086 testing acc: 0.9646\n",
      "epoch: 6 iter: 668 loss: 0.10048 training acc: 0.97022 testing acc: 0.9646\n",
      "epoch: 6 iter: 669 loss: 0.10972 training acc: 0.96912 testing acc: 0.9614\n",
      "epoch: 6 iter: 670 loss: 0.10675 training acc: 0.97028 testing acc: 0.9622\n",
      "epoch: 6 iter: 671 loss: 0.14062 training acc: 0.95498 testing acc: 0.9466\n",
      "epoch: 6 iter: 672 loss: 0.11888 training acc: 0.96358 testing acc: 0.9572\n",
      "epoch: 6 iter: 673 loss: 0.11147 training acc: 0.96644 testing acc: 0.961\n",
      "epoch: 6 iter: 674 loss: 0.10927 training acc: 0.96752 testing acc: 0.9624\n",
      "epoch: 6 iter: 675 loss: 0.10451 training acc: 0.96914 testing acc: 0.9646\n",
      "epoch: 6 iter: 676 loss: 0.11135 training acc: 0.96724 testing acc: 0.9624\n",
      "epoch: 6 iter: 677 loss: 0.12349 training acc: 0.96326 testing acc: 0.9552\n",
      "epoch: 6 iter: 678 loss: 0.10149 training acc: 0.97014 testing acc: 0.9664\n",
      "epoch: 6 iter: 679 loss: 0.10464 training acc: 0.96896 testing acc: 0.9638\n",
      "epoch: 6 iter: 680 loss: 0.10570 training acc: 0.96858 testing acc: 0.9622\n",
      "epoch: 6 iter: 681 loss: 0.10456 training acc: 0.96900 testing acc: 0.9638\n",
      "epoch: 6 iter: 682 loss: 0.10095 training acc: 0.97052 testing acc: 0.965\n",
      "epoch: 6 iter: 683 loss: 0.09845 training acc: 0.97152 testing acc: 0.9664\n",
      "epoch: 6 iter: 684 loss: 0.09774 training acc: 0.97170 testing acc: 0.9664\n",
      "epoch: 6 iter: 685 loss: 0.09865 training acc: 0.97066 testing acc: 0.9658\n",
      "epoch: 6 iter: 686 loss: 0.09877 training acc: 0.97126 testing acc: 0.9648\n",
      "epoch: 6 iter: 687 loss: 0.09909 training acc: 0.97076 testing acc: 0.965\n",
      "epoch: 6 iter: 688 loss: 0.09766 training acc: 0.97154 testing acc: 0.9658\n",
      "epoch: 6 iter: 689 loss: 0.09857 training acc: 0.97102 testing acc: 0.9654\n",
      "epoch: 6 iter: 690 loss: 0.09853 training acc: 0.97122 testing acc: 0.966\n",
      "epoch: 6 iter: 691 loss: 0.10710 training acc: 0.96910 testing acc: 0.964\n",
      "epoch: 6 iter: 692 loss: 0.10278 training acc: 0.97072 testing acc: 0.966\n",
      "epoch: 6 iter: 693 loss: 0.10243 training acc: 0.97072 testing acc: 0.9646\n",
      "epoch: 6 iter: 694 loss: 0.10046 training acc: 0.97122 testing acc: 0.9648\n",
      "epoch: 6 iter: 695 loss: 0.11706 training acc: 0.96566 testing acc: 0.9572\n",
      "epoch: 6 iter: 696 loss: 0.11238 training acc: 0.96712 testing acc: 0.9582\n",
      "epoch: 6 iter: 697 loss: 0.11046 training acc: 0.96824 testing acc: 0.9598\n",
      "epoch: 6 iter: 698 loss: 0.13454 training acc: 0.96012 testing acc: 0.9516\n",
      "epoch: 6 iter: 699 loss: 0.12404 training acc: 0.96396 testing acc: 0.9548\n",
      "epoch: 6 iter: 700 loss: 0.10691 training acc: 0.96972 testing acc: 0.9622\n",
      "epoch: 6 iter: 701 loss: 0.10524 training acc: 0.97004 testing acc: 0.9632\n",
      "epoch: 6 iter: 702 loss: 0.10177 training acc: 0.97108 testing acc: 0.964\n",
      "epoch: 6 iter: 703 loss: 0.10844 training acc: 0.96860 testing acc: 0.963\n",
      "epoch: 6 iter: 704 loss: 0.11193 training acc: 0.96686 testing acc: 0.959\n",
      "epoch: 6 iter: 705 loss: 0.12054 training acc: 0.96412 testing acc: 0.9548\n",
      "epoch: 6 iter: 706 loss: 0.10173 training acc: 0.97164 testing acc: 0.966\n",
      "epoch: 6 iter: 707 loss: 0.10095 training acc: 0.97122 testing acc: 0.9662\n",
      "epoch: 6 iter: 708 loss: 0.09992 training acc: 0.97166 testing acc: 0.965\n",
      "epoch: 6 iter: 709 loss: 0.10147 training acc: 0.97124 testing acc: 0.965\n",
      "epoch: 6 iter: 710 loss: 0.10894 training acc: 0.96846 testing acc: 0.9602\n",
      "epoch: 6 iter: 711 loss: 0.10958 training acc: 0.96796 testing acc: 0.9594\n",
      "epoch: 6 iter: 712 loss: 0.10223 training acc: 0.97006 testing acc: 0.9636\n",
      "epoch: 6 iter: 713 loss: 0.10976 training acc: 0.96808 testing acc: 0.9602\n",
      "epoch: 6 iter: 714 loss: 0.10410 training acc: 0.96986 testing acc: 0.9614\n",
      "epoch: 6 iter: 715 loss: 0.10419 training acc: 0.96998 testing acc: 0.9618\n",
      "epoch: 6 iter: 716 loss: 0.11928 training acc: 0.96450 testing acc: 0.9572\n",
      "epoch: 6 iter: 717 loss: 0.11820 training acc: 0.96496 testing acc: 0.9572\n",
      "epoch: 6 iter: 718 loss: 0.10473 training acc: 0.97030 testing acc: 0.9616\n",
      "epoch: 6 iter: 719 loss: 0.10432 training acc: 0.97030 testing acc: 0.963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 iter: 720 loss: 0.10473 training acc: 0.96964 testing acc: 0.9618\n",
      "epoch: 6 iter: 721 loss: 0.10149 training acc: 0.97046 testing acc: 0.9632\n",
      "epoch: 6 iter: 722 loss: 0.10198 training acc: 0.97048 testing acc: 0.9632\n",
      "epoch: 6 iter: 723 loss: 0.10406 training acc: 0.96938 testing acc: 0.962\n",
      "epoch: 6 iter: 724 loss: 0.10706 training acc: 0.96832 testing acc: 0.9628\n",
      "epoch: 6 iter: 725 loss: 0.10207 training acc: 0.97040 testing acc: 0.963\n",
      "epoch: 6 iter: 726 loss: 0.10269 training acc: 0.96922 testing acc: 0.9632\n",
      "epoch: 6 iter: 727 loss: 0.10604 training acc: 0.96892 testing acc: 0.9612\n",
      "epoch: 6 iter: 728 loss: 0.10997 training acc: 0.96822 testing acc: 0.961\n",
      "epoch: 6 iter: 729 loss: 0.11279 training acc: 0.96738 testing acc: 0.9616\n",
      "epoch: 6 iter: 730 loss: 0.10761 training acc: 0.96850 testing acc: 0.9614\n",
      "epoch: 6 iter: 731 loss: 0.10539 training acc: 0.96950 testing acc: 0.9616\n",
      "epoch: 6 iter: 732 loss: 0.11290 training acc: 0.96688 testing acc: 0.9594\n",
      "epoch: 6 iter: 733 loss: 0.11174 training acc: 0.96742 testing acc: 0.9598\n",
      "epoch: 6 iter: 734 loss: 0.10901 training acc: 0.96862 testing acc: 0.9608\n",
      "epoch: 6 iter: 735 loss: 0.10305 training acc: 0.97096 testing acc: 0.9634\n",
      "epoch: 6 iter: 736 loss: 0.10927 training acc: 0.96836 testing acc: 0.9594\n",
      "epoch: 6 iter: 737 loss: 0.11397 training acc: 0.96708 testing acc: 0.9592\n",
      "epoch: 6 iter: 738 loss: 0.10956 training acc: 0.96844 testing acc: 0.962\n",
      "epoch: 6 iter: 739 loss: 0.10649 training acc: 0.96944 testing acc: 0.9614\n",
      "epoch: 6 iter: 740 loss: 0.10607 training acc: 0.96922 testing acc: 0.9608\n",
      "epoch: 6 iter: 741 loss: 0.10829 training acc: 0.96814 testing acc: 0.9604\n",
      "epoch: 6 iter: 742 loss: 0.10209 training acc: 0.97040 testing acc: 0.9614\n",
      "epoch: 6 iter: 743 loss: 0.10965 training acc: 0.96752 testing acc: 0.9596\n",
      "epoch: 6 iter: 744 loss: 0.10018 training acc: 0.97116 testing acc: 0.9622\n",
      "epoch: 6 iter: 745 loss: 0.10038 training acc: 0.97094 testing acc: 0.963\n",
      "epoch: 6 iter: 746 loss: 0.10296 training acc: 0.97000 testing acc: 0.9612\n",
      "epoch: 6 iter: 747 loss: 0.10318 training acc: 0.97062 testing acc: 0.9618\n",
      "epoch: 6 iter: 748 loss: 0.10357 training acc: 0.97032 testing acc: 0.9612\n",
      "epoch: 6 iter: 749 loss: 0.10234 training acc: 0.97102 testing acc: 0.9612\n",
      "epoch: 6 iter: 750 loss: 0.10322 training acc: 0.97014 testing acc: 0.961\n",
      "epoch: 6 iter: 751 loss: 0.10240 training acc: 0.97118 testing acc: 0.963\n",
      "epoch: 6 iter: 752 loss: 0.10298 training acc: 0.97070 testing acc: 0.963\n",
      "epoch: 6 iter: 753 loss: 0.13029 training acc: 0.96080 testing acc: 0.9518\n",
      "epoch: 6 iter: 754 loss: 0.10657 training acc: 0.96848 testing acc: 0.9628\n",
      "epoch: 6 iter: 755 loss: 0.10241 training acc: 0.97086 testing acc: 0.9642\n",
      "epoch: 6 iter: 756 loss: 0.10112 training acc: 0.97076 testing acc: 0.9636\n",
      "epoch: 6 iter: 757 loss: 0.10051 training acc: 0.97072 testing acc: 0.9648\n",
      "epoch: 6 iter: 758 loss: 0.09863 training acc: 0.97190 testing acc: 0.9642\n",
      "epoch: 6 iter: 759 loss: 0.10015 training acc: 0.97138 testing acc: 0.9644\n",
      "epoch: 6 iter: 760 loss: 0.09819 training acc: 0.97194 testing acc: 0.9642\n",
      "epoch: 6 iter: 761 loss: 0.11021 training acc: 0.96722 testing acc: 0.9592\n",
      "epoch: 6 iter: 762 loss: 0.10808 training acc: 0.96860 testing acc: 0.9608\n",
      "epoch: 6 iter: 763 loss: 0.10553 training acc: 0.96922 testing acc: 0.961\n",
      "epoch: 6 iter: 764 loss: 0.10600 training acc: 0.96914 testing acc: 0.9614\n",
      "epoch: 6 iter: 765 loss: 0.10917 training acc: 0.96868 testing acc: 0.9596\n",
      "epoch: 6 iter: 766 loss: 0.11308 training acc: 0.96768 testing acc: 0.9584\n",
      "epoch: 6 iter: 767 loss: 0.11978 training acc: 0.96550 testing acc: 0.9566\n",
      "epoch: 6 iter: 768 loss: 0.12401 training acc: 0.96292 testing acc: 0.954\n",
      "epoch: 6 iter: 769 loss: 0.13495 training acc: 0.95922 testing acc: 0.95\n",
      "epoch: 6 iter: 770 loss: 0.12522 training acc: 0.96288 testing acc: 0.9532\n",
      "epoch: 6 iter: 771 loss: 0.13433 training acc: 0.95932 testing acc: 0.9498\n",
      "epoch: 6 iter: 772 loss: 0.10572 training acc: 0.96940 testing acc: 0.9596\n",
      "epoch: 6 iter: 773 loss: 0.10041 training acc: 0.97194 testing acc: 0.9642\n",
      "epoch: 6 iter: 774 loss: 0.10593 training acc: 0.96964 testing acc: 0.9636\n",
      "epoch: 6 iter: 775 loss: 0.10244 training acc: 0.97100 testing acc: 0.9648\n",
      "epoch: 6 iter: 776 loss: 0.10089 training acc: 0.97218 testing acc: 0.9638\n",
      "epoch: 6 iter: 777 loss: 0.10199 training acc: 0.97064 testing acc: 0.9622\n",
      "epoch: 6 iter: 778 loss: 0.09896 training acc: 0.97146 testing acc: 0.964\n",
      "epoch: 6 iter: 779 loss: 0.09738 training acc: 0.97228 testing acc: 0.965\n",
      "epoch: 6 iter: 780 loss: 0.09996 training acc: 0.97184 testing acc: 0.9636\n",
      "epoch: 6 iter: 781 loss: 0.10421 training acc: 0.96972 testing acc: 0.9612\n",
      "epoch: 6 iter: 782 loss: 0.12090 training acc: 0.96360 testing acc: 0.9566\n",
      "epoch: 6 iter: 783 loss: 0.12082 training acc: 0.96404 testing acc: 0.9568\n",
      "epoch: 6 iter: 784 loss: 0.11414 training acc: 0.96622 testing acc: 0.9596\n",
      "epoch: 6 iter: 785 loss: 0.11379 training acc: 0.96638 testing acc: 0.9598\n",
      "epoch: 6 iter: 786 loss: 0.10728 training acc: 0.96900 testing acc: 0.9618\n",
      "epoch: 6 iter: 787 loss: 0.10646 training acc: 0.96914 testing acc: 0.9626\n",
      "epoch: 6 iter: 788 loss: 0.11289 training acc: 0.96704 testing acc: 0.9602\n",
      "epoch: 6 iter: 789 loss: 0.10407 training acc: 0.97074 testing acc: 0.9628\n",
      "epoch: 6 iter: 790 loss: 0.10396 training acc: 0.97058 testing acc: 0.9636\n",
      "epoch: 6 iter: 791 loss: 0.10337 training acc: 0.97088 testing acc: 0.9628\n",
      "epoch: 6 iter: 792 loss: 0.10349 training acc: 0.97066 testing acc: 0.961\n",
      "epoch: 6 iter: 793 loss: 0.11891 training acc: 0.96540 testing acc: 0.9546\n",
      "epoch: 6 iter: 794 loss: 0.11545 training acc: 0.96650 testing acc: 0.9576\n",
      "epoch: 6 iter: 795 loss: 0.12212 training acc: 0.96508 testing acc: 0.9588\n",
      "epoch: 6 iter: 796 loss: 0.18187 training acc: 0.94190 testing acc: 0.9348\n",
      "epoch: 6 iter: 797 loss: 0.13847 training acc: 0.95832 testing acc: 0.954\n",
      "epoch: 6 iter: 798 loss: 0.11488 training acc: 0.96614 testing acc: 0.9616\n",
      "epoch: 6 iter: 799 loss: 0.12457 training acc: 0.96230 testing acc: 0.9586\n",
      "epoch: 6 iter: 800 loss: 0.12143 training acc: 0.96462 testing acc: 0.9592\n",
      "epoch: 6 iter: 801 loss: 0.11106 training acc: 0.96772 testing acc: 0.9634\n",
      "epoch: 6 iter: 802 loss: 0.10583 training acc: 0.97028 testing acc: 0.9648\n",
      "epoch: 6 iter: 803 loss: 0.10768 training acc: 0.96914 testing acc: 0.96\n",
      "epoch: 6 iter: 804 loss: 0.10375 training acc: 0.97060 testing acc: 0.9618\n",
      "epoch: 6 iter: 805 loss: 0.10272 training acc: 0.97128 testing acc: 0.9614\n",
      "epoch: 6 iter: 806 loss: 0.10421 training acc: 0.97028 testing acc: 0.9626\n",
      "epoch: 6 iter: 807 loss: 0.10610 training acc: 0.96968 testing acc: 0.9608\n",
      "epoch: 6 iter: 808 loss: 0.10206 training acc: 0.97062 testing acc: 0.96\n",
      "epoch: 6 iter: 809 loss: 0.10642 training acc: 0.96886 testing acc: 0.958\n",
      "epoch: 6 iter: 810 loss: 0.10592 training acc: 0.96896 testing acc: 0.9582\n",
      "epoch: 6 iter: 811 loss: 0.10234 training acc: 0.97020 testing acc: 0.9596\n",
      "epoch: 6 iter: 812 loss: 0.10516 training acc: 0.96922 testing acc: 0.9594\n",
      "epoch: 6 iter: 813 loss: 0.11686 training acc: 0.96540 testing acc: 0.9572\n",
      "epoch: 6 iter: 814 loss: 0.11397 training acc: 0.96676 testing acc: 0.957\n",
      "epoch: 6 iter: 815 loss: 0.11189 training acc: 0.96678 testing acc: 0.9586\n",
      "epoch: 6 iter: 816 loss: 0.10354 training acc: 0.96966 testing acc: 0.963\n",
      "epoch: 6 iter: 817 loss: 0.10024 training acc: 0.97080 testing acc: 0.9626\n",
      "epoch: 6 iter: 818 loss: 0.10667 training acc: 0.96904 testing acc: 0.962\n",
      "epoch: 6 iter: 819 loss: 0.10385 training acc: 0.96982 testing acc: 0.9626\n",
      "epoch: 6 iter: 820 loss: 0.09768 training acc: 0.97196 testing acc: 0.9634\n",
      "epoch: 6 iter: 821 loss: 0.10347 training acc: 0.96906 testing acc: 0.9602\n",
      "epoch: 6 iter: 822 loss: 0.10312 training acc: 0.96934 testing acc: 0.96\n",
      "epoch: 6 iter: 823 loss: 0.10354 training acc: 0.96930 testing acc: 0.9592\n",
      "epoch: 6 iter: 824 loss: 0.10660 training acc: 0.96850 testing acc: 0.959\n",
      "epoch: 6 iter: 825 loss: 0.11161 training acc: 0.96658 testing acc: 0.9576\n",
      "epoch: 6 iter: 826 loss: 0.10811 training acc: 0.96794 testing acc: 0.9582\n",
      "epoch: 6 iter: 827 loss: 0.10555 training acc: 0.96924 testing acc: 0.9576\n",
      "epoch: 6 iter: 828 loss: 0.10313 training acc: 0.96920 testing acc: 0.961\n",
      "epoch: 6 iter: 829 loss: 0.10149 training acc: 0.96958 testing acc: 0.9618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 iter: 830 loss: 0.10982 training acc: 0.96680 testing acc: 0.959\n",
      "epoch: 6 iter: 831 loss: 0.10598 training acc: 0.96850 testing acc: 0.9596\n",
      "epoch: 6 iter: 832 loss: 0.11020 training acc: 0.96712 testing acc: 0.958\n",
      "epoch: 6 iter: 833 loss: 0.10733 training acc: 0.96850 testing acc: 0.9604\n",
      "epoch: 6 iter: 834 loss: 0.11104 training acc: 0.96670 testing acc: 0.9594\n",
      "epoch: 6 iter: 835 loss: 0.11201 training acc: 0.96638 testing acc: 0.9574\n",
      "epoch: 6 iter: 836 loss: 0.11366 training acc: 0.96596 testing acc: 0.9566\n",
      "epoch: 6 iter: 837 loss: 0.11314 training acc: 0.96562 testing acc: 0.9566\n",
      "epoch: 6 iter: 838 loss: 0.11462 training acc: 0.96548 testing acc: 0.9562\n",
      "epoch: 6 iter: 839 loss: 0.11452 training acc: 0.96538 testing acc: 0.957\n",
      "epoch: 6 iter: 840 loss: 0.10250 training acc: 0.96934 testing acc: 0.9616\n",
      "epoch: 6 iter: 841 loss: 0.11584 training acc: 0.96480 testing acc: 0.9574\n",
      "epoch: 6 iter: 842 loss: 0.10935 training acc: 0.96688 testing acc: 0.9588\n",
      "epoch: 6 iter: 843 loss: 0.11009 training acc: 0.96664 testing acc: 0.9576\n",
      "epoch: 6 iter: 844 loss: 0.10451 training acc: 0.96914 testing acc: 0.961\n",
      "epoch: 6 iter: 845 loss: 0.10152 training acc: 0.96948 testing acc: 0.9624\n",
      "epoch: 6 iter: 846 loss: 0.10692 training acc: 0.96796 testing acc: 0.9612\n",
      "epoch: 6 iter: 847 loss: 0.09797 training acc: 0.97156 testing acc: 0.9654\n",
      "epoch: 6 iter: 848 loss: 0.09835 training acc: 0.97160 testing acc: 0.9654\n",
      "epoch: 6 iter: 849 loss: 0.09870 training acc: 0.97126 testing acc: 0.965\n",
      "epoch: 6 iter: 850 loss: 0.09776 training acc: 0.97130 testing acc: 0.9652\n",
      "epoch: 6 iter: 851 loss: 0.10706 training acc: 0.96764 testing acc: 0.9616\n",
      "epoch: 6 iter: 852 loss: 0.10241 training acc: 0.96968 testing acc: 0.964\n",
      "epoch: 6 iter: 853 loss: 0.10182 training acc: 0.96994 testing acc: 0.9644\n",
      "epoch: 6 iter: 854 loss: 0.12147 training acc: 0.96300 testing acc: 0.9596\n",
      "epoch: 6 iter: 855 loss: 0.11311 training acc: 0.96598 testing acc: 0.963\n",
      "epoch: 6 iter: 856 loss: 0.10205 training acc: 0.97038 testing acc: 0.9648\n",
      "epoch: 6 iter: 857 loss: 0.10855 training acc: 0.96932 testing acc: 0.9648\n",
      "epoch: 6 iter: 858 loss: 0.10507 training acc: 0.97064 testing acc: 0.9632\n",
      "epoch: 6 iter: 859 loss: 0.11098 training acc: 0.96874 testing acc: 0.9616\n",
      "epoch: 6 iter: 860 loss: 0.11330 training acc: 0.96590 testing acc: 0.9608\n",
      "epoch: 6 iter: 861 loss: 0.10111 training acc: 0.97072 testing acc: 0.9626\n",
      "epoch: 6 iter: 862 loss: 0.10361 training acc: 0.96932 testing acc: 0.96\n",
      "epoch: 6 iter: 863 loss: 0.10720 training acc: 0.96848 testing acc: 0.9572\n",
      "epoch: 6 iter: 864 loss: 0.10327 training acc: 0.97006 testing acc: 0.9624\n",
      "epoch: 6 iter: 865 loss: 0.10400 training acc: 0.96946 testing acc: 0.9612\n",
      "epoch: 6 iter: 866 loss: 0.10258 training acc: 0.96974 testing acc: 0.9612\n",
      "epoch: 6 iter: 867 loss: 0.11319 training acc: 0.96634 testing acc: 0.9608\n",
      "epoch: 6 iter: 868 loss: 0.11018 training acc: 0.96728 testing acc: 0.9632\n",
      "epoch: 6 iter: 869 loss: 0.10274 training acc: 0.96950 testing acc: 0.9646\n",
      "epoch: 6 iter: 870 loss: 0.11777 training acc: 0.96474 testing acc: 0.9588\n",
      "epoch: 6 iter: 871 loss: 0.10751 training acc: 0.96866 testing acc: 0.963\n",
      "epoch: 6 iter: 872 loss: 0.10482 training acc: 0.96970 testing acc: 0.9636\n",
      "epoch: 6 iter: 873 loss: 0.12964 training acc: 0.96114 testing acc: 0.9524\n",
      "epoch: 6 iter: 874 loss: 0.11151 training acc: 0.96836 testing acc: 0.9584\n",
      "epoch: 6 iter: 875 loss: 0.10584 training acc: 0.96980 testing acc: 0.9604\n",
      "epoch: 6 iter: 876 loss: 0.10536 training acc: 0.97016 testing acc: 0.9604\n",
      "epoch: 6 iter: 877 loss: 0.10563 training acc: 0.96962 testing acc: 0.9624\n",
      "epoch: 6 iter: 878 loss: 0.10674 training acc: 0.96902 testing acc: 0.963\n",
      "epoch: 6 iter: 879 loss: 0.10244 training acc: 0.97052 testing acc: 0.9632\n",
      "epoch: 6 iter: 880 loss: 0.10266 training acc: 0.97018 testing acc: 0.9626\n",
      "epoch: 6 iter: 881 loss: 0.10495 training acc: 0.96852 testing acc: 0.9638\n",
      "epoch: 6 iter: 882 loss: 0.11348 training acc: 0.96568 testing acc: 0.9602\n",
      "epoch: 6 iter: 883 loss: 0.11514 training acc: 0.96596 testing acc: 0.9604\n",
      "epoch: 6 iter: 884 loss: 0.11541 training acc: 0.96586 testing acc: 0.9598\n",
      "epoch: 6 iter: 885 loss: 0.10578 training acc: 0.96904 testing acc: 0.963\n",
      "epoch: 6 iter: 886 loss: 0.10917 training acc: 0.96782 testing acc: 0.9612\n",
      "epoch: 6 iter: 887 loss: 0.11668 training acc: 0.96540 testing acc: 0.9592\n",
      "epoch: 6 iter: 888 loss: 0.10313 training acc: 0.96982 testing acc: 0.9644\n",
      "epoch: 6 iter: 889 loss: 0.09859 training acc: 0.97190 testing acc: 0.9668\n",
      "epoch: 6 iter: 890 loss: 0.10128 training acc: 0.97058 testing acc: 0.9648\n",
      "epoch: 6 iter: 891 loss: 0.10054 training acc: 0.97128 testing acc: 0.9654\n",
      "epoch: 6 iter: 892 loss: 0.10278 training acc: 0.97028 testing acc: 0.9638\n",
      "epoch: 6 iter: 893 loss: 0.11341 training acc: 0.96622 testing acc: 0.9598\n",
      "epoch: 6 iter: 894 loss: 0.11037 training acc: 0.96696 testing acc: 0.9616\n",
      "epoch: 6 iter: 895 loss: 0.10374 training acc: 0.96896 testing acc: 0.961\n",
      "epoch: 6 iter: 896 loss: 0.11743 training acc: 0.96468 testing acc: 0.9536\n",
      "epoch: 6 iter: 897 loss: 0.19206 training acc: 0.93770 testing acc: 0.925\n",
      "epoch: 6 iter: 898 loss: 0.11048 training acc: 0.96708 testing acc: 0.9568\n",
      "epoch: 6 iter: 899 loss: 0.10746 training acc: 0.96818 testing acc: 0.9582\n",
      "epoch: 6 iter: 900 loss: 0.10073 training acc: 0.97138 testing acc: 0.9638\n",
      "epoch: 6 iter: 901 loss: 0.10277 training acc: 0.97072 testing acc: 0.963\n",
      "epoch: 6 iter: 902 loss: 0.11213 training acc: 0.96726 testing acc: 0.9604\n",
      "epoch: 6 iter: 903 loss: 0.10916 training acc: 0.96794 testing acc: 0.9638\n",
      "epoch: 6 iter: 904 loss: 0.10591 training acc: 0.96836 testing acc: 0.9642\n",
      "epoch: 6 iter: 905 loss: 0.10462 training acc: 0.96946 testing acc: 0.9656\n",
      "epoch: 6 iter: 906 loss: 0.10109 training acc: 0.97126 testing acc: 0.965\n",
      "epoch: 6 iter: 907 loss: 0.09670 training acc: 0.97188 testing acc: 0.9638\n",
      "epoch: 6 iter: 908 loss: 0.09887 training acc: 0.97138 testing acc: 0.9638\n",
      "epoch: 6 iter: 909 loss: 0.09790 training acc: 0.97158 testing acc: 0.9628\n",
      "epoch: 6 iter: 910 loss: 0.10938 training acc: 0.96734 testing acc: 0.9582\n",
      "epoch: 6 iter: 911 loss: 0.10362 training acc: 0.97018 testing acc: 0.9598\n",
      "epoch: 6 iter: 912 loss: 0.09733 training acc: 0.97188 testing acc: 0.9654\n",
      "epoch: 6 iter: 913 loss: 0.09782 training acc: 0.97182 testing acc: 0.965\n",
      "epoch: 6 iter: 914 loss: 0.09825 training acc: 0.97198 testing acc: 0.9654\n",
      "epoch: 6 iter: 915 loss: 0.09637 training acc: 0.97274 testing acc: 0.9664\n",
      "epoch: 6 iter: 916 loss: 0.09654 training acc: 0.97244 testing acc: 0.9668\n",
      "epoch: 6 iter: 917 loss: 0.09828 training acc: 0.97206 testing acc: 0.9654\n",
      "epoch: 6 iter: 918 loss: 0.14619 training acc: 0.95348 testing acc: 0.9502\n",
      "epoch: 6 iter: 919 loss: 0.14464 training acc: 0.95374 testing acc: 0.9508\n",
      "epoch: 6 iter: 920 loss: 0.11054 training acc: 0.96772 testing acc: 0.961\n",
      "epoch: 6 iter: 921 loss: 0.11963 training acc: 0.96288 testing acc: 0.955\n",
      "epoch: 6 iter: 922 loss: 0.10880 training acc: 0.96744 testing acc: 0.961\n",
      "epoch: 6 iter: 923 loss: 0.10422 training acc: 0.96894 testing acc: 0.9622\n",
      "epoch: 6 iter: 924 loss: 0.09926 training acc: 0.97086 testing acc: 0.965\n",
      "epoch: 6 iter: 925 loss: 0.10789 training acc: 0.96700 testing acc: 0.9632\n",
      "epoch: 6 iter: 926 loss: 0.10482 training acc: 0.96862 testing acc: 0.9614\n",
      "epoch: 6 iter: 927 loss: 0.10499 training acc: 0.96864 testing acc: 0.9596\n",
      "epoch: 6 iter: 928 loss: 0.10195 training acc: 0.96898 testing acc: 0.9622\n",
      "epoch: 6 iter: 929 loss: 0.10435 training acc: 0.96852 testing acc: 0.9618\n",
      "epoch: 6 iter: 930 loss: 0.10041 training acc: 0.97014 testing acc: 0.9632\n",
      "epoch: 6 iter: 931 loss: 0.09787 training acc: 0.97094 testing acc: 0.964\n",
      "epoch: 6 iter: 932 loss: 0.10312 training acc: 0.97030 testing acc: 0.9608\n",
      "epoch: 6 iter: 933 loss: 0.09396 training acc: 0.97274 testing acc: 0.9638\n",
      "epoch: 6 iter: 934 loss: 0.09633 training acc: 0.97226 testing acc: 0.9628\n",
      "epoch: 6 iter: 935 loss: 0.09451 training acc: 0.97298 testing acc: 0.966\n",
      "epoch: 6 iter: 936 loss: 0.09890 training acc: 0.97126 testing acc: 0.9622\n",
      "epoch: 6 iter: 937 loss: 0.09820 training acc: 0.97192 testing acc: 0.9634\n",
      "epoch: 6 iter: 938 loss: 0.09967 training acc: 0.97202 testing acc: 0.9618\n",
      "epoch: 6 iter: 939 loss: 0.09792 training acc: 0.97284 testing acc: 0.9628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 iter: 940 loss: 0.09696 training acc: 0.97262 testing acc: 0.9618\n",
      "epoch: 6 iter: 941 loss: 0.09501 training acc: 0.97370 testing acc: 0.9632\n",
      "epoch: 6 iter: 942 loss: 0.09434 training acc: 0.97368 testing acc: 0.9638\n",
      "epoch: 6 iter: 943 loss: 0.09315 training acc: 0.97380 testing acc: 0.9654\n",
      "epoch: 6 iter: 944 loss: 0.10295 training acc: 0.96996 testing acc: 0.9628\n",
      "epoch: 6 iter: 945 loss: 0.10046 training acc: 0.97124 testing acc: 0.9626\n",
      "epoch: 6 iter: 946 loss: 0.10009 training acc: 0.97110 testing acc: 0.965\n",
      "epoch: 6 iter: 947 loss: 0.09300 training acc: 0.97374 testing acc: 0.9668\n",
      "epoch: 6 iter: 948 loss: 0.09400 training acc: 0.97308 testing acc: 0.9666\n",
      "epoch: 6 iter: 949 loss: 0.09488 training acc: 0.97274 testing acc: 0.964\n",
      "epoch: 6 iter: 950 loss: 0.09497 training acc: 0.97284 testing acc: 0.9638\n",
      "epoch: 6 iter: 951 loss: 0.09452 training acc: 0.97286 testing acc: 0.9644\n",
      "epoch: 6 iter: 952 loss: 0.09933 training acc: 0.97040 testing acc: 0.9604\n",
      "epoch: 6 iter: 953 loss: 0.09681 training acc: 0.97128 testing acc: 0.9618\n",
      "epoch: 6 iter: 954 loss: 0.09819 training acc: 0.97088 testing acc: 0.9616\n",
      "epoch: 6 iter: 955 loss: 0.09511 training acc: 0.97262 testing acc: 0.9654\n",
      "epoch: 6 iter: 956 loss: 0.10324 training acc: 0.96908 testing acc: 0.961\n",
      "epoch: 6 iter: 957 loss: 0.11243 training acc: 0.96602 testing acc: 0.9568\n",
      "epoch: 6 iter: 958 loss: 0.09872 training acc: 0.97082 testing acc: 0.9624\n",
      "epoch: 6 iter: 959 loss: 0.09973 training acc: 0.97030 testing acc: 0.9622\n",
      "epoch: 6 iter: 960 loss: 0.10402 training acc: 0.96860 testing acc: 0.9604\n",
      "epoch: 6 iter: 961 loss: 0.09856 training acc: 0.97166 testing acc: 0.9612\n",
      "epoch: 6 iter: 962 loss: 0.10030 training acc: 0.97088 testing acc: 0.9624\n",
      "epoch: 6 iter: 963 loss: 0.09980 training acc: 0.97066 testing acc: 0.9614\n",
      "epoch: 6 iter: 964 loss: 0.09662 training acc: 0.97184 testing acc: 0.962\n",
      "epoch: 6 iter: 965 loss: 0.09838 training acc: 0.97066 testing acc: 0.9614\n",
      "epoch: 6 iter: 966 loss: 0.09765 training acc: 0.97128 testing acc: 0.9612\n",
      "epoch: 6 iter: 967 loss: 0.09835 training acc: 0.97142 testing acc: 0.9606\n",
      "epoch: 6 iter: 968 loss: 0.10418 training acc: 0.96898 testing acc: 0.9578\n",
      "epoch: 6 iter: 969 loss: 0.10830 training acc: 0.96670 testing acc: 0.9578\n",
      "epoch: 6 iter: 970 loss: 0.11289 training acc: 0.96594 testing acc: 0.9562\n",
      "epoch: 6 iter: 971 loss: 0.10341 training acc: 0.96924 testing acc: 0.959\n",
      "epoch: 6 iter: 972 loss: 0.10508 training acc: 0.96860 testing acc: 0.9588\n",
      "epoch: 6 iter: 973 loss: 0.10332 training acc: 0.96904 testing acc: 0.961\n",
      "epoch: 6 iter: 974 loss: 0.10389 training acc: 0.96922 testing acc: 0.9596\n",
      "epoch: 6 iter: 975 loss: 0.10655 training acc: 0.96826 testing acc: 0.9602\n",
      "epoch: 6 iter: 976 loss: 0.10003 training acc: 0.97108 testing acc: 0.9622\n",
      "epoch: 6 iter: 977 loss: 0.09835 training acc: 0.97142 testing acc: 0.9616\n",
      "epoch: 6 iter: 978 loss: 0.10092 training acc: 0.97066 testing acc: 0.9596\n",
      "epoch: 6 iter: 979 loss: 0.10003 training acc: 0.97114 testing acc: 0.9616\n",
      "epoch: 6 iter: 980 loss: 0.09530 training acc: 0.97272 testing acc: 0.9646\n",
      "epoch: 6 iter: 981 loss: 0.09458 training acc: 0.97308 testing acc: 0.9638\n",
      "epoch: 6 iter: 982 loss: 0.09479 training acc: 0.97258 testing acc: 0.9646\n",
      "epoch: 6 iter: 983 loss: 0.09834 training acc: 0.97162 testing acc: 0.9626\n",
      "epoch: 6 iter: 984 loss: 0.10017 training acc: 0.97060 testing acc: 0.9634\n",
      "epoch: 6 iter: 985 loss: 0.09463 training acc: 0.97254 testing acc: 0.965\n",
      "epoch: 6 iter: 986 loss: 0.09565 training acc: 0.97232 testing acc: 0.9646\n",
      "epoch: 6 iter: 987 loss: 0.10366 training acc: 0.96856 testing acc: 0.9584\n",
      "epoch: 6 iter: 988 loss: 0.10285 training acc: 0.96926 testing acc: 0.9608\n",
      "epoch: 6 iter: 989 loss: 0.09114 training acc: 0.97394 testing acc: 0.9646\n",
      "epoch: 6 iter: 990 loss: 0.10140 training acc: 0.97056 testing acc: 0.9634\n",
      "epoch: 6 iter: 991 loss: 0.10378 training acc: 0.96930 testing acc: 0.963\n",
      "epoch: 6 iter: 992 loss: 0.09693 training acc: 0.97206 testing acc: 0.9626\n",
      "epoch: 6 iter: 993 loss: 0.09566 training acc: 0.97258 testing acc: 0.9638\n",
      "epoch: 6 iter: 994 loss: 0.10291 training acc: 0.97094 testing acc: 0.9626\n",
      "epoch: 6 iter: 995 loss: 0.10272 training acc: 0.97096 testing acc: 0.963\n",
      "epoch: 6 iter: 996 loss: 0.10307 training acc: 0.97070 testing acc: 0.962\n",
      "epoch: 6 iter: 997 loss: 0.10640 training acc: 0.96986 testing acc: 0.9616\n",
      "epoch: 6 iter: 998 loss: 0.11434 training acc: 0.96764 testing acc: 0.9594\n",
      "epoch: 6 iter: 999 loss: 0.11158 training acc: 0.96808 testing acc: 0.9606\n",
      "epoch: 7 iter:   0 loss: 0.10073 training acc: 0.97184 testing acc: 0.9634\n",
      "epoch: 7 iter:   1 loss: 0.09621 training acc: 0.97346 testing acc: 0.9664\n",
      "epoch: 7 iter:   2 loss: 0.09371 training acc: 0.97354 testing acc: 0.9646\n",
      "epoch: 7 iter:   3 loss: 0.09312 training acc: 0.97392 testing acc: 0.9668\n",
      "epoch: 7 iter:   4 loss: 0.09526 training acc: 0.97258 testing acc: 0.9646\n",
      "epoch: 7 iter:   5 loss: 0.09485 training acc: 0.97248 testing acc: 0.965\n",
      "epoch: 7 iter:   6 loss: 0.10051 training acc: 0.97044 testing acc: 0.9612\n",
      "epoch: 7 iter:   7 loss: 0.10104 training acc: 0.97020 testing acc: 0.9618\n",
      "epoch: 7 iter:   8 loss: 0.09821 training acc: 0.97134 testing acc: 0.9638\n",
      "epoch: 7 iter:   9 loss: 0.11926 training acc: 0.96438 testing acc: 0.957\n",
      "epoch: 7 iter:  10 loss: 0.11493 training acc: 0.96638 testing acc: 0.9582\n",
      "epoch: 7 iter:  11 loss: 0.09587 training acc: 0.97308 testing acc: 0.9654\n",
      "epoch: 7 iter:  12 loss: 0.09727 training acc: 0.97266 testing acc: 0.9646\n",
      "epoch: 7 iter:  13 loss: 0.09641 training acc: 0.97232 testing acc: 0.9652\n",
      "epoch: 7 iter:  14 loss: 0.10999 training acc: 0.96652 testing acc: 0.9608\n",
      "epoch: 7 iter:  15 loss: 0.11209 training acc: 0.96548 testing acc: 0.9576\n",
      "epoch: 7 iter:  16 loss: 0.10452 training acc: 0.96898 testing acc: 0.9624\n",
      "epoch: 7 iter:  17 loss: 0.11016 training acc: 0.96680 testing acc: 0.9608\n",
      "epoch: 7 iter:  18 loss: 0.10369 training acc: 0.96946 testing acc: 0.962\n",
      "epoch: 7 iter:  19 loss: 0.10284 training acc: 0.97048 testing acc: 0.9634\n",
      "epoch: 7 iter:  20 loss: 0.10398 training acc: 0.96962 testing acc: 0.963\n",
      "epoch: 7 iter:  21 loss: 0.10233 training acc: 0.97106 testing acc: 0.9614\n",
      "epoch: 7 iter:  22 loss: 0.09850 training acc: 0.97240 testing acc: 0.9642\n",
      "epoch: 7 iter:  23 loss: 0.09803 training acc: 0.97230 testing acc: 0.963\n",
      "epoch: 7 iter:  24 loss: 0.09887 training acc: 0.97206 testing acc: 0.9622\n",
      "epoch: 7 iter:  25 loss: 0.09548 training acc: 0.97306 testing acc: 0.9638\n",
      "epoch: 7 iter:  26 loss: 0.09621 training acc: 0.97240 testing acc: 0.9652\n",
      "epoch: 7 iter:  27 loss: 0.09590 training acc: 0.97294 testing acc: 0.964\n",
      "epoch: 7 iter:  28 loss: 0.09778 training acc: 0.97212 testing acc: 0.9612\n",
      "epoch: 7 iter:  29 loss: 0.09379 training acc: 0.97338 testing acc: 0.9632\n",
      "epoch: 7 iter:  30 loss: 0.09644 training acc: 0.97234 testing acc: 0.9622\n",
      "epoch: 7 iter:  31 loss: 0.10375 training acc: 0.97010 testing acc: 0.9596\n",
      "epoch: 7 iter:  32 loss: 0.09771 training acc: 0.97198 testing acc: 0.9642\n",
      "epoch: 7 iter:  33 loss: 0.09942 training acc: 0.97096 testing acc: 0.9614\n",
      "epoch: 7 iter:  34 loss: 0.09762 training acc: 0.97164 testing acc: 0.9616\n",
      "epoch: 7 iter:  35 loss: 0.09503 training acc: 0.97214 testing acc: 0.963\n",
      "epoch: 7 iter:  36 loss: 0.09518 training acc: 0.97234 testing acc: 0.9618\n",
      "epoch: 7 iter:  37 loss: 0.09940 training acc: 0.97188 testing acc: 0.9634\n",
      "epoch: 7 iter:  38 loss: 0.11261 training acc: 0.96688 testing acc: 0.9598\n",
      "epoch: 7 iter:  39 loss: 0.09965 training acc: 0.97160 testing acc: 0.9632\n",
      "epoch: 7 iter:  40 loss: 0.09961 training acc: 0.97114 testing acc: 0.9628\n",
      "epoch: 7 iter:  41 loss: 0.09675 training acc: 0.97204 testing acc: 0.962\n",
      "epoch: 7 iter:  42 loss: 0.09624 training acc: 0.97264 testing acc: 0.9642\n",
      "epoch: 7 iter:  43 loss: 0.09479 training acc: 0.97312 testing acc: 0.9636\n",
      "epoch: 7 iter:  44 loss: 0.09461 training acc: 0.97308 testing acc: 0.964\n",
      "epoch: 7 iter:  45 loss: 0.09262 training acc: 0.97356 testing acc: 0.9638\n",
      "epoch: 7 iter:  46 loss: 0.09787 training acc: 0.97112 testing acc: 0.9618\n",
      "epoch: 7 iter:  47 loss: 0.09548 training acc: 0.97208 testing acc: 0.963\n",
      "epoch: 7 iter:  48 loss: 0.09553 training acc: 0.97276 testing acc: 0.9626\n",
      "epoch: 7 iter:  49 loss: 0.09208 training acc: 0.97348 testing acc: 0.9638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 iter:  50 loss: 0.09200 training acc: 0.97358 testing acc: 0.9638\n",
      "epoch: 7 iter:  51 loss: 0.09121 training acc: 0.97434 testing acc: 0.9634\n",
      "epoch: 7 iter:  52 loss: 0.09147 training acc: 0.97416 testing acc: 0.9646\n",
      "epoch: 7 iter:  53 loss: 0.09089 training acc: 0.97440 testing acc: 0.9652\n",
      "epoch: 7 iter:  54 loss: 0.09491 training acc: 0.97352 testing acc: 0.9654\n",
      "epoch: 7 iter:  55 loss: 0.09166 training acc: 0.97446 testing acc: 0.9638\n",
      "epoch: 7 iter:  56 loss: 0.09851 training acc: 0.97170 testing acc: 0.9616\n",
      "epoch: 7 iter:  57 loss: 0.10743 training acc: 0.96892 testing acc: 0.9578\n",
      "epoch: 7 iter:  58 loss: 0.11240 training acc: 0.96746 testing acc: 0.9578\n",
      "epoch: 7 iter:  59 loss: 0.10403 training acc: 0.97092 testing acc: 0.961\n",
      "epoch: 7 iter:  60 loss: 0.10109 training acc: 0.97182 testing acc: 0.9624\n",
      "epoch: 7 iter:  61 loss: 0.09947 training acc: 0.97214 testing acc: 0.9624\n",
      "epoch: 7 iter:  62 loss: 0.09753 training acc: 0.97288 testing acc: 0.9634\n",
      "epoch: 7 iter:  63 loss: 0.09543 training acc: 0.97254 testing acc: 0.9624\n",
      "epoch: 7 iter:  64 loss: 0.09548 training acc: 0.97246 testing acc: 0.9628\n",
      "epoch: 7 iter:  65 loss: 0.09564 training acc: 0.97244 testing acc: 0.9618\n",
      "epoch: 7 iter:  66 loss: 0.09449 training acc: 0.97288 testing acc: 0.966\n",
      "epoch: 7 iter:  67 loss: 0.09388 training acc: 0.97312 testing acc: 0.966\n",
      "epoch: 7 iter:  68 loss: 0.09395 training acc: 0.97272 testing acc: 0.9654\n",
      "epoch: 7 iter:  69 loss: 0.10156 training acc: 0.97010 testing acc: 0.96\n",
      "epoch: 7 iter:  70 loss: 0.09937 training acc: 0.97140 testing acc: 0.9618\n",
      "epoch: 7 iter:  71 loss: 0.09848 training acc: 0.97142 testing acc: 0.9628\n",
      "epoch: 7 iter:  72 loss: 0.09593 training acc: 0.97228 testing acc: 0.9636\n",
      "epoch: 7 iter:  73 loss: 0.09436 training acc: 0.97296 testing acc: 0.964\n",
      "epoch: 7 iter:  74 loss: 0.09577 training acc: 0.97230 testing acc: 0.9616\n",
      "epoch: 7 iter:  75 loss: 0.09589 training acc: 0.97250 testing acc: 0.9626\n",
      "epoch: 7 iter:  76 loss: 0.09732 training acc: 0.97188 testing acc: 0.9628\n",
      "epoch: 7 iter:  77 loss: 0.09725 training acc: 0.97176 testing acc: 0.9628\n",
      "epoch: 7 iter:  78 loss: 0.09600 training acc: 0.97220 testing acc: 0.963\n",
      "epoch: 7 iter:  79 loss: 0.09677 training acc: 0.97202 testing acc: 0.9612\n",
      "epoch: 7 iter:  80 loss: 0.10380 training acc: 0.97026 testing acc: 0.9584\n",
      "epoch: 7 iter:  81 loss: 0.09878 training acc: 0.97140 testing acc: 0.959\n",
      "epoch: 7 iter:  82 loss: 0.09901 training acc: 0.97104 testing acc: 0.9602\n",
      "epoch: 7 iter:  83 loss: 0.09699 training acc: 0.97138 testing acc: 0.9624\n",
      "epoch: 7 iter:  84 loss: 0.09315 training acc: 0.97272 testing acc: 0.9632\n",
      "epoch: 7 iter:  85 loss: 0.09175 training acc: 0.97308 testing acc: 0.9626\n",
      "epoch: 7 iter:  86 loss: 0.09639 training acc: 0.97188 testing acc: 0.9622\n",
      "epoch: 7 iter:  87 loss: 0.09884 training acc: 0.97080 testing acc: 0.9608\n",
      "epoch: 7 iter:  88 loss: 0.09916 training acc: 0.97062 testing acc: 0.9618\n",
      "epoch: 7 iter:  89 loss: 0.09640 training acc: 0.97168 testing acc: 0.9618\n",
      "epoch: 7 iter:  90 loss: 0.10288 training acc: 0.96958 testing acc: 0.9602\n",
      "epoch: 7 iter:  91 loss: 0.09462 training acc: 0.97234 testing acc: 0.9614\n",
      "epoch: 7 iter:  92 loss: 0.09710 training acc: 0.97078 testing acc: 0.962\n",
      "epoch: 7 iter:  93 loss: 0.09727 training acc: 0.97052 testing acc: 0.9618\n",
      "epoch: 7 iter:  94 loss: 0.11073 training acc: 0.96618 testing acc: 0.9566\n",
      "epoch: 7 iter:  95 loss: 0.10684 training acc: 0.96716 testing acc: 0.9584\n",
      "epoch: 7 iter:  96 loss: 0.09701 training acc: 0.97144 testing acc: 0.9616\n",
      "epoch: 7 iter:  97 loss: 0.09365 training acc: 0.97232 testing acc: 0.9642\n",
      "epoch: 7 iter:  98 loss: 0.09750 training acc: 0.97154 testing acc: 0.9604\n",
      "epoch: 7 iter:  99 loss: 0.10925 training acc: 0.96826 testing acc: 0.9594\n",
      "epoch: 7 iter: 100 loss: 0.10296 training acc: 0.97022 testing acc: 0.9612\n",
      "epoch: 7 iter: 101 loss: 0.09701 training acc: 0.97206 testing acc: 0.9632\n",
      "epoch: 7 iter: 102 loss: 0.09555 training acc: 0.97272 testing acc: 0.9638\n",
      "epoch: 7 iter: 103 loss: 0.09389 training acc: 0.97336 testing acc: 0.9642\n",
      "epoch: 7 iter: 104 loss: 0.09956 training acc: 0.97026 testing acc: 0.962\n",
      "epoch: 7 iter: 105 loss: 0.09287 training acc: 0.97346 testing acc: 0.9646\n",
      "epoch: 7 iter: 106 loss: 0.09877 training acc: 0.97060 testing acc: 0.9602\n",
      "epoch: 7 iter: 107 loss: 0.09696 training acc: 0.97188 testing acc: 0.9612\n",
      "epoch: 7 iter: 108 loss: 0.09648 training acc: 0.97236 testing acc: 0.9626\n",
      "epoch: 7 iter: 109 loss: 0.09533 training acc: 0.97224 testing acc: 0.962\n",
      "epoch: 7 iter: 110 loss: 0.09646 training acc: 0.97178 testing acc: 0.961\n",
      "epoch: 7 iter: 111 loss: 0.10895 training acc: 0.96856 testing acc: 0.961\n",
      "epoch: 7 iter: 112 loss: 0.10581 training acc: 0.96942 testing acc: 0.962\n",
      "epoch: 7 iter: 113 loss: 0.10474 training acc: 0.96996 testing acc: 0.9616\n",
      "epoch: 7 iter: 114 loss: 0.12049 training acc: 0.96380 testing acc: 0.956\n",
      "epoch: 7 iter: 115 loss: 0.11798 training acc: 0.96354 testing acc: 0.9542\n",
      "epoch: 7 iter: 116 loss: 0.10900 training acc: 0.96732 testing acc: 0.9606\n",
      "epoch: 7 iter: 117 loss: 0.11138 training acc: 0.96550 testing acc: 0.9592\n",
      "epoch: 7 iter: 118 loss: 0.10961 training acc: 0.96620 testing acc: 0.9588\n",
      "epoch: 7 iter: 119 loss: 0.10159 training acc: 0.97086 testing acc: 0.9612\n",
      "epoch: 7 iter: 120 loss: 0.09893 training acc: 0.97104 testing acc: 0.961\n",
      "epoch: 7 iter: 121 loss: 0.10398 training acc: 0.96992 testing acc: 0.958\n",
      "epoch: 7 iter: 122 loss: 0.10015 training acc: 0.97066 testing acc: 0.9598\n",
      "epoch: 7 iter: 123 loss: 0.09858 training acc: 0.97176 testing acc: 0.9598\n",
      "epoch: 7 iter: 124 loss: 0.10241 training acc: 0.97058 testing acc: 0.9588\n",
      "epoch: 7 iter: 125 loss: 0.09510 training acc: 0.97278 testing acc: 0.9634\n",
      "epoch: 7 iter: 126 loss: 0.10676 training acc: 0.96796 testing acc: 0.9576\n",
      "epoch: 7 iter: 127 loss: 0.12790 training acc: 0.95862 testing acc: 0.9538\n",
      "epoch: 7 iter: 128 loss: 0.10177 training acc: 0.96914 testing acc: 0.9618\n",
      "epoch: 7 iter: 129 loss: 0.10096 training acc: 0.96972 testing acc: 0.9636\n",
      "epoch: 7 iter: 130 loss: 0.09585 training acc: 0.97234 testing acc: 0.9634\n",
      "epoch: 7 iter: 131 loss: 0.09654 training acc: 0.97152 testing acc: 0.9638\n",
      "epoch: 7 iter: 132 loss: 0.10406 training acc: 0.96904 testing acc: 0.961\n",
      "epoch: 7 iter: 133 loss: 0.10374 training acc: 0.96868 testing acc: 0.9612\n",
      "epoch: 7 iter: 134 loss: 0.11830 training acc: 0.96428 testing acc: 0.9576\n",
      "epoch: 7 iter: 135 loss: 0.10472 training acc: 0.96838 testing acc: 0.9618\n",
      "epoch: 7 iter: 136 loss: 0.10018 training acc: 0.96994 testing acc: 0.9636\n",
      "epoch: 7 iter: 137 loss: 0.10330 training acc: 0.96908 testing acc: 0.9598\n",
      "epoch: 7 iter: 138 loss: 0.09834 training acc: 0.97092 testing acc: 0.962\n",
      "epoch: 7 iter: 139 loss: 0.09550 training acc: 0.97116 testing acc: 0.9618\n",
      "epoch: 7 iter: 140 loss: 0.09461 training acc: 0.97226 testing acc: 0.963\n",
      "epoch: 7 iter: 141 loss: 0.10011 training acc: 0.96992 testing acc: 0.961\n",
      "epoch: 7 iter: 142 loss: 0.09497 training acc: 0.97174 testing acc: 0.963\n",
      "epoch: 7 iter: 143 loss: 0.08882 training acc: 0.97462 testing acc: 0.965\n",
      "epoch: 7 iter: 144 loss: 0.09124 training acc: 0.97402 testing acc: 0.9654\n",
      "epoch: 7 iter: 145 loss: 0.09212 training acc: 0.97372 testing acc: 0.9646\n",
      "epoch: 7 iter: 146 loss: 0.09006 training acc: 0.97436 testing acc: 0.966\n",
      "epoch: 7 iter: 147 loss: 0.09318 training acc: 0.97320 testing acc: 0.965\n",
      "epoch: 7 iter: 148 loss: 0.09231 training acc: 0.97342 testing acc: 0.9646\n",
      "epoch: 7 iter: 149 loss: 0.09172 training acc: 0.97396 testing acc: 0.9646\n",
      "epoch: 7 iter: 150 loss: 0.09281 training acc: 0.97344 testing acc: 0.9654\n",
      "epoch: 7 iter: 151 loss: 0.10124 training acc: 0.97060 testing acc: 0.9634\n",
      "epoch: 7 iter: 152 loss: 0.08993 training acc: 0.97400 testing acc: 0.9664\n",
      "epoch: 7 iter: 153 loss: 0.08955 training acc: 0.97382 testing acc: 0.9672\n",
      "epoch: 7 iter: 154 loss: 0.09725 training acc: 0.97108 testing acc: 0.964\n",
      "epoch: 7 iter: 155 loss: 0.09080 training acc: 0.97430 testing acc: 0.9666\n",
      "epoch: 7 iter: 156 loss: 0.09582 training acc: 0.97198 testing acc: 0.9636\n",
      "epoch: 7 iter: 157 loss: 0.09682 training acc: 0.97160 testing acc: 0.9618\n",
      "epoch: 7 iter: 158 loss: 0.09519 training acc: 0.97202 testing acc: 0.9658\n",
      "epoch: 7 iter: 159 loss: 0.09849 training acc: 0.97022 testing acc: 0.9614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 iter: 160 loss: 0.11263 training acc: 0.96624 testing acc: 0.953\n",
      "epoch: 7 iter: 161 loss: 0.09370 training acc: 0.97248 testing acc: 0.964\n",
      "epoch: 7 iter: 162 loss: 0.09537 training acc: 0.97180 testing acc: 0.9626\n",
      "epoch: 7 iter: 163 loss: 0.09981 training acc: 0.97002 testing acc: 0.9604\n",
      "epoch: 7 iter: 164 loss: 0.09687 training acc: 0.97120 testing acc: 0.9614\n",
      "epoch: 7 iter: 165 loss: 0.09794 training acc: 0.97054 testing acc: 0.9598\n",
      "epoch: 7 iter: 166 loss: 0.09148 training acc: 0.97344 testing acc: 0.9638\n",
      "epoch: 7 iter: 167 loss: 0.08925 training acc: 0.97442 testing acc: 0.9652\n",
      "epoch: 7 iter: 168 loss: 0.08834 training acc: 0.97474 testing acc: 0.9654\n",
      "epoch: 7 iter: 169 loss: 0.09544 training acc: 0.97162 testing acc: 0.9622\n",
      "epoch: 7 iter: 170 loss: 0.10748 training acc: 0.96672 testing acc: 0.9562\n",
      "epoch: 7 iter: 171 loss: 0.09688 training acc: 0.97080 testing acc: 0.9616\n",
      "epoch: 7 iter: 172 loss: 0.09246 training acc: 0.97268 testing acc: 0.9656\n",
      "epoch: 7 iter: 173 loss: 0.09502 training acc: 0.97176 testing acc: 0.9626\n",
      "epoch: 7 iter: 174 loss: 0.09857 training acc: 0.97066 testing acc: 0.9608\n",
      "epoch: 7 iter: 175 loss: 0.10603 training acc: 0.96774 testing acc: 0.961\n",
      "epoch: 7 iter: 176 loss: 0.09684 training acc: 0.97196 testing acc: 0.965\n",
      "epoch: 7 iter: 177 loss: 0.09676 training acc: 0.97210 testing acc: 0.9648\n",
      "epoch: 7 iter: 178 loss: 0.10730 training acc: 0.96912 testing acc: 0.961\n",
      "epoch: 7 iter: 179 loss: 0.13905 training acc: 0.95598 testing acc: 0.9514\n",
      "epoch: 7 iter: 180 loss: 0.10042 training acc: 0.97068 testing acc: 0.9596\n",
      "epoch: 7 iter: 181 loss: 0.10705 training acc: 0.96808 testing acc: 0.9584\n",
      "epoch: 7 iter: 182 loss: 0.09570 training acc: 0.97248 testing acc: 0.9622\n",
      "epoch: 7 iter: 183 loss: 0.09307 training acc: 0.97330 testing acc: 0.9632\n",
      "epoch: 7 iter: 184 loss: 0.09476 training acc: 0.97262 testing acc: 0.9622\n",
      "epoch: 7 iter: 185 loss: 0.09489 training acc: 0.97236 testing acc: 0.9612\n",
      "epoch: 7 iter: 186 loss: 0.09431 training acc: 0.97260 testing acc: 0.9624\n",
      "epoch: 7 iter: 187 loss: 0.09220 training acc: 0.97360 testing acc: 0.9614\n",
      "epoch: 7 iter: 188 loss: 0.10527 training acc: 0.96972 testing acc: 0.9588\n",
      "epoch: 7 iter: 189 loss: 0.10073 training acc: 0.97046 testing acc: 0.9598\n",
      "epoch: 7 iter: 190 loss: 0.09999 training acc: 0.97094 testing acc: 0.96\n",
      "epoch: 7 iter: 191 loss: 0.10915 training acc: 0.96730 testing acc: 0.959\n",
      "epoch: 7 iter: 192 loss: 0.09872 training acc: 0.97118 testing acc: 0.96\n",
      "epoch: 7 iter: 193 loss: 0.09867 training acc: 0.97194 testing acc: 0.9608\n",
      "epoch: 7 iter: 194 loss: 0.09912 training acc: 0.97146 testing acc: 0.9598\n",
      "epoch: 7 iter: 195 loss: 0.09633 training acc: 0.97250 testing acc: 0.9606\n",
      "epoch: 7 iter: 196 loss: 0.09337 training acc: 0.97278 testing acc: 0.9606\n",
      "epoch: 7 iter: 197 loss: 0.10917 training acc: 0.96650 testing acc: 0.9536\n",
      "epoch: 7 iter: 198 loss: 0.10843 training acc: 0.96664 testing acc: 0.954\n",
      "epoch: 7 iter: 199 loss: 0.09772 training acc: 0.97066 testing acc: 0.9584\n",
      "epoch: 7 iter: 200 loss: 0.09980 training acc: 0.97042 testing acc: 0.9564\n",
      "epoch: 7 iter: 201 loss: 0.12274 training acc: 0.96116 testing acc: 0.9496\n",
      "epoch: 7 iter: 202 loss: 0.12204 training acc: 0.96124 testing acc: 0.9502\n",
      "epoch: 7 iter: 203 loss: 0.11350 training acc: 0.96502 testing acc: 0.9548\n",
      "epoch: 7 iter: 204 loss: 0.09199 training acc: 0.97324 testing acc: 0.961\n",
      "epoch: 7 iter: 205 loss: 0.10032 training acc: 0.96980 testing acc: 0.9578\n",
      "epoch: 7 iter: 206 loss: 0.09177 training acc: 0.97400 testing acc: 0.9614\n",
      "epoch: 7 iter: 207 loss: 0.09345 training acc: 0.97320 testing acc: 0.9608\n",
      "epoch: 7 iter: 208 loss: 0.09066 training acc: 0.97406 testing acc: 0.9626\n",
      "epoch: 7 iter: 209 loss: 0.09021 training acc: 0.97410 testing acc: 0.9626\n",
      "epoch: 7 iter: 210 loss: 0.09245 training acc: 0.97320 testing acc: 0.9622\n",
      "epoch: 7 iter: 211 loss: 0.09092 training acc: 0.97378 testing acc: 0.9626\n",
      "epoch: 7 iter: 212 loss: 0.09000 training acc: 0.97388 testing acc: 0.9654\n",
      "epoch: 7 iter: 213 loss: 0.10282 training acc: 0.96966 testing acc: 0.96\n",
      "epoch: 7 iter: 214 loss: 0.09351 training acc: 0.97272 testing acc: 0.9642\n",
      "epoch: 7 iter: 215 loss: 0.09892 training acc: 0.97018 testing acc: 0.9624\n",
      "epoch: 7 iter: 216 loss: 0.09915 training acc: 0.97052 testing acc: 0.9604\n",
      "epoch: 7 iter: 217 loss: 0.11509 training acc: 0.96500 testing acc: 0.9558\n",
      "epoch: 7 iter: 218 loss: 0.10191 training acc: 0.97024 testing acc: 0.9586\n",
      "epoch: 7 iter: 219 loss: 0.09631 training acc: 0.97254 testing acc: 0.9616\n",
      "epoch: 7 iter: 220 loss: 0.09684 training acc: 0.97218 testing acc: 0.9612\n",
      "epoch: 7 iter: 221 loss: 0.09888 training acc: 0.97054 testing acc: 0.9622\n",
      "epoch: 7 iter: 222 loss: 0.09636 training acc: 0.97194 testing acc: 0.9626\n",
      "epoch: 7 iter: 223 loss: 0.09591 training acc: 0.97230 testing acc: 0.9626\n",
      "epoch: 7 iter: 224 loss: 0.09472 training acc: 0.97292 testing acc: 0.9648\n",
      "epoch: 7 iter: 225 loss: 0.09869 training acc: 0.97218 testing acc: 0.9628\n",
      "epoch: 7 iter: 226 loss: 0.09723 training acc: 0.97216 testing acc: 0.9632\n",
      "epoch: 7 iter: 227 loss: 0.10009 training acc: 0.97080 testing acc: 0.9626\n",
      "epoch: 7 iter: 228 loss: 0.09289 training acc: 0.97324 testing acc: 0.9652\n",
      "epoch: 7 iter: 229 loss: 0.09685 training acc: 0.97142 testing acc: 0.965\n",
      "epoch: 7 iter: 230 loss: 0.09065 training acc: 0.97360 testing acc: 0.9654\n",
      "epoch: 7 iter: 231 loss: 0.08849 training acc: 0.97458 testing acc: 0.9644\n",
      "epoch: 7 iter: 232 loss: 0.09654 training acc: 0.97148 testing acc: 0.9626\n",
      "epoch: 7 iter: 233 loss: 0.09637 training acc: 0.97178 testing acc: 0.9626\n",
      "epoch: 7 iter: 234 loss: 0.10302 training acc: 0.96866 testing acc: 0.96\n",
      "epoch: 7 iter: 235 loss: 0.10161 training acc: 0.96948 testing acc: 0.9612\n",
      "epoch: 7 iter: 236 loss: 0.09917 training acc: 0.97074 testing acc: 0.9622\n",
      "epoch: 7 iter: 237 loss: 0.09563 training acc: 0.97172 testing acc: 0.9644\n",
      "epoch: 7 iter: 238 loss: 0.09505 training acc: 0.97276 testing acc: 0.966\n",
      "epoch: 7 iter: 239 loss: 0.10672 training acc: 0.96868 testing acc: 0.9614\n",
      "epoch: 7 iter: 240 loss: 0.10230 training acc: 0.97018 testing acc: 0.9616\n",
      "epoch: 7 iter: 241 loss: 0.09828 training acc: 0.97152 testing acc: 0.963\n",
      "epoch: 7 iter: 242 loss: 0.09502 training acc: 0.97250 testing acc: 0.9638\n",
      "epoch: 7 iter: 243 loss: 0.11528 training acc: 0.96486 testing acc: 0.9566\n",
      "epoch: 7 iter: 244 loss: 0.10869 training acc: 0.96680 testing acc: 0.9586\n",
      "epoch: 7 iter: 245 loss: 0.10072 training acc: 0.96976 testing acc: 0.9626\n",
      "epoch: 7 iter: 246 loss: 0.10074 training acc: 0.96950 testing acc: 0.9626\n",
      "epoch: 7 iter: 247 loss: 0.10081 training acc: 0.96964 testing acc: 0.963\n",
      "epoch: 7 iter: 248 loss: 0.10437 training acc: 0.96856 testing acc: 0.9622\n",
      "epoch: 7 iter: 249 loss: 0.10706 training acc: 0.96722 testing acc: 0.9608\n",
      "epoch: 7 iter: 250 loss: 0.09758 training acc: 0.96984 testing acc: 0.9616\n",
      "epoch: 7 iter: 251 loss: 0.12501 training acc: 0.96062 testing acc: 0.9506\n",
      "epoch: 7 iter: 252 loss: 0.12319 training acc: 0.96200 testing acc: 0.9542\n",
      "epoch: 7 iter: 253 loss: 0.11017 training acc: 0.96610 testing acc: 0.9594\n",
      "epoch: 7 iter: 254 loss: 0.10444 training acc: 0.96834 testing acc: 0.9596\n",
      "epoch: 7 iter: 255 loss: 0.10413 training acc: 0.96822 testing acc: 0.9578\n",
      "epoch: 7 iter: 256 loss: 0.10860 training acc: 0.96628 testing acc: 0.9584\n",
      "epoch: 7 iter: 257 loss: 0.10268 training acc: 0.96868 testing acc: 0.9606\n",
      "epoch: 7 iter: 258 loss: 0.10452 training acc: 0.96920 testing acc: 0.96\n",
      "epoch: 7 iter: 259 loss: 0.10065 training acc: 0.96964 testing acc: 0.9608\n",
      "epoch: 7 iter: 260 loss: 0.10801 training acc: 0.96748 testing acc: 0.9598\n",
      "epoch: 7 iter: 261 loss: 0.10527 training acc: 0.96828 testing acc: 0.9612\n",
      "epoch: 7 iter: 262 loss: 0.09682 training acc: 0.97128 testing acc: 0.9638\n",
      "epoch: 7 iter: 263 loss: 0.09527 training acc: 0.97154 testing acc: 0.9636\n",
      "epoch: 7 iter: 264 loss: 0.10061 training acc: 0.97002 testing acc: 0.962\n",
      "epoch: 7 iter: 265 loss: 0.10324 training acc: 0.96894 testing acc: 0.9608\n",
      "epoch: 7 iter: 266 loss: 0.09506 training acc: 0.97168 testing acc: 0.9644\n",
      "epoch: 7 iter: 267 loss: 0.09430 training acc: 0.97196 testing acc: 0.965\n",
      "epoch: 7 iter: 268 loss: 0.09204 training acc: 0.97302 testing acc: 0.9658\n",
      "epoch: 7 iter: 269 loss: 0.09176 training acc: 0.97302 testing acc: 0.9658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 iter: 270 loss: 0.09304 training acc: 0.97236 testing acc: 0.9646\n",
      "epoch: 7 iter: 271 loss: 0.09928 training acc: 0.97098 testing acc: 0.9614\n",
      "epoch: 7 iter: 272 loss: 0.09773 training acc: 0.97204 testing acc: 0.9634\n",
      "epoch: 7 iter: 273 loss: 0.10002 training acc: 0.97012 testing acc: 0.9638\n",
      "epoch: 7 iter: 274 loss: 0.09176 training acc: 0.97276 testing acc: 0.9644\n",
      "epoch: 7 iter: 275 loss: 0.11106 training acc: 0.96686 testing acc: 0.9594\n",
      "epoch: 7 iter: 276 loss: 0.10041 training acc: 0.97000 testing acc: 0.9616\n",
      "epoch: 7 iter: 277 loss: 0.09570 training acc: 0.97200 testing acc: 0.9634\n",
      "epoch: 7 iter: 278 loss: 0.09264 training acc: 0.97334 testing acc: 0.9646\n",
      "epoch: 7 iter: 279 loss: 0.09587 training acc: 0.97236 testing acc: 0.9646\n",
      "epoch: 7 iter: 280 loss: 0.09982 training acc: 0.97074 testing acc: 0.9626\n",
      "epoch: 7 iter: 281 loss: 0.09104 training acc: 0.97318 testing acc: 0.9642\n",
      "epoch: 7 iter: 282 loss: 0.09439 training acc: 0.97164 testing acc: 0.965\n",
      "epoch: 7 iter: 283 loss: 0.09952 training acc: 0.97018 testing acc: 0.962\n",
      "epoch: 7 iter: 284 loss: 0.10369 training acc: 0.96884 testing acc: 0.961\n",
      "epoch: 7 iter: 285 loss: 0.10246 training acc: 0.96950 testing acc: 0.9612\n",
      "epoch: 7 iter: 286 loss: 0.09952 training acc: 0.97030 testing acc: 0.9602\n",
      "epoch: 7 iter: 287 loss: 0.09951 training acc: 0.97040 testing acc: 0.9622\n",
      "epoch: 7 iter: 288 loss: 0.08957 training acc: 0.97396 testing acc: 0.965\n",
      "epoch: 7 iter: 289 loss: 0.09422 training acc: 0.97242 testing acc: 0.9642\n",
      "epoch: 7 iter: 290 loss: 0.09415 training acc: 0.97330 testing acc: 0.9628\n",
      "epoch: 7 iter: 291 loss: 0.09314 training acc: 0.97314 testing acc: 0.964\n",
      "epoch: 7 iter: 292 loss: 0.09266 training acc: 0.97298 testing acc: 0.9644\n",
      "epoch: 7 iter: 293 loss: 0.09193 training acc: 0.97318 testing acc: 0.9652\n",
      "epoch: 7 iter: 294 loss: 0.09232 training acc: 0.97326 testing acc: 0.9644\n",
      "epoch: 7 iter: 295 loss: 0.09440 training acc: 0.97256 testing acc: 0.9638\n",
      "epoch: 7 iter: 296 loss: 0.09285 training acc: 0.97284 testing acc: 0.9638\n",
      "epoch: 7 iter: 297 loss: 0.10303 training acc: 0.96946 testing acc: 0.9626\n",
      "epoch: 7 iter: 298 loss: 0.09749 training acc: 0.97170 testing acc: 0.9614\n",
      "epoch: 7 iter: 299 loss: 0.10466 training acc: 0.96832 testing acc: 0.9606\n",
      "epoch: 7 iter: 300 loss: 0.09902 training acc: 0.97058 testing acc: 0.9606\n",
      "epoch: 7 iter: 301 loss: 0.09562 training acc: 0.97240 testing acc: 0.9624\n",
      "epoch: 7 iter: 302 loss: 0.09568 training acc: 0.97232 testing acc: 0.9624\n",
      "epoch: 7 iter: 303 loss: 0.11812 training acc: 0.96542 testing acc: 0.9546\n",
      "epoch: 7 iter: 304 loss: 0.11390 training acc: 0.96670 testing acc: 0.9556\n",
      "epoch: 7 iter: 305 loss: 0.09424 training acc: 0.97336 testing acc: 0.9648\n",
      "epoch: 7 iter: 306 loss: 0.09459 training acc: 0.97362 testing acc: 0.9636\n",
      "epoch: 7 iter: 307 loss: 0.09104 training acc: 0.97434 testing acc: 0.965\n",
      "epoch: 7 iter: 308 loss: 0.10176 training acc: 0.96996 testing acc: 0.9628\n",
      "epoch: 7 iter: 309 loss: 0.09837 training acc: 0.97148 testing acc: 0.9634\n",
      "epoch: 7 iter: 310 loss: 0.09554 training acc: 0.97234 testing acc: 0.9638\n",
      "epoch: 7 iter: 311 loss: 0.09733 training acc: 0.97152 testing acc: 0.9626\n",
      "epoch: 7 iter: 312 loss: 0.08770 training acc: 0.97536 testing acc: 0.9652\n",
      "epoch: 7 iter: 313 loss: 0.10689 training acc: 0.96832 testing acc: 0.9576\n",
      "epoch: 7 iter: 314 loss: 0.09676 training acc: 0.97228 testing acc: 0.961\n",
      "epoch: 7 iter: 315 loss: 0.09462 training acc: 0.97346 testing acc: 0.9632\n",
      "epoch: 7 iter: 316 loss: 0.09022 training acc: 0.97482 testing acc: 0.9646\n",
      "epoch: 7 iter: 317 loss: 0.08984 training acc: 0.97422 testing acc: 0.9642\n",
      "epoch: 7 iter: 318 loss: 0.09785 training acc: 0.97170 testing acc: 0.964\n",
      "epoch: 7 iter: 319 loss: 0.08991 training acc: 0.97468 testing acc: 0.9644\n",
      "epoch: 7 iter: 320 loss: 0.09325 training acc: 0.97332 testing acc: 0.9648\n",
      "epoch: 7 iter: 321 loss: 0.09598 training acc: 0.97284 testing acc: 0.963\n",
      "epoch: 7 iter: 322 loss: 0.09360 training acc: 0.97324 testing acc: 0.9646\n",
      "epoch: 7 iter: 323 loss: 0.09295 training acc: 0.97278 testing acc: 0.963\n",
      "epoch: 7 iter: 324 loss: 0.09887 training acc: 0.97066 testing acc: 0.9606\n",
      "epoch: 7 iter: 325 loss: 0.09025 training acc: 0.97388 testing acc: 0.9628\n",
      "epoch: 7 iter: 326 loss: 0.09019 training acc: 0.97426 testing acc: 0.9642\n",
      "epoch: 7 iter: 327 loss: 0.08857 training acc: 0.97438 testing acc: 0.9642\n",
      "epoch: 7 iter: 328 loss: 0.08873 training acc: 0.97430 testing acc: 0.9646\n",
      "epoch: 7 iter: 329 loss: 0.08818 training acc: 0.97474 testing acc: 0.9644\n",
      "epoch: 7 iter: 330 loss: 0.09288 training acc: 0.97230 testing acc: 0.9626\n",
      "epoch: 7 iter: 331 loss: 0.09111 training acc: 0.97296 testing acc: 0.9626\n",
      "epoch: 7 iter: 332 loss: 0.09157 training acc: 0.97406 testing acc: 0.9644\n",
      "epoch: 7 iter: 333 loss: 0.09055 training acc: 0.97414 testing acc: 0.9652\n",
      "epoch: 7 iter: 334 loss: 0.08781 training acc: 0.97494 testing acc: 0.9658\n",
      "epoch: 7 iter: 335 loss: 0.10702 training acc: 0.96702 testing acc: 0.957\n",
      "epoch: 7 iter: 336 loss: 0.09415 training acc: 0.97174 testing acc: 0.9622\n",
      "epoch: 7 iter: 337 loss: 0.09486 training acc: 0.97188 testing acc: 0.963\n",
      "epoch: 7 iter: 338 loss: 0.09224 training acc: 0.97266 testing acc: 0.9618\n",
      "epoch: 7 iter: 339 loss: 0.08826 training acc: 0.97410 testing acc: 0.9662\n",
      "epoch: 7 iter: 340 loss: 0.08993 training acc: 0.97400 testing acc: 0.9644\n",
      "epoch: 7 iter: 341 loss: 0.08685 training acc: 0.97450 testing acc: 0.9636\n",
      "epoch: 7 iter: 342 loss: 0.08709 training acc: 0.97528 testing acc: 0.964\n",
      "epoch: 7 iter: 343 loss: 0.08362 training acc: 0.97616 testing acc: 0.9662\n",
      "epoch: 7 iter: 344 loss: 0.11747 training acc: 0.96560 testing acc: 0.9558\n",
      "epoch: 7 iter: 345 loss: 0.10667 training acc: 0.96904 testing acc: 0.9594\n",
      "epoch: 7 iter: 346 loss: 0.09770 training acc: 0.97202 testing acc: 0.9624\n",
      "epoch: 7 iter: 347 loss: 0.12029 training acc: 0.96362 testing acc: 0.9542\n",
      "epoch: 7 iter: 348 loss: 0.10234 training acc: 0.96990 testing acc: 0.9602\n",
      "epoch: 7 iter: 349 loss: 0.09582 training acc: 0.97260 testing acc: 0.9634\n",
      "epoch: 7 iter: 350 loss: 0.09505 training acc: 0.97250 testing acc: 0.963\n",
      "epoch: 7 iter: 351 loss: 0.09675 training acc: 0.97178 testing acc: 0.9642\n",
      "epoch: 7 iter: 352 loss: 0.09750 training acc: 0.97184 testing acc: 0.9642\n",
      "epoch: 7 iter: 353 loss: 0.09468 training acc: 0.97258 testing acc: 0.9646\n",
      "epoch: 7 iter: 354 loss: 0.09402 training acc: 0.97268 testing acc: 0.9644\n",
      "epoch: 7 iter: 355 loss: 0.09430 training acc: 0.97218 testing acc: 0.9648\n",
      "epoch: 7 iter: 356 loss: 0.09705 training acc: 0.97198 testing acc: 0.9634\n",
      "epoch: 7 iter: 357 loss: 0.10258 training acc: 0.96982 testing acc: 0.9594\n",
      "epoch: 7 iter: 358 loss: 0.08955 training acc: 0.97436 testing acc: 0.964\n",
      "epoch: 7 iter: 359 loss: 0.08861 training acc: 0.97478 testing acc: 0.9646\n",
      "epoch: 7 iter: 360 loss: 0.09159 training acc: 0.97346 testing acc: 0.9642\n",
      "epoch: 7 iter: 361 loss: 0.08717 training acc: 0.97480 testing acc: 0.966\n",
      "epoch: 7 iter: 362 loss: 0.08833 training acc: 0.97474 testing acc: 0.9648\n",
      "epoch: 7 iter: 363 loss: 0.09636 training acc: 0.97060 testing acc: 0.9616\n",
      "epoch: 7 iter: 364 loss: 0.09771 training acc: 0.97070 testing acc: 0.9614\n",
      "epoch: 7 iter: 365 loss: 0.09937 training acc: 0.97050 testing acc: 0.9626\n",
      "epoch: 7 iter: 366 loss: 0.08762 training acc: 0.97468 testing acc: 0.9662\n",
      "epoch: 7 iter: 367 loss: 0.10500 training acc: 0.96774 testing acc: 0.9584\n",
      "epoch: 7 iter: 368 loss: 0.11939 training acc: 0.96282 testing acc: 0.9534\n",
      "epoch: 7 iter: 369 loss: 0.10635 training acc: 0.96698 testing acc: 0.9586\n",
      "epoch: 7 iter: 370 loss: 0.10978 training acc: 0.96644 testing acc: 0.9574\n",
      "epoch: 7 iter: 371 loss: 0.09367 training acc: 0.97244 testing acc: 0.9628\n",
      "epoch: 7 iter: 372 loss: 0.09557 training acc: 0.97100 testing acc: 0.963\n",
      "epoch: 7 iter: 373 loss: 0.09951 training acc: 0.96970 testing acc: 0.9604\n",
      "epoch: 7 iter: 374 loss: 0.10019 training acc: 0.97028 testing acc: 0.9612\n",
      "epoch: 7 iter: 375 loss: 0.09792 training acc: 0.97074 testing acc: 0.9618\n",
      "epoch: 7 iter: 376 loss: 0.09608 training acc: 0.97158 testing acc: 0.9632\n",
      "epoch: 7 iter: 377 loss: 0.09560 training acc: 0.97168 testing acc: 0.963\n",
      "epoch: 7 iter: 378 loss: 0.09747 training acc: 0.97128 testing acc: 0.9622\n",
      "epoch: 7 iter: 379 loss: 0.10520 training acc: 0.96864 testing acc: 0.9584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 iter: 380 loss: 0.09515 training acc: 0.97184 testing acc: 0.9618\n",
      "epoch: 7 iter: 381 loss: 0.09610 training acc: 0.97116 testing acc: 0.9608\n",
      "epoch: 7 iter: 382 loss: 0.09427 training acc: 0.97212 testing acc: 0.9638\n",
      "epoch: 7 iter: 383 loss: 0.09012 training acc: 0.97374 testing acc: 0.9634\n",
      "epoch: 7 iter: 384 loss: 0.08958 training acc: 0.97370 testing acc: 0.9644\n",
      "epoch: 7 iter: 385 loss: 0.08595 training acc: 0.97480 testing acc: 0.9652\n",
      "epoch: 7 iter: 386 loss: 0.08882 training acc: 0.97372 testing acc: 0.9652\n",
      "epoch: 7 iter: 387 loss: 0.08798 training acc: 0.97384 testing acc: 0.965\n",
      "epoch: 7 iter: 388 loss: 0.08802 training acc: 0.97428 testing acc: 0.9644\n",
      "epoch: 7 iter: 389 loss: 0.08987 training acc: 0.97352 testing acc: 0.9622\n",
      "epoch: 7 iter: 390 loss: 0.10389 training acc: 0.96828 testing acc: 0.9582\n",
      "epoch: 7 iter: 391 loss: 0.10010 training acc: 0.96938 testing acc: 0.9596\n",
      "epoch: 7 iter: 392 loss: 0.08808 training acc: 0.97390 testing acc: 0.9622\n",
      "epoch: 7 iter: 393 loss: 0.08892 training acc: 0.97400 testing acc: 0.9626\n",
      "epoch: 7 iter: 394 loss: 0.10257 training acc: 0.96938 testing acc: 0.9602\n",
      "epoch: 7 iter: 395 loss: 0.08494 training acc: 0.97552 testing acc: 0.967\n",
      "epoch: 7 iter: 396 loss: 0.09067 training acc: 0.97354 testing acc: 0.9648\n",
      "epoch: 7 iter: 397 loss: 0.09099 training acc: 0.97288 testing acc: 0.9628\n",
      "epoch: 7 iter: 398 loss: 0.09029 training acc: 0.97334 testing acc: 0.964\n",
      "epoch: 7 iter: 399 loss: 0.09198 training acc: 0.97274 testing acc: 0.964\n",
      "epoch: 7 iter: 400 loss: 0.09080 training acc: 0.97274 testing acc: 0.964\n",
      "epoch: 7 iter: 401 loss: 0.13583 training acc: 0.95606 testing acc: 0.9486\n",
      "epoch: 7 iter: 402 loss: 0.10719 training acc: 0.96782 testing acc: 0.9582\n",
      "epoch: 7 iter: 403 loss: 0.10733 training acc: 0.96734 testing acc: 0.959\n",
      "epoch: 7 iter: 404 loss: 0.10125 training acc: 0.96924 testing acc: 0.9604\n",
      "epoch: 7 iter: 405 loss: 0.09103 training acc: 0.97310 testing acc: 0.9614\n",
      "epoch: 7 iter: 406 loss: 0.09077 training acc: 0.97286 testing acc: 0.9618\n",
      "epoch: 7 iter: 407 loss: 0.09503 training acc: 0.97272 testing acc: 0.9604\n",
      "epoch: 7 iter: 408 loss: 0.09068 training acc: 0.97394 testing acc: 0.9634\n",
      "epoch: 7 iter: 409 loss: 0.09023 training acc: 0.97382 testing acc: 0.9644\n",
      "epoch: 7 iter: 410 loss: 0.09341 training acc: 0.97382 testing acc: 0.9638\n",
      "epoch: 7 iter: 411 loss: 0.09634 training acc: 0.97270 testing acc: 0.9612\n",
      "epoch: 7 iter: 412 loss: 0.09766 training acc: 0.97180 testing acc: 0.9608\n",
      "epoch: 7 iter: 413 loss: 0.09591 training acc: 0.97232 testing acc: 0.9626\n",
      "epoch: 7 iter: 414 loss: 0.09477 training acc: 0.97280 testing acc: 0.9612\n",
      "epoch: 7 iter: 415 loss: 0.09151 training acc: 0.97360 testing acc: 0.9632\n",
      "epoch: 7 iter: 416 loss: 0.09147 training acc: 0.97346 testing acc: 0.9634\n",
      "epoch: 7 iter: 417 loss: 0.09448 training acc: 0.97284 testing acc: 0.9614\n",
      "epoch: 7 iter: 418 loss: 0.09408 training acc: 0.97296 testing acc: 0.9614\n",
      "epoch: 7 iter: 419 loss: 0.08908 training acc: 0.97464 testing acc: 0.9652\n",
      "epoch: 7 iter: 420 loss: 0.08829 training acc: 0.97480 testing acc: 0.965\n",
      "epoch: 7 iter: 421 loss: 0.08834 training acc: 0.97460 testing acc: 0.9648\n",
      "epoch: 7 iter: 422 loss: 0.09842 training acc: 0.97136 testing acc: 0.962\n",
      "epoch: 7 iter: 423 loss: 0.08798 training acc: 0.97436 testing acc: 0.964\n",
      "epoch: 7 iter: 424 loss: 0.08776 training acc: 0.97462 testing acc: 0.965\n",
      "epoch: 7 iter: 425 loss: 0.08757 training acc: 0.97474 testing acc: 0.9642\n",
      "epoch: 7 iter: 426 loss: 0.08983 training acc: 0.97376 testing acc: 0.9622\n",
      "epoch: 7 iter: 427 loss: 0.09795 training acc: 0.97086 testing acc: 0.9606\n",
      "epoch: 7 iter: 428 loss: 0.09205 training acc: 0.97234 testing acc: 0.9628\n",
      "epoch: 7 iter: 429 loss: 0.09541 training acc: 0.97140 testing acc: 0.961\n",
      "epoch: 7 iter: 430 loss: 0.10026 training acc: 0.97030 testing acc: 0.963\n",
      "epoch: 7 iter: 431 loss: 0.09684 training acc: 0.97106 testing acc: 0.9646\n",
      "epoch: 7 iter: 432 loss: 0.09522 training acc: 0.97106 testing acc: 0.9634\n",
      "epoch: 7 iter: 433 loss: 0.10030 training acc: 0.96942 testing acc: 0.9618\n",
      "epoch: 7 iter: 434 loss: 0.09308 training acc: 0.97226 testing acc: 0.9648\n",
      "epoch: 7 iter: 435 loss: 0.10145 training acc: 0.96920 testing acc: 0.9598\n",
      "epoch: 7 iter: 436 loss: 0.09076 training acc: 0.97304 testing acc: 0.964\n",
      "epoch: 7 iter: 437 loss: 0.08989 training acc: 0.97274 testing acc: 0.9658\n",
      "epoch: 7 iter: 438 loss: 0.08860 training acc: 0.97276 testing acc: 0.9658\n",
      "epoch: 7 iter: 439 loss: 0.09486 training acc: 0.97072 testing acc: 0.962\n",
      "epoch: 7 iter: 440 loss: 0.09163 training acc: 0.97282 testing acc: 0.9646\n",
      "epoch: 7 iter: 441 loss: 0.08724 training acc: 0.97466 testing acc: 0.9656\n",
      "epoch: 7 iter: 442 loss: 0.08885 training acc: 0.97386 testing acc: 0.964\n",
      "epoch: 7 iter: 443 loss: 0.08576 training acc: 0.97458 testing acc: 0.9652\n",
      "epoch: 7 iter: 444 loss: 0.09342 training acc: 0.97188 testing acc: 0.962\n",
      "epoch: 7 iter: 445 loss: 0.09209 training acc: 0.97302 testing acc: 0.9642\n",
      "epoch: 7 iter: 446 loss: 0.08889 training acc: 0.97328 testing acc: 0.964\n",
      "epoch: 7 iter: 447 loss: 0.08800 training acc: 0.97396 testing acc: 0.9668\n",
      "epoch: 7 iter: 448 loss: 0.08902 training acc: 0.97418 testing acc: 0.968\n",
      "epoch: 7 iter: 449 loss: 0.08947 training acc: 0.97410 testing acc: 0.9662\n",
      "epoch: 7 iter: 450 loss: 0.09027 training acc: 0.97360 testing acc: 0.966\n",
      "epoch: 7 iter: 451 loss: 0.09083 training acc: 0.97376 testing acc: 0.9658\n",
      "epoch: 7 iter: 452 loss: 0.08963 training acc: 0.97456 testing acc: 0.9666\n",
      "epoch: 7 iter: 453 loss: 0.09450 training acc: 0.97258 testing acc: 0.9648\n",
      "epoch: 7 iter: 454 loss: 0.09835 training acc: 0.97100 testing acc: 0.962\n",
      "epoch: 7 iter: 455 loss: 0.09332 training acc: 0.97280 testing acc: 0.9654\n",
      "epoch: 7 iter: 456 loss: 0.09454 training acc: 0.97288 testing acc: 0.9646\n",
      "epoch: 7 iter: 457 loss: 0.10306 training acc: 0.96878 testing acc: 0.9622\n",
      "epoch: 7 iter: 458 loss: 0.13648 training acc: 0.95728 testing acc: 0.9502\n",
      "epoch: 7 iter: 459 loss: 0.10224 training acc: 0.96924 testing acc: 0.9618\n",
      "epoch: 7 iter: 460 loss: 0.09846 training acc: 0.97028 testing acc: 0.9612\n",
      "epoch: 7 iter: 461 loss: 0.09222 training acc: 0.97294 testing acc: 0.9644\n",
      "epoch: 7 iter: 462 loss: 0.08881 training acc: 0.97398 testing acc: 0.9672\n",
      "epoch: 7 iter: 463 loss: 0.08856 training acc: 0.97418 testing acc: 0.9664\n",
      "epoch: 7 iter: 464 loss: 0.08764 training acc: 0.97454 testing acc: 0.9668\n",
      "epoch: 7 iter: 465 loss: 0.08660 training acc: 0.97514 testing acc: 0.9672\n",
      "epoch: 7 iter: 466 loss: 0.09209 training acc: 0.97282 testing acc: 0.966\n",
      "epoch: 7 iter: 467 loss: 0.09764 training acc: 0.97100 testing acc: 0.9646\n",
      "epoch: 7 iter: 468 loss: 0.09373 training acc: 0.97240 testing acc: 0.965\n",
      "epoch: 7 iter: 469 loss: 0.09315 training acc: 0.97270 testing acc: 0.9654\n",
      "epoch: 7 iter: 470 loss: 0.09197 training acc: 0.97276 testing acc: 0.9656\n",
      "epoch: 7 iter: 471 loss: 0.08949 training acc: 0.97390 testing acc: 0.9652\n",
      "epoch: 7 iter: 472 loss: 0.08466 training acc: 0.97558 testing acc: 0.9678\n",
      "epoch: 7 iter: 473 loss: 0.08744 training acc: 0.97490 testing acc: 0.9658\n",
      "epoch: 7 iter: 474 loss: 0.09690 training acc: 0.97094 testing acc: 0.9632\n",
      "epoch: 7 iter: 475 loss: 0.11868 training acc: 0.96290 testing acc: 0.9564\n",
      "epoch: 7 iter: 476 loss: 0.09751 training acc: 0.97090 testing acc: 0.9636\n",
      "epoch: 7 iter: 477 loss: 0.09678 training acc: 0.97160 testing acc: 0.9642\n",
      "epoch: 7 iter: 478 loss: 0.09790 training acc: 0.97040 testing acc: 0.9626\n",
      "epoch: 7 iter: 479 loss: 0.09784 training acc: 0.97056 testing acc: 0.963\n",
      "epoch: 7 iter: 480 loss: 0.10050 training acc: 0.96946 testing acc: 0.9614\n",
      "epoch: 7 iter: 481 loss: 0.09656 training acc: 0.97090 testing acc: 0.9642\n",
      "epoch: 7 iter: 482 loss: 0.09528 training acc: 0.97074 testing acc: 0.964\n",
      "epoch: 7 iter: 483 loss: 0.09471 training acc: 0.97112 testing acc: 0.9626\n",
      "epoch: 7 iter: 484 loss: 0.09335 training acc: 0.97188 testing acc: 0.9626\n",
      "epoch: 7 iter: 485 loss: 0.09346 training acc: 0.97232 testing acc: 0.9612\n",
      "epoch: 7 iter: 486 loss: 0.08940 training acc: 0.97394 testing acc: 0.9632\n",
      "epoch: 7 iter: 487 loss: 0.09246 training acc: 0.97246 testing acc: 0.9628\n",
      "epoch: 7 iter: 488 loss: 0.09105 training acc: 0.97308 testing acc: 0.9632\n",
      "epoch: 7 iter: 489 loss: 0.09186 training acc: 0.97322 testing acc: 0.9648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 iter: 490 loss: 0.09053 training acc: 0.97350 testing acc: 0.9648\n",
      "epoch: 7 iter: 491 loss: 0.09502 training acc: 0.97202 testing acc: 0.9642\n",
      "epoch: 7 iter: 492 loss: 0.09458 training acc: 0.97236 testing acc: 0.9646\n",
      "epoch: 7 iter: 493 loss: 0.09014 training acc: 0.97348 testing acc: 0.9662\n",
      "epoch: 7 iter: 494 loss: 0.09419 training acc: 0.97242 testing acc: 0.965\n",
      "epoch: 7 iter: 495 loss: 0.09948 training acc: 0.97048 testing acc: 0.9618\n",
      "epoch: 7 iter: 496 loss: 0.08770 training acc: 0.97434 testing acc: 0.9666\n",
      "epoch: 7 iter: 497 loss: 0.09547 training acc: 0.97170 testing acc: 0.9644\n",
      "epoch: 7 iter: 498 loss: 0.08951 training acc: 0.97406 testing acc: 0.9676\n",
      "epoch: 7 iter: 499 loss: 0.09098 training acc: 0.97334 testing acc: 0.967\n",
      "epoch: 7 iter: 500 loss: 0.09139 training acc: 0.97318 testing acc: 0.9674\n",
      "epoch: 7 iter: 501 loss: 0.09166 training acc: 0.97302 testing acc: 0.9666\n",
      "epoch: 7 iter: 502 loss: 0.09406 training acc: 0.97126 testing acc: 0.9658\n",
      "epoch: 7 iter: 503 loss: 0.09979 training acc: 0.96940 testing acc: 0.963\n",
      "epoch: 7 iter: 504 loss: 0.09582 training acc: 0.97106 testing acc: 0.9654\n",
      "epoch: 7 iter: 505 loss: 0.09118 training acc: 0.97286 testing acc: 0.9654\n",
      "epoch: 7 iter: 506 loss: 0.10096 training acc: 0.96904 testing acc: 0.9626\n",
      "epoch: 7 iter: 507 loss: 0.10442 training acc: 0.96748 testing acc: 0.963\n",
      "epoch: 7 iter: 508 loss: 0.09837 training acc: 0.97020 testing acc: 0.9644\n",
      "epoch: 7 iter: 509 loss: 0.09718 training acc: 0.97102 testing acc: 0.9652\n",
      "epoch: 7 iter: 510 loss: 0.10087 training acc: 0.97012 testing acc: 0.9622\n",
      "epoch: 7 iter: 511 loss: 0.09642 training acc: 0.97160 testing acc: 0.9648\n",
      "epoch: 7 iter: 512 loss: 0.08842 training acc: 0.97472 testing acc: 0.9658\n",
      "epoch: 7 iter: 513 loss: 0.09090 training acc: 0.97296 testing acc: 0.966\n",
      "epoch: 7 iter: 514 loss: 0.08492 training acc: 0.97524 testing acc: 0.9664\n",
      "epoch: 7 iter: 515 loss: 0.08533 training acc: 0.97526 testing acc: 0.9662\n",
      "epoch: 7 iter: 516 loss: 0.08978 training acc: 0.97350 testing acc: 0.9616\n",
      "epoch: 7 iter: 517 loss: 0.08604 training acc: 0.97496 testing acc: 0.966\n",
      "epoch: 7 iter: 518 loss: 0.08946 training acc: 0.97350 testing acc: 0.9648\n",
      "epoch: 7 iter: 519 loss: 0.08936 training acc: 0.97326 testing acc: 0.9648\n",
      "epoch: 7 iter: 520 loss: 0.08988 training acc: 0.97300 testing acc: 0.9646\n",
      "epoch: 7 iter: 521 loss: 0.09385 training acc: 0.97264 testing acc: 0.9626\n",
      "epoch: 7 iter: 522 loss: 0.09468 training acc: 0.97218 testing acc: 0.9636\n",
      "epoch: 7 iter: 523 loss: 0.08745 training acc: 0.97468 testing acc: 0.9646\n",
      "epoch: 7 iter: 524 loss: 0.08734 training acc: 0.97436 testing acc: 0.9654\n",
      "epoch: 7 iter: 525 loss: 0.08733 training acc: 0.97438 testing acc: 0.965\n",
      "epoch: 7 iter: 526 loss: 0.09188 training acc: 0.97300 testing acc: 0.9622\n",
      "epoch: 7 iter: 527 loss: 0.10059 training acc: 0.97038 testing acc: 0.9614\n",
      "epoch: 7 iter: 528 loss: 0.09082 training acc: 0.97374 testing acc: 0.9634\n",
      "epoch: 7 iter: 529 loss: 0.09137 training acc: 0.97296 testing acc: 0.963\n",
      "epoch: 7 iter: 530 loss: 0.09397 training acc: 0.97202 testing acc: 0.9622\n",
      "epoch: 7 iter: 531 loss: 0.09638 training acc: 0.97096 testing acc: 0.9602\n",
      "epoch: 7 iter: 532 loss: 0.09916 training acc: 0.96950 testing acc: 0.9602\n",
      "epoch: 7 iter: 533 loss: 0.09915 training acc: 0.96978 testing acc: 0.9594\n",
      "epoch: 7 iter: 534 loss: 0.09606 training acc: 0.97066 testing acc: 0.9604\n",
      "epoch: 7 iter: 535 loss: 0.09726 training acc: 0.97060 testing acc: 0.9598\n",
      "epoch: 7 iter: 536 loss: 0.09845 training acc: 0.97062 testing acc: 0.9618\n",
      "epoch: 7 iter: 537 loss: 0.11696 training acc: 0.96504 testing acc: 0.956\n",
      "epoch: 7 iter: 538 loss: 0.10843 training acc: 0.96786 testing acc: 0.9602\n",
      "epoch: 7 iter: 539 loss: 0.12318 training acc: 0.96294 testing acc: 0.9538\n",
      "epoch: 7 iter: 540 loss: 0.10944 training acc: 0.96710 testing acc: 0.959\n",
      "epoch: 7 iter: 541 loss: 0.09328 training acc: 0.97228 testing acc: 0.9652\n",
      "epoch: 7 iter: 542 loss: 0.11387 training acc: 0.96480 testing acc: 0.9598\n",
      "epoch: 7 iter: 543 loss: 0.09260 training acc: 0.97226 testing acc: 0.9656\n",
      "epoch: 7 iter: 544 loss: 0.09328 training acc: 0.97172 testing acc: 0.9642\n",
      "epoch: 7 iter: 545 loss: 0.09554 training acc: 0.97150 testing acc: 0.9622\n",
      "epoch: 7 iter: 546 loss: 0.10199 training acc: 0.96868 testing acc: 0.958\n",
      "epoch: 7 iter: 547 loss: 0.09672 training acc: 0.97144 testing acc: 0.9618\n",
      "epoch: 7 iter: 548 loss: 0.09645 training acc: 0.97180 testing acc: 0.963\n",
      "epoch: 7 iter: 549 loss: 0.09258 training acc: 0.97308 testing acc: 0.9644\n",
      "epoch: 7 iter: 550 loss: 0.09258 training acc: 0.97322 testing acc: 0.9656\n",
      "epoch: 7 iter: 551 loss: 0.09205 training acc: 0.97318 testing acc: 0.9656\n",
      "epoch: 7 iter: 552 loss: 0.09495 training acc: 0.97204 testing acc: 0.963\n",
      "epoch: 7 iter: 553 loss: 0.10167 training acc: 0.96938 testing acc: 0.9594\n",
      "epoch: 7 iter: 554 loss: 0.09288 training acc: 0.97308 testing acc: 0.9652\n",
      "epoch: 7 iter: 555 loss: 0.09540 training acc: 0.97226 testing acc: 0.9648\n",
      "epoch: 7 iter: 556 loss: 0.09452 training acc: 0.97260 testing acc: 0.9638\n",
      "epoch: 7 iter: 557 loss: 0.09628 training acc: 0.97226 testing acc: 0.9624\n",
      "epoch: 7 iter: 558 loss: 0.09998 training acc: 0.97030 testing acc: 0.9614\n",
      "epoch: 7 iter: 559 loss: 0.10559 training acc: 0.96796 testing acc: 0.9614\n",
      "epoch: 7 iter: 560 loss: 0.09791 training acc: 0.97124 testing acc: 0.9622\n",
      "epoch: 7 iter: 561 loss: 0.09195 training acc: 0.97236 testing acc: 0.9642\n",
      "epoch: 7 iter: 562 loss: 0.09165 training acc: 0.97226 testing acc: 0.9632\n",
      "epoch: 7 iter: 563 loss: 0.08695 training acc: 0.97436 testing acc: 0.9668\n",
      "epoch: 7 iter: 564 loss: 0.09138 training acc: 0.97212 testing acc: 0.965\n",
      "epoch: 7 iter: 565 loss: 0.08886 training acc: 0.97314 testing acc: 0.9654\n",
      "epoch: 7 iter: 566 loss: 0.08900 training acc: 0.97276 testing acc: 0.9646\n",
      "epoch: 7 iter: 567 loss: 0.09606 training acc: 0.97094 testing acc: 0.9604\n",
      "epoch: 7 iter: 568 loss: 0.09067 training acc: 0.97304 testing acc: 0.9624\n",
      "epoch: 7 iter: 569 loss: 0.08991 training acc: 0.97334 testing acc: 0.964\n",
      "epoch: 7 iter: 570 loss: 0.09583 training acc: 0.97128 testing acc: 0.9624\n",
      "epoch: 7 iter: 571 loss: 0.09374 training acc: 0.97174 testing acc: 0.9628\n",
      "epoch: 7 iter: 572 loss: 0.08829 training acc: 0.97378 testing acc: 0.965\n",
      "epoch: 7 iter: 573 loss: 0.08695 training acc: 0.97436 testing acc: 0.968\n",
      "epoch: 7 iter: 574 loss: 0.09142 training acc: 0.97274 testing acc: 0.9656\n",
      "epoch: 7 iter: 575 loss: 0.08758 training acc: 0.97398 testing acc: 0.9666\n",
      "epoch: 7 iter: 576 loss: 0.09290 training acc: 0.97238 testing acc: 0.9666\n",
      "epoch: 7 iter: 577 loss: 0.09064 training acc: 0.97286 testing acc: 0.9668\n",
      "epoch: 7 iter: 578 loss: 0.09102 training acc: 0.97336 testing acc: 0.9676\n",
      "epoch: 7 iter: 579 loss: 0.10783 training acc: 0.96796 testing acc: 0.959\n",
      "epoch: 7 iter: 580 loss: 0.10082 training acc: 0.97018 testing acc: 0.962\n",
      "epoch: 7 iter: 581 loss: 0.09080 training acc: 0.97380 testing acc: 0.9648\n",
      "epoch: 7 iter: 582 loss: 0.08976 training acc: 0.97426 testing acc: 0.9664\n",
      "epoch: 7 iter: 583 loss: 0.08836 training acc: 0.97476 testing acc: 0.9668\n",
      "epoch: 7 iter: 584 loss: 0.08817 training acc: 0.97500 testing acc: 0.9642\n",
      "epoch: 7 iter: 585 loss: 0.08800 training acc: 0.97510 testing acc: 0.9642\n",
      "epoch: 7 iter: 586 loss: 0.09091 training acc: 0.97388 testing acc: 0.9632\n",
      "epoch: 7 iter: 587 loss: 0.09033 training acc: 0.97384 testing acc: 0.9648\n",
      "epoch: 7 iter: 588 loss: 0.08632 training acc: 0.97498 testing acc: 0.9658\n",
      "epoch: 7 iter: 589 loss: 0.09446 training acc: 0.97196 testing acc: 0.966\n",
      "epoch: 7 iter: 590 loss: 0.09448 training acc: 0.97178 testing acc: 0.9658\n",
      "epoch: 7 iter: 591 loss: 0.09330 training acc: 0.97224 testing acc: 0.9636\n",
      "epoch: 7 iter: 592 loss: 0.09238 training acc: 0.97228 testing acc: 0.964\n",
      "epoch: 7 iter: 593 loss: 0.08983 training acc: 0.97308 testing acc: 0.9628\n",
      "epoch: 7 iter: 594 loss: 0.08604 training acc: 0.97470 testing acc: 0.9642\n",
      "epoch: 7 iter: 595 loss: 0.08844 training acc: 0.97376 testing acc: 0.966\n",
      "epoch: 7 iter: 596 loss: 0.08761 training acc: 0.97410 testing acc: 0.9656\n",
      "epoch: 7 iter: 597 loss: 0.09127 training acc: 0.97324 testing acc: 0.9628\n",
      "epoch: 7 iter: 598 loss: 0.09230 training acc: 0.97326 testing acc: 0.9644\n",
      "epoch: 7 iter: 599 loss: 0.09449 training acc: 0.97276 testing acc: 0.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 iter: 600 loss: 0.09058 training acc: 0.97358 testing acc: 0.9644\n",
      "epoch: 7 iter: 601 loss: 0.10394 training acc: 0.96846 testing acc: 0.9602\n",
      "epoch: 7 iter: 602 loss: 0.10681 training acc: 0.96724 testing acc: 0.9596\n",
      "epoch: 7 iter: 603 loss: 0.09109 training acc: 0.97246 testing acc: 0.9652\n",
      "epoch: 7 iter: 604 loss: 0.10186 training acc: 0.96882 testing acc: 0.9604\n",
      "epoch: 7 iter: 605 loss: 0.10024 training acc: 0.96988 testing acc: 0.9606\n",
      "epoch: 7 iter: 606 loss: 0.09725 training acc: 0.97106 testing acc: 0.9616\n",
      "epoch: 7 iter: 607 loss: 0.09513 training acc: 0.97180 testing acc: 0.9628\n",
      "epoch: 7 iter: 608 loss: 0.09403 training acc: 0.97240 testing acc: 0.963\n",
      "epoch: 7 iter: 609 loss: 0.08851 training acc: 0.97400 testing acc: 0.9666\n",
      "epoch: 7 iter: 610 loss: 0.08713 training acc: 0.97472 testing acc: 0.9668\n",
      "epoch: 7 iter: 611 loss: 0.09220 training acc: 0.97302 testing acc: 0.962\n",
      "epoch: 7 iter: 612 loss: 0.09298 training acc: 0.97238 testing acc: 0.9652\n",
      "epoch: 7 iter: 613 loss: 0.09013 training acc: 0.97336 testing acc: 0.9654\n",
      "epoch: 7 iter: 614 loss: 0.08801 training acc: 0.97436 testing acc: 0.9668\n",
      "epoch: 7 iter: 615 loss: 0.08641 training acc: 0.97486 testing acc: 0.9668\n",
      "epoch: 7 iter: 616 loss: 0.08965 training acc: 0.97388 testing acc: 0.9654\n",
      "epoch: 7 iter: 617 loss: 0.08777 training acc: 0.97452 testing acc: 0.9668\n",
      "epoch: 7 iter: 618 loss: 0.08322 training acc: 0.97582 testing acc: 0.9674\n",
      "epoch: 7 iter: 619 loss: 0.08272 training acc: 0.97618 testing acc: 0.968\n",
      "epoch: 7 iter: 620 loss: 0.08916 training acc: 0.97450 testing acc: 0.968\n",
      "epoch: 7 iter: 621 loss: 0.08573 training acc: 0.97574 testing acc: 0.9684\n",
      "epoch: 7 iter: 622 loss: 0.08728 training acc: 0.97506 testing acc: 0.9678\n",
      "epoch: 7 iter: 623 loss: 0.08774 training acc: 0.97422 testing acc: 0.9656\n",
      "epoch: 7 iter: 624 loss: 0.08670 training acc: 0.97434 testing acc: 0.9658\n",
      "epoch: 7 iter: 625 loss: 0.08787 training acc: 0.97422 testing acc: 0.9662\n",
      "epoch: 7 iter: 626 loss: 0.08679 training acc: 0.97452 testing acc: 0.9664\n",
      "epoch: 7 iter: 627 loss: 0.09276 training acc: 0.97188 testing acc: 0.9638\n",
      "epoch: 7 iter: 628 loss: 0.09551 training acc: 0.97068 testing acc: 0.9604\n",
      "epoch: 7 iter: 629 loss: 0.08577 training acc: 0.97432 testing acc: 0.9648\n",
      "epoch: 7 iter: 630 loss: 0.08981 training acc: 0.97288 testing acc: 0.9616\n",
      "epoch: 7 iter: 631 loss: 0.09330 training acc: 0.97216 testing acc: 0.963\n",
      "epoch: 7 iter: 632 loss: 0.08580 training acc: 0.97408 testing acc: 0.9638\n",
      "epoch: 7 iter: 633 loss: 0.08898 training acc: 0.97316 testing acc: 0.9634\n",
      "epoch: 7 iter: 634 loss: 0.10023 training acc: 0.96938 testing acc: 0.9602\n",
      "epoch: 7 iter: 635 loss: 0.08671 training acc: 0.97430 testing acc: 0.9662\n",
      "epoch: 7 iter: 636 loss: 0.08423 training acc: 0.97584 testing acc: 0.9662\n",
      "epoch: 7 iter: 637 loss: 0.08485 training acc: 0.97544 testing acc: 0.9658\n",
      "epoch: 7 iter: 638 loss: 0.09359 training acc: 0.97180 testing acc: 0.9624\n",
      "epoch: 7 iter: 639 loss: 0.09554 training acc: 0.97112 testing acc: 0.9632\n",
      "epoch: 7 iter: 640 loss: 0.08592 training acc: 0.97504 testing acc: 0.9672\n",
      "epoch: 7 iter: 641 loss: 0.08716 training acc: 0.97442 testing acc: 0.9656\n",
      "epoch: 7 iter: 642 loss: 0.08385 training acc: 0.97600 testing acc: 0.9684\n",
      "epoch: 7 iter: 643 loss: 0.08823 training acc: 0.97432 testing acc: 0.9662\n",
      "epoch: 7 iter: 644 loss: 0.08728 training acc: 0.97450 testing acc: 0.9658\n",
      "epoch: 7 iter: 645 loss: 0.09502 training acc: 0.97184 testing acc: 0.9636\n",
      "epoch: 7 iter: 646 loss: 0.09263 training acc: 0.97246 testing acc: 0.9648\n",
      "epoch: 7 iter: 647 loss: 0.10665 training acc: 0.96794 testing acc: 0.959\n",
      "epoch: 7 iter: 648 loss: 0.10515 training acc: 0.96820 testing acc: 0.9584\n",
      "epoch: 7 iter: 649 loss: 0.09757 training acc: 0.97054 testing acc: 0.9622\n",
      "epoch: 7 iter: 650 loss: 0.09616 training acc: 0.97080 testing acc: 0.9624\n",
      "epoch: 7 iter: 651 loss: 0.09150 training acc: 0.97314 testing acc: 0.9644\n",
      "epoch: 7 iter: 652 loss: 0.09162 training acc: 0.97198 testing acc: 0.9628\n",
      "epoch: 7 iter: 653 loss: 0.08818 training acc: 0.97332 testing acc: 0.963\n",
      "epoch: 7 iter: 654 loss: 0.08788 training acc: 0.97384 testing acc: 0.9628\n",
      "epoch: 7 iter: 655 loss: 0.08858 training acc: 0.97328 testing acc: 0.9638\n",
      "epoch: 7 iter: 656 loss: 0.08726 training acc: 0.97472 testing acc: 0.964\n",
      "epoch: 7 iter: 657 loss: 0.09039 training acc: 0.97322 testing acc: 0.962\n",
      "epoch: 7 iter: 658 loss: 0.08966 training acc: 0.97342 testing acc: 0.9622\n",
      "epoch: 7 iter: 659 loss: 0.09111 training acc: 0.97352 testing acc: 0.9636\n",
      "epoch: 7 iter: 660 loss: 0.09631 training acc: 0.97030 testing acc: 0.9616\n",
      "epoch: 7 iter: 661 loss: 0.09114 training acc: 0.97274 testing acc: 0.9632\n",
      "epoch: 7 iter: 662 loss: 0.09347 training acc: 0.97212 testing acc: 0.9626\n",
      "epoch: 7 iter: 663 loss: 0.08806 training acc: 0.97440 testing acc: 0.9662\n",
      "epoch: 7 iter: 664 loss: 0.08521 training acc: 0.97532 testing acc: 0.9664\n",
      "epoch: 7 iter: 665 loss: 0.08525 training acc: 0.97538 testing acc: 0.966\n",
      "epoch: 7 iter: 666 loss: 0.08584 training acc: 0.97510 testing acc: 0.9666\n",
      "epoch: 7 iter: 667 loss: 0.08227 training acc: 0.97692 testing acc: 0.9696\n",
      "epoch: 7 iter: 668 loss: 0.08364 training acc: 0.97580 testing acc: 0.9668\n",
      "epoch: 7 iter: 669 loss: 0.08637 training acc: 0.97474 testing acc: 0.9668\n",
      "epoch: 7 iter: 670 loss: 0.09183 training acc: 0.97230 testing acc: 0.9628\n",
      "epoch: 7 iter: 671 loss: 0.08542 training acc: 0.97504 testing acc: 0.9662\n",
      "epoch: 7 iter: 672 loss: 0.08614 training acc: 0.97420 testing acc: 0.9656\n",
      "epoch: 7 iter: 673 loss: 0.08640 training acc: 0.97434 testing acc: 0.9654\n",
      "epoch: 7 iter: 674 loss: 0.08559 training acc: 0.97452 testing acc: 0.9648\n",
      "epoch: 7 iter: 675 loss: 0.08749 training acc: 0.97424 testing acc: 0.9642\n",
      "epoch: 7 iter: 676 loss: 0.09907 training acc: 0.97064 testing acc: 0.9592\n",
      "epoch: 7 iter: 677 loss: 0.09091 training acc: 0.97338 testing acc: 0.9626\n",
      "epoch: 7 iter: 678 loss: 0.09320 training acc: 0.97274 testing acc: 0.963\n",
      "epoch: 7 iter: 679 loss: 0.10340 training acc: 0.96902 testing acc: 0.9586\n",
      "epoch: 7 iter: 680 loss: 0.09129 training acc: 0.97360 testing acc: 0.9638\n",
      "epoch: 7 iter: 681 loss: 0.08943 training acc: 0.97410 testing acc: 0.9644\n",
      "epoch: 7 iter: 682 loss: 0.08606 training acc: 0.97476 testing acc: 0.9656\n",
      "epoch: 7 iter: 683 loss: 0.08701 training acc: 0.97432 testing acc: 0.9648\n",
      "epoch: 7 iter: 684 loss: 0.09370 training acc: 0.97220 testing acc: 0.9636\n",
      "epoch: 7 iter: 685 loss: 0.09335 training acc: 0.97284 testing acc: 0.9624\n",
      "epoch: 7 iter: 686 loss: 0.09458 training acc: 0.97230 testing acc: 0.9642\n",
      "epoch: 7 iter: 687 loss: 0.09139 training acc: 0.97350 testing acc: 0.9654\n",
      "epoch: 7 iter: 688 loss: 0.09098 training acc: 0.97356 testing acc: 0.9648\n",
      "epoch: 7 iter: 689 loss: 0.09143 training acc: 0.97344 testing acc: 0.965\n",
      "epoch: 7 iter: 690 loss: 0.09032 training acc: 0.97360 testing acc: 0.9658\n",
      "epoch: 7 iter: 691 loss: 0.09309 training acc: 0.97268 testing acc: 0.9642\n",
      "epoch: 7 iter: 692 loss: 0.09194 training acc: 0.97320 testing acc: 0.9646\n",
      "epoch: 7 iter: 693 loss: 0.08795 training acc: 0.97546 testing acc: 0.9664\n",
      "epoch: 7 iter: 694 loss: 0.09076 training acc: 0.97364 testing acc: 0.9676\n",
      "epoch: 7 iter: 695 loss: 0.10536 training acc: 0.96856 testing acc: 0.9616\n",
      "epoch: 7 iter: 696 loss: 0.10611 training acc: 0.96852 testing acc: 0.9612\n",
      "epoch: 7 iter: 697 loss: 0.08902 training acc: 0.97418 testing acc: 0.966\n",
      "epoch: 7 iter: 698 loss: 0.08778 training acc: 0.97476 testing acc: 0.9672\n",
      "epoch: 7 iter: 699 loss: 0.09025 training acc: 0.97390 testing acc: 0.9666\n",
      "epoch: 7 iter: 700 loss: 0.08757 training acc: 0.97484 testing acc: 0.9672\n",
      "epoch: 7 iter: 701 loss: 0.08414 training acc: 0.97560 testing acc: 0.9688\n",
      "epoch: 7 iter: 702 loss: 0.09098 training acc: 0.97392 testing acc: 0.9626\n",
      "epoch: 7 iter: 703 loss: 0.08621 training acc: 0.97464 testing acc: 0.9666\n",
      "epoch: 7 iter: 704 loss: 0.08462 training acc: 0.97546 testing acc: 0.9682\n",
      "epoch: 7 iter: 705 loss: 0.08395 training acc: 0.97604 testing acc: 0.9676\n",
      "epoch: 7 iter: 706 loss: 0.08448 training acc: 0.97592 testing acc: 0.967\n",
      "epoch: 7 iter: 707 loss: 0.08151 training acc: 0.97668 testing acc: 0.969\n",
      "epoch: 7 iter: 708 loss: 0.08412 training acc: 0.97610 testing acc: 0.968\n",
      "epoch: 7 iter: 709 loss: 0.08227 training acc: 0.97654 testing acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 iter: 710 loss: 0.08629 training acc: 0.97574 testing acc: 0.967\n",
      "epoch: 7 iter: 711 loss: 0.08882 training acc: 0.97494 testing acc: 0.965\n",
      "epoch: 7 iter: 712 loss: 0.08638 training acc: 0.97556 testing acc: 0.9654\n",
      "epoch: 7 iter: 713 loss: 0.08886 training acc: 0.97434 testing acc: 0.9646\n",
      "epoch: 7 iter: 714 loss: 0.08617 training acc: 0.97640 testing acc: 0.9668\n",
      "epoch: 7 iter: 715 loss: 0.09514 training acc: 0.97198 testing acc: 0.961\n",
      "epoch: 7 iter: 716 loss: 0.09066 training acc: 0.97340 testing acc: 0.9634\n",
      "epoch: 7 iter: 717 loss: 0.08591 training acc: 0.97534 testing acc: 0.9642\n",
      "epoch: 7 iter: 718 loss: 0.08097 training acc: 0.97714 testing acc: 0.9668\n",
      "epoch: 7 iter: 719 loss: 0.07982 training acc: 0.97764 testing acc: 0.9674\n",
      "epoch: 7 iter: 720 loss: 0.08006 training acc: 0.97722 testing acc: 0.967\n",
      "epoch: 7 iter: 721 loss: 0.07911 training acc: 0.97750 testing acc: 0.9664\n",
      "epoch: 7 iter: 722 loss: 0.08530 training acc: 0.97498 testing acc: 0.965\n",
      "epoch: 7 iter: 723 loss: 0.08416 training acc: 0.97576 testing acc: 0.9648\n",
      "epoch: 7 iter: 724 loss: 0.08667 training acc: 0.97506 testing acc: 0.9644\n",
      "epoch: 7 iter: 725 loss: 0.08579 training acc: 0.97484 testing acc: 0.9644\n",
      "epoch: 7 iter: 726 loss: 0.08561 training acc: 0.97498 testing acc: 0.9654\n",
      "epoch: 7 iter: 727 loss: 0.08292 training acc: 0.97652 testing acc: 0.9658\n",
      "epoch: 7 iter: 728 loss: 0.08432 training acc: 0.97590 testing acc: 0.9658\n",
      "epoch: 7 iter: 729 loss: 0.08229 training acc: 0.97648 testing acc: 0.9668\n",
      "epoch: 7 iter: 730 loss: 0.08163 training acc: 0.97688 testing acc: 0.9658\n",
      "epoch: 7 iter: 731 loss: 0.08622 training acc: 0.97400 testing acc: 0.9646\n",
      "epoch: 7 iter: 732 loss: 0.08024 training acc: 0.97702 testing acc: 0.9694\n",
      "epoch: 7 iter: 733 loss: 0.09058 training acc: 0.97414 testing acc: 0.964\n",
      "epoch: 7 iter: 734 loss: 0.08572 training acc: 0.97562 testing acc: 0.9666\n",
      "epoch: 7 iter: 735 loss: 0.08506 training acc: 0.97584 testing acc: 0.967\n",
      "epoch: 7 iter: 736 loss: 0.08973 training acc: 0.97348 testing acc: 0.9642\n",
      "epoch: 7 iter: 737 loss: 0.08042 training acc: 0.97732 testing acc: 0.9688\n",
      "epoch: 7 iter: 738 loss: 0.07943 training acc: 0.97726 testing acc: 0.969\n",
      "epoch: 7 iter: 739 loss: 0.08025 training acc: 0.97720 testing acc: 0.9698\n",
      "epoch: 7 iter: 740 loss: 0.10337 training acc: 0.96980 testing acc: 0.9602\n",
      "epoch: 7 iter: 741 loss: 0.08456 training acc: 0.97534 testing acc: 0.9674\n",
      "epoch: 7 iter: 742 loss: 0.09783 training acc: 0.97044 testing acc: 0.9606\n",
      "epoch: 7 iter: 743 loss: 0.09104 training acc: 0.97292 testing acc: 0.9628\n",
      "epoch: 7 iter: 744 loss: 0.09144 training acc: 0.97290 testing acc: 0.9626\n",
      "epoch: 7 iter: 745 loss: 0.08868 training acc: 0.97364 testing acc: 0.9636\n",
      "epoch: 7 iter: 746 loss: 0.08522 training acc: 0.97492 testing acc: 0.9672\n",
      "epoch: 7 iter: 747 loss: 0.08332 training acc: 0.97512 testing acc: 0.9688\n",
      "epoch: 7 iter: 748 loss: 0.08326 training acc: 0.97532 testing acc: 0.9682\n",
      "epoch: 7 iter: 749 loss: 0.08263 training acc: 0.97604 testing acc: 0.9684\n",
      "epoch: 7 iter: 750 loss: 0.08626 training acc: 0.97534 testing acc: 0.9684\n",
      "epoch: 7 iter: 751 loss: 0.08706 training acc: 0.97502 testing acc: 0.965\n",
      "epoch: 7 iter: 752 loss: 0.08471 training acc: 0.97556 testing acc: 0.9664\n",
      "epoch: 7 iter: 753 loss: 0.08299 training acc: 0.97620 testing acc: 0.9682\n",
      "epoch: 7 iter: 754 loss: 0.08144 training acc: 0.97658 testing acc: 0.969\n",
      "epoch: 7 iter: 755 loss: 0.08222 training acc: 0.97640 testing acc: 0.9698\n",
      "epoch: 7 iter: 756 loss: 0.07979 training acc: 0.97748 testing acc: 0.9704\n",
      "epoch: 7 iter: 757 loss: 0.08336 training acc: 0.97632 testing acc: 0.969\n",
      "epoch: 7 iter: 758 loss: 0.08221 training acc: 0.97668 testing acc: 0.9684\n",
      "epoch: 7 iter: 759 loss: 0.08232 training acc: 0.97634 testing acc: 0.9686\n",
      "epoch: 7 iter: 760 loss: 0.07910 training acc: 0.97740 testing acc: 0.9698\n",
      "epoch: 7 iter: 761 loss: 0.08276 training acc: 0.97636 testing acc: 0.9676\n",
      "epoch: 7 iter: 762 loss: 0.08146 training acc: 0.97634 testing acc: 0.9678\n",
      "epoch: 7 iter: 763 loss: 0.08009 training acc: 0.97796 testing acc: 0.9696\n",
      "epoch: 7 iter: 764 loss: 0.08175 training acc: 0.97566 testing acc: 0.968\n",
      "epoch: 7 iter: 765 loss: 0.07922 training acc: 0.97664 testing acc: 0.9706\n",
      "epoch: 7 iter: 766 loss: 0.08121 training acc: 0.97668 testing acc: 0.9686\n",
      "epoch: 7 iter: 767 loss: 0.08112 training acc: 0.97672 testing acc: 0.969\n",
      "epoch: 7 iter: 768 loss: 0.11199 training acc: 0.96520 testing acc: 0.9548\n",
      "epoch: 7 iter: 769 loss: 0.11097 training acc: 0.96478 testing acc: 0.957\n",
      "epoch: 7 iter: 770 loss: 0.10717 training acc: 0.96626 testing acc: 0.9574\n",
      "epoch: 7 iter: 771 loss: 0.09888 training acc: 0.97008 testing acc: 0.9626\n",
      "epoch: 7 iter: 772 loss: 0.08635 training acc: 0.97448 testing acc: 0.966\n",
      "epoch: 7 iter: 773 loss: 0.08796 training acc: 0.97390 testing acc: 0.9664\n",
      "epoch: 7 iter: 774 loss: 0.10570 training acc: 0.96876 testing acc: 0.958\n",
      "epoch: 7 iter: 775 loss: 0.11116 training acc: 0.96734 testing acc: 0.958\n",
      "epoch: 7 iter: 776 loss: 0.08976 training acc: 0.97304 testing acc: 0.961\n",
      "epoch: 7 iter: 777 loss: 0.08921 training acc: 0.97336 testing acc: 0.9632\n",
      "epoch: 7 iter: 778 loss: 0.08712 training acc: 0.97422 testing acc: 0.964\n",
      "epoch: 7 iter: 779 loss: 0.08958 training acc: 0.97324 testing acc: 0.962\n",
      "epoch: 7 iter: 780 loss: 0.08678 training acc: 0.97472 testing acc: 0.9642\n",
      "epoch: 7 iter: 781 loss: 0.08501 training acc: 0.97534 testing acc: 0.9638\n",
      "epoch: 7 iter: 782 loss: 0.08089 training acc: 0.97608 testing acc: 0.9656\n",
      "epoch: 7 iter: 783 loss: 0.08217 training acc: 0.97620 testing acc: 0.9662\n",
      "epoch: 7 iter: 784 loss: 0.08072 training acc: 0.97678 testing acc: 0.9664\n",
      "epoch: 7 iter: 785 loss: 0.08002 training acc: 0.97700 testing acc: 0.9668\n",
      "epoch: 7 iter: 786 loss: 0.08044 training acc: 0.97694 testing acc: 0.967\n",
      "epoch: 7 iter: 787 loss: 0.08059 training acc: 0.97678 testing acc: 0.966\n",
      "epoch: 7 iter: 788 loss: 0.08067 training acc: 0.97686 testing acc: 0.9666\n",
      "epoch: 7 iter: 789 loss: 0.08297 training acc: 0.97608 testing acc: 0.9666\n",
      "epoch: 7 iter: 790 loss: 0.08364 training acc: 0.97580 testing acc: 0.9662\n",
      "epoch: 7 iter: 791 loss: 0.08257 training acc: 0.97622 testing acc: 0.9656\n",
      "epoch: 7 iter: 792 loss: 0.08243 training acc: 0.97604 testing acc: 0.9666\n",
      "epoch: 7 iter: 793 loss: 0.08158 training acc: 0.97598 testing acc: 0.9676\n",
      "epoch: 7 iter: 794 loss: 0.07948 training acc: 0.97734 testing acc: 0.967\n",
      "epoch: 7 iter: 795 loss: 0.08234 training acc: 0.97576 testing acc: 0.9646\n",
      "epoch: 7 iter: 796 loss: 0.08351 training acc: 0.97548 testing acc: 0.965\n",
      "epoch: 7 iter: 797 loss: 0.08607 training acc: 0.97430 testing acc: 0.9646\n",
      "epoch: 7 iter: 798 loss: 0.08520 training acc: 0.97454 testing acc: 0.9642\n",
      "epoch: 7 iter: 799 loss: 0.08222 training acc: 0.97586 testing acc: 0.9642\n",
      "epoch: 7 iter: 800 loss: 0.08087 training acc: 0.97670 testing acc: 0.9658\n",
      "epoch: 7 iter: 801 loss: 0.08094 training acc: 0.97676 testing acc: 0.9662\n",
      "epoch: 7 iter: 802 loss: 0.08145 training acc: 0.97656 testing acc: 0.9658\n",
      "epoch: 7 iter: 803 loss: 0.08086 training acc: 0.97684 testing acc: 0.966\n",
      "epoch: 7 iter: 804 loss: 0.08114 training acc: 0.97638 testing acc: 0.9656\n",
      "epoch: 7 iter: 805 loss: 0.08137 training acc: 0.97608 testing acc: 0.9666\n",
      "epoch: 7 iter: 806 loss: 0.08195 training acc: 0.97624 testing acc: 0.9666\n",
      "epoch: 7 iter: 807 loss: 0.08195 training acc: 0.97636 testing acc: 0.9656\n",
      "epoch: 7 iter: 808 loss: 0.08182 training acc: 0.97648 testing acc: 0.966\n",
      "epoch: 7 iter: 809 loss: 0.08483 training acc: 0.97576 testing acc: 0.9652\n",
      "epoch: 7 iter: 810 loss: 0.08166 training acc: 0.97684 testing acc: 0.9664\n",
      "epoch: 7 iter: 811 loss: 0.08191 training acc: 0.97642 testing acc: 0.9666\n",
      "epoch: 7 iter: 812 loss: 0.08828 training acc: 0.97400 testing acc: 0.9638\n",
      "epoch: 7 iter: 813 loss: 0.08867 training acc: 0.97312 testing acc: 0.9628\n",
      "epoch: 7 iter: 814 loss: 0.08612 training acc: 0.97430 testing acc: 0.9614\n",
      "epoch: 7 iter: 815 loss: 0.08152 training acc: 0.97630 testing acc: 0.9668\n",
      "epoch: 7 iter: 816 loss: 0.08213 training acc: 0.97648 testing acc: 0.9664\n",
      "epoch: 7 iter: 817 loss: 0.08172 training acc: 0.97658 testing acc: 0.9664\n",
      "epoch: 7 iter: 818 loss: 0.09115 training acc: 0.97286 testing acc: 0.9644\n",
      "epoch: 7 iter: 819 loss: 0.09517 training acc: 0.97128 testing acc: 0.961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 iter: 820 loss: 0.08673 training acc: 0.97512 testing acc: 0.9644\n",
      "epoch: 7 iter: 821 loss: 0.08485 training acc: 0.97568 testing acc: 0.9654\n",
      "epoch: 7 iter: 822 loss: 0.08661 training acc: 0.97484 testing acc: 0.9644\n",
      "epoch: 7 iter: 823 loss: 0.08528 training acc: 0.97574 testing acc: 0.9652\n",
      "epoch: 7 iter: 824 loss: 0.08653 training acc: 0.97476 testing acc: 0.9638\n",
      "epoch: 7 iter: 825 loss: 0.09206 training acc: 0.97248 testing acc: 0.9636\n",
      "epoch: 7 iter: 826 loss: 0.08486 training acc: 0.97592 testing acc: 0.9666\n",
      "epoch: 7 iter: 827 loss: 0.08665 training acc: 0.97468 testing acc: 0.9648\n",
      "epoch: 7 iter: 828 loss: 0.09567 training acc: 0.97132 testing acc: 0.9598\n",
      "epoch: 7 iter: 829 loss: 0.08971 training acc: 0.97290 testing acc: 0.9624\n",
      "epoch: 7 iter: 830 loss: 0.09134 training acc: 0.97228 testing acc: 0.9622\n",
      "epoch: 7 iter: 831 loss: 0.09635 training acc: 0.97060 testing acc: 0.9624\n",
      "epoch: 7 iter: 832 loss: 0.09368 training acc: 0.97224 testing acc: 0.9604\n",
      "epoch: 7 iter: 833 loss: 0.08972 training acc: 0.97356 testing acc: 0.9624\n",
      "epoch: 7 iter: 834 loss: 0.08475 training acc: 0.97538 testing acc: 0.9654\n",
      "epoch: 7 iter: 835 loss: 0.08679 training acc: 0.97522 testing acc: 0.963\n",
      "epoch: 7 iter: 836 loss: 0.08873 training acc: 0.97402 testing acc: 0.964\n",
      "epoch: 7 iter: 837 loss: 0.08555 training acc: 0.97510 testing acc: 0.9662\n",
      "epoch: 7 iter: 838 loss: 0.08475 training acc: 0.97542 testing acc: 0.9656\n",
      "epoch: 7 iter: 839 loss: 0.08952 training acc: 0.97428 testing acc: 0.9638\n",
      "epoch: 7 iter: 840 loss: 0.08482 training acc: 0.97596 testing acc: 0.9656\n",
      "epoch: 7 iter: 841 loss: 0.08457 training acc: 0.97604 testing acc: 0.9662\n",
      "epoch: 7 iter: 842 loss: 0.09570 training acc: 0.97250 testing acc: 0.9616\n",
      "epoch: 7 iter: 843 loss: 0.09105 training acc: 0.97414 testing acc: 0.9628\n",
      "epoch: 7 iter: 844 loss: 0.08488 training acc: 0.97544 testing acc: 0.9664\n",
      "epoch: 7 iter: 845 loss: 0.08855 training acc: 0.97426 testing acc: 0.9658\n",
      "epoch: 7 iter: 846 loss: 0.09789 training acc: 0.97030 testing acc: 0.9622\n",
      "epoch: 7 iter: 847 loss: 0.09703 training acc: 0.97042 testing acc: 0.9616\n",
      "epoch: 7 iter: 848 loss: 0.09505 training acc: 0.97124 testing acc: 0.9614\n",
      "epoch: 7 iter: 849 loss: 0.09044 training acc: 0.97310 testing acc: 0.9634\n",
      "epoch: 7 iter: 850 loss: 0.09657 training acc: 0.97102 testing acc: 0.9612\n",
      "epoch: 7 iter: 851 loss: 0.09441 training acc: 0.97160 testing acc: 0.9604\n",
      "epoch: 7 iter: 852 loss: 0.09300 training acc: 0.97222 testing acc: 0.9628\n",
      "epoch: 7 iter: 853 loss: 0.08931 training acc: 0.97376 testing acc: 0.9648\n",
      "epoch: 7 iter: 854 loss: 0.08328 training acc: 0.97580 testing acc: 0.9678\n",
      "epoch: 7 iter: 855 loss: 0.08933 training acc: 0.97436 testing acc: 0.963\n",
      "epoch: 7 iter: 856 loss: 0.08894 training acc: 0.97460 testing acc: 0.962\n",
      "epoch: 7 iter: 857 loss: 0.08682 training acc: 0.97466 testing acc: 0.9626\n",
      "epoch: 7 iter: 858 loss: 0.08389 training acc: 0.97562 testing acc: 0.965\n",
      "epoch: 7 iter: 859 loss: 0.09413 training acc: 0.97178 testing acc: 0.9582\n",
      "epoch: 7 iter: 860 loss: 0.10832 training acc: 0.96576 testing acc: 0.955\n",
      "epoch: 7 iter: 861 loss: 0.10333 training acc: 0.96808 testing acc: 0.9558\n",
      "epoch: 7 iter: 862 loss: 0.09283 training acc: 0.97266 testing acc: 0.9594\n",
      "epoch: 7 iter: 863 loss: 0.08836 training acc: 0.97400 testing acc: 0.9622\n",
      "epoch: 7 iter: 864 loss: 0.09387 training acc: 0.97152 testing acc: 0.9584\n",
      "epoch: 7 iter: 865 loss: 0.09033 training acc: 0.97300 testing acc: 0.9594\n",
      "epoch: 7 iter: 866 loss: 0.09250 training acc: 0.97222 testing acc: 0.9602\n",
      "epoch: 7 iter: 867 loss: 0.09838 training acc: 0.97016 testing acc: 0.9602\n",
      "epoch: 7 iter: 868 loss: 0.09240 training acc: 0.97214 testing acc: 0.961\n",
      "epoch: 7 iter: 869 loss: 0.08639 training acc: 0.97426 testing acc: 0.9616\n",
      "epoch: 7 iter: 870 loss: 0.09368 training acc: 0.97288 testing acc: 0.962\n",
      "epoch: 7 iter: 871 loss: 0.09036 training acc: 0.97372 testing acc: 0.962\n",
      "epoch: 7 iter: 872 loss: 0.08996 training acc: 0.97386 testing acc: 0.9618\n",
      "epoch: 7 iter: 873 loss: 0.08752 training acc: 0.97454 testing acc: 0.9642\n",
      "epoch: 7 iter: 874 loss: 0.08267 training acc: 0.97598 testing acc: 0.9662\n",
      "epoch: 7 iter: 875 loss: 0.08285 training acc: 0.97596 testing acc: 0.9656\n",
      "epoch: 7 iter: 876 loss: 0.09032 training acc: 0.97370 testing acc: 0.9614\n",
      "epoch: 7 iter: 877 loss: 0.08683 training acc: 0.97478 testing acc: 0.9644\n",
      "epoch: 7 iter: 878 loss: 0.08510 training acc: 0.97524 testing acc: 0.9634\n",
      "epoch: 7 iter: 879 loss: 0.08118 training acc: 0.97648 testing acc: 0.9674\n",
      "epoch: 7 iter: 880 loss: 0.08339 training acc: 0.97612 testing acc: 0.9656\n",
      "epoch: 7 iter: 881 loss: 0.07875 training acc: 0.97792 testing acc: 0.9674\n",
      "epoch: 7 iter: 882 loss: 0.08240 training acc: 0.97616 testing acc: 0.966\n",
      "epoch: 7 iter: 883 loss: 0.07846 training acc: 0.97728 testing acc: 0.9668\n",
      "epoch: 7 iter: 884 loss: 0.07766 training acc: 0.97760 testing acc: 0.9662\n",
      "epoch: 7 iter: 885 loss: 0.08036 training acc: 0.97648 testing acc: 0.9646\n",
      "epoch: 7 iter: 886 loss: 0.07789 training acc: 0.97766 testing acc: 0.9674\n",
      "epoch: 7 iter: 887 loss: 0.07810 training acc: 0.97760 testing acc: 0.967\n",
      "epoch: 7 iter: 888 loss: 0.07848 training acc: 0.97738 testing acc: 0.9672\n",
      "epoch: 7 iter: 889 loss: 0.07803 training acc: 0.97726 testing acc: 0.9672\n",
      "epoch: 7 iter: 890 loss: 0.07939 training acc: 0.97732 testing acc: 0.9688\n",
      "epoch: 7 iter: 891 loss: 0.08209 training acc: 0.97632 testing acc: 0.9678\n",
      "epoch: 7 iter: 892 loss: 0.08366 training acc: 0.97650 testing acc: 0.9668\n",
      "epoch: 7 iter: 893 loss: 0.08474 training acc: 0.97520 testing acc: 0.9646\n",
      "epoch: 7 iter: 894 loss: 0.08906 training acc: 0.97380 testing acc: 0.9622\n",
      "epoch: 7 iter: 895 loss: 0.11419 training acc: 0.96392 testing acc: 0.9502\n",
      "epoch: 7 iter: 896 loss: 0.09806 training acc: 0.97076 testing acc: 0.9578\n",
      "epoch: 7 iter: 897 loss: 0.08943 training acc: 0.97376 testing acc: 0.9614\n",
      "epoch: 7 iter: 898 loss: 0.09070 training acc: 0.97264 testing acc: 0.9616\n",
      "epoch: 7 iter: 899 loss: 0.09553 training acc: 0.97144 testing acc: 0.9598\n",
      "epoch: 7 iter: 900 loss: 0.09858 training acc: 0.96974 testing acc: 0.96\n",
      "epoch: 7 iter: 901 loss: 0.08287 training acc: 0.97572 testing acc: 0.9648\n",
      "epoch: 7 iter: 902 loss: 0.08098 training acc: 0.97638 testing acc: 0.9658\n",
      "epoch: 7 iter: 903 loss: 0.10044 training acc: 0.96912 testing acc: 0.9552\n",
      "epoch: 7 iter: 904 loss: 0.08405 training acc: 0.97546 testing acc: 0.9662\n",
      "epoch: 7 iter: 905 loss: 0.08189 training acc: 0.97616 testing acc: 0.9668\n",
      "epoch: 7 iter: 906 loss: 0.08528 training acc: 0.97464 testing acc: 0.9648\n",
      "epoch: 7 iter: 907 loss: 0.08317 training acc: 0.97558 testing acc: 0.9652\n",
      "epoch: 7 iter: 908 loss: 0.09632 training acc: 0.97106 testing acc: 0.9594\n",
      "epoch: 7 iter: 909 loss: 0.08965 training acc: 0.97316 testing acc: 0.963\n",
      "epoch: 7 iter: 910 loss: 0.08431 training acc: 0.97506 testing acc: 0.965\n",
      "epoch: 7 iter: 911 loss: 0.08255 training acc: 0.97576 testing acc: 0.9676\n",
      "epoch: 7 iter: 912 loss: 0.08514 training acc: 0.97498 testing acc: 0.9646\n",
      "epoch: 7 iter: 913 loss: 0.08235 training acc: 0.97608 testing acc: 0.9658\n",
      "epoch: 7 iter: 914 loss: 0.08980 training acc: 0.97422 testing acc: 0.9618\n",
      "epoch: 7 iter: 915 loss: 0.09072 training acc: 0.97378 testing acc: 0.962\n",
      "epoch: 7 iter: 916 loss: 0.09334 training acc: 0.97282 testing acc: 0.9632\n",
      "epoch: 7 iter: 917 loss: 0.08677 training acc: 0.97520 testing acc: 0.966\n",
      "epoch: 7 iter: 918 loss: 0.08883 training acc: 0.97420 testing acc: 0.9648\n",
      "epoch: 7 iter: 919 loss: 0.09230 training acc: 0.97370 testing acc: 0.963\n",
      "epoch: 7 iter: 920 loss: 0.11119 training acc: 0.96672 testing acc: 0.9552\n",
      "epoch: 7 iter: 921 loss: 0.09243 training acc: 0.97298 testing acc: 0.9618\n",
      "epoch: 7 iter: 922 loss: 0.09641 training acc: 0.97128 testing acc: 0.96\n",
      "epoch: 7 iter: 923 loss: 0.09107 training acc: 0.97384 testing acc: 0.9614\n",
      "epoch: 7 iter: 924 loss: 0.09401 training acc: 0.97316 testing acc: 0.9618\n",
      "epoch: 7 iter: 925 loss: 0.08122 training acc: 0.97728 testing acc: 0.9676\n",
      "epoch: 7 iter: 926 loss: 0.08159 training acc: 0.97708 testing acc: 0.9674\n",
      "epoch: 7 iter: 927 loss: 0.08145 training acc: 0.97680 testing acc: 0.968\n",
      "epoch: 7 iter: 928 loss: 0.08342 training acc: 0.97592 testing acc: 0.9638\n",
      "epoch: 7 iter: 929 loss: 0.08273 training acc: 0.97592 testing acc: 0.9642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 iter: 930 loss: 0.10910 training acc: 0.96724 testing acc: 0.9586\n",
      "epoch: 7 iter: 931 loss: 0.09268 training acc: 0.97254 testing acc: 0.963\n",
      "epoch: 7 iter: 932 loss: 0.09925 training acc: 0.97012 testing acc: 0.9614\n",
      "epoch: 7 iter: 933 loss: 0.09907 training acc: 0.96994 testing acc: 0.9624\n",
      "epoch: 7 iter: 934 loss: 0.09668 training acc: 0.97104 testing acc: 0.9624\n",
      "epoch: 7 iter: 935 loss: 0.09258 training acc: 0.97238 testing acc: 0.9642\n",
      "epoch: 7 iter: 936 loss: 0.09040 training acc: 0.97226 testing acc: 0.9652\n",
      "epoch: 7 iter: 937 loss: 0.08978 training acc: 0.97276 testing acc: 0.9654\n",
      "epoch: 7 iter: 938 loss: 0.08382 training acc: 0.97570 testing acc: 0.9662\n",
      "epoch: 7 iter: 939 loss: 0.08038 training acc: 0.97670 testing acc: 0.9666\n",
      "epoch: 7 iter: 940 loss: 0.08268 training acc: 0.97582 testing acc: 0.9652\n",
      "epoch: 7 iter: 941 loss: 0.08629 training acc: 0.97502 testing acc: 0.9652\n",
      "epoch: 7 iter: 942 loss: 0.08367 training acc: 0.97598 testing acc: 0.9658\n",
      "epoch: 7 iter: 943 loss: 0.08564 training acc: 0.97486 testing acc: 0.9636\n",
      "epoch: 7 iter: 944 loss: 0.08321 training acc: 0.97592 testing acc: 0.9658\n",
      "epoch: 7 iter: 945 loss: 0.08707 training acc: 0.97486 testing acc: 0.9636\n",
      "epoch: 7 iter: 946 loss: 0.07683 training acc: 0.97828 testing acc: 0.9688\n",
      "epoch: 7 iter: 947 loss: 0.07762 training acc: 0.97824 testing acc: 0.9684\n",
      "epoch: 7 iter: 948 loss: 0.07962 training acc: 0.97738 testing acc: 0.968\n",
      "epoch: 7 iter: 949 loss: 0.07807 training acc: 0.97770 testing acc: 0.968\n",
      "epoch: 7 iter: 950 loss: 0.08126 training acc: 0.97620 testing acc: 0.9664\n",
      "epoch: 7 iter: 951 loss: 0.08186 training acc: 0.97578 testing acc: 0.9664\n",
      "epoch: 7 iter: 952 loss: 0.07997 training acc: 0.97672 testing acc: 0.9666\n",
      "epoch: 7 iter: 953 loss: 0.07894 training acc: 0.97738 testing acc: 0.9672\n",
      "epoch: 7 iter: 954 loss: 0.07730 training acc: 0.97802 testing acc: 0.9666\n",
      "epoch: 7 iter: 955 loss: 0.08829 training acc: 0.97414 testing acc: 0.9638\n",
      "epoch: 7 iter: 956 loss: 0.09563 training acc: 0.97168 testing acc: 0.9604\n",
      "epoch: 7 iter: 957 loss: 0.08630 training acc: 0.97474 testing acc: 0.9644\n",
      "epoch: 7 iter: 958 loss: 0.09073 training acc: 0.97290 testing acc: 0.9646\n",
      "epoch: 7 iter: 959 loss: 0.08815 training acc: 0.97372 testing acc: 0.9652\n",
      "epoch: 7 iter: 960 loss: 0.08399 training acc: 0.97540 testing acc: 0.9668\n",
      "epoch: 7 iter: 961 loss: 0.08484 training acc: 0.97504 testing acc: 0.9668\n",
      "epoch: 7 iter: 962 loss: 0.09262 training acc: 0.97226 testing acc: 0.9628\n",
      "epoch: 7 iter: 963 loss: 0.08793 training acc: 0.97410 testing acc: 0.965\n",
      "epoch: 7 iter: 964 loss: 0.09263 training acc: 0.97198 testing acc: 0.9612\n",
      "epoch: 7 iter: 965 loss: 0.09522 training acc: 0.97074 testing acc: 0.9592\n",
      "epoch: 7 iter: 966 loss: 0.09620 training acc: 0.97056 testing acc: 0.9596\n",
      "epoch: 7 iter: 967 loss: 0.08493 training acc: 0.97488 testing acc: 0.9648\n",
      "epoch: 7 iter: 968 loss: 0.08212 training acc: 0.97604 testing acc: 0.9664\n",
      "epoch: 7 iter: 969 loss: 0.08494 training acc: 0.97516 testing acc: 0.9676\n",
      "epoch: 7 iter: 970 loss: 0.08215 training acc: 0.97522 testing acc: 0.9662\n",
      "epoch: 7 iter: 971 loss: 0.08651 training acc: 0.97374 testing acc: 0.9646\n",
      "epoch: 7 iter: 972 loss: 0.08398 training acc: 0.97460 testing acc: 0.9654\n",
      "epoch: 7 iter: 973 loss: 0.08295 training acc: 0.97546 testing acc: 0.9662\n",
      "epoch: 7 iter: 974 loss: 0.08120 training acc: 0.97608 testing acc: 0.9668\n",
      "epoch: 7 iter: 975 loss: 0.08390 training acc: 0.97512 testing acc: 0.9664\n",
      "epoch: 7 iter: 976 loss: 0.08046 training acc: 0.97646 testing acc: 0.9672\n",
      "epoch: 7 iter: 977 loss: 0.08753 training acc: 0.97430 testing acc: 0.9656\n",
      "epoch: 7 iter: 978 loss: 0.08087 training acc: 0.97650 testing acc: 0.9688\n",
      "epoch: 7 iter: 979 loss: 0.07964 training acc: 0.97730 testing acc: 0.969\n",
      "epoch: 7 iter: 980 loss: 0.08485 training acc: 0.97520 testing acc: 0.9662\n",
      "epoch: 7 iter: 981 loss: 0.09003 training acc: 0.97360 testing acc: 0.9652\n",
      "epoch: 7 iter: 982 loss: 0.08726 training acc: 0.97478 testing acc: 0.9666\n",
      "epoch: 7 iter: 983 loss: 0.08109 training acc: 0.97754 testing acc: 0.97\n",
      "epoch: 7 iter: 984 loss: 0.07893 training acc: 0.97772 testing acc: 0.9686\n",
      "epoch: 7 iter: 985 loss: 0.07788 training acc: 0.97824 testing acc: 0.9692\n",
      "epoch: 7 iter: 986 loss: 0.08430 training acc: 0.97604 testing acc: 0.9672\n",
      "epoch: 7 iter: 987 loss: 0.08385 training acc: 0.97530 testing acc: 0.9676\n",
      "epoch: 7 iter: 988 loss: 0.08132 training acc: 0.97632 testing acc: 0.968\n",
      "epoch: 7 iter: 989 loss: 0.08270 training acc: 0.97614 testing acc: 0.9674\n",
      "epoch: 7 iter: 990 loss: 0.10217 training acc: 0.96912 testing acc: 0.9602\n",
      "epoch: 7 iter: 991 loss: 0.08509 training acc: 0.97572 testing acc: 0.9648\n",
      "epoch: 7 iter: 992 loss: 0.08744 training acc: 0.97456 testing acc: 0.9656\n",
      "epoch: 7 iter: 993 loss: 0.07853 training acc: 0.97790 testing acc: 0.9684\n",
      "epoch: 7 iter: 994 loss: 0.07867 training acc: 0.97810 testing acc: 0.9704\n",
      "epoch: 7 iter: 995 loss: 0.07860 training acc: 0.97810 testing acc: 0.9706\n",
      "epoch: 7 iter: 996 loss: 0.07980 training acc: 0.97702 testing acc: 0.9688\n",
      "epoch: 7 iter: 997 loss: 0.09351 training acc: 0.97188 testing acc: 0.9632\n",
      "epoch: 7 iter: 998 loss: 0.09250 training acc: 0.97266 testing acc: 0.9626\n",
      "epoch: 7 iter: 999 loss: 0.08849 training acc: 0.97440 testing acc: 0.9642\n",
      "epoch: 8 iter:   0 loss: 0.08420 training acc: 0.97590 testing acc: 0.9654\n",
      "epoch: 8 iter:   1 loss: 0.07843 training acc: 0.97762 testing acc: 0.9692\n",
      "epoch: 8 iter:   2 loss: 0.08221 training acc: 0.97606 testing acc: 0.9652\n",
      "epoch: 8 iter:   3 loss: 0.08174 training acc: 0.97596 testing acc: 0.9656\n",
      "epoch: 8 iter:   4 loss: 0.08235 training acc: 0.97664 testing acc: 0.9664\n",
      "epoch: 8 iter:   5 loss: 0.08583 training acc: 0.97528 testing acc: 0.967\n",
      "epoch: 8 iter:   6 loss: 0.08727 training acc: 0.97536 testing acc: 0.9642\n",
      "epoch: 8 iter:   7 loss: 0.08996 training acc: 0.97328 testing acc: 0.9646\n",
      "epoch: 8 iter:   8 loss: 0.08818 training acc: 0.97438 testing acc: 0.965\n",
      "epoch: 8 iter:   9 loss: 0.08690 training acc: 0.97448 testing acc: 0.965\n",
      "epoch: 8 iter:  10 loss: 0.08576 training acc: 0.97534 testing acc: 0.9658\n",
      "epoch: 8 iter:  11 loss: 0.08959 training acc: 0.97314 testing acc: 0.964\n",
      "epoch: 8 iter:  12 loss: 0.08698 training acc: 0.97440 testing acc: 0.965\n",
      "epoch: 8 iter:  13 loss: 0.08527 training acc: 0.97588 testing acc: 0.9656\n",
      "epoch: 8 iter:  14 loss: 0.08265 training acc: 0.97640 testing acc: 0.9658\n",
      "epoch: 8 iter:  15 loss: 0.08943 training acc: 0.97486 testing acc: 0.9652\n",
      "epoch: 8 iter:  16 loss: 0.08328 training acc: 0.97582 testing acc: 0.9678\n",
      "epoch: 8 iter:  17 loss: 0.08146 training acc: 0.97678 testing acc: 0.969\n",
      "epoch: 8 iter:  18 loss: 0.08155 training acc: 0.97668 testing acc: 0.9686\n",
      "epoch: 8 iter:  19 loss: 0.08636 training acc: 0.97532 testing acc: 0.966\n",
      "epoch: 8 iter:  20 loss: 0.08564 training acc: 0.97538 testing acc: 0.9666\n",
      "epoch: 8 iter:  21 loss: 0.08657 training acc: 0.97502 testing acc: 0.9656\n",
      "epoch: 8 iter:  22 loss: 0.08411 training acc: 0.97568 testing acc: 0.9658\n",
      "epoch: 8 iter:  23 loss: 0.07956 training acc: 0.97836 testing acc: 0.9672\n",
      "epoch: 8 iter:  24 loss: 0.08763 training acc: 0.97430 testing acc: 0.9662\n",
      "epoch: 8 iter:  25 loss: 0.09182 training acc: 0.97344 testing acc: 0.9662\n",
      "epoch: 8 iter:  26 loss: 0.08478 training acc: 0.97590 testing acc: 0.9678\n",
      "epoch: 8 iter:  27 loss: 0.08491 training acc: 0.97570 testing acc: 0.967\n",
      "epoch: 8 iter:  28 loss: 0.08735 training acc: 0.97520 testing acc: 0.966\n",
      "epoch: 8 iter:  29 loss: 0.08137 training acc: 0.97716 testing acc: 0.9686\n",
      "epoch: 8 iter:  30 loss: 0.08118 training acc: 0.97752 testing acc: 0.9692\n",
      "epoch: 8 iter:  31 loss: 0.08240 training acc: 0.97688 testing acc: 0.9684\n",
      "epoch: 8 iter:  32 loss: 0.08205 training acc: 0.97696 testing acc: 0.9678\n",
      "epoch: 8 iter:  33 loss: 0.10357 training acc: 0.96994 testing acc: 0.963\n",
      "epoch: 8 iter:  34 loss: 0.10148 training acc: 0.97090 testing acc: 0.9642\n",
      "epoch: 8 iter:  35 loss: 0.09099 training acc: 0.97480 testing acc: 0.9656\n",
      "epoch: 8 iter:  36 loss: 0.08691 training acc: 0.97642 testing acc: 0.967\n",
      "epoch: 8 iter:  37 loss: 0.08872 training acc: 0.97566 testing acc: 0.966\n",
      "epoch: 8 iter:  38 loss: 0.09298 training acc: 0.97340 testing acc: 0.9614\n",
      "epoch: 8 iter:  39 loss: 0.08308 training acc: 0.97624 testing acc: 0.9664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 iter:  40 loss: 0.08324 training acc: 0.97690 testing acc: 0.9676\n",
      "epoch: 8 iter:  41 loss: 0.08093 training acc: 0.97744 testing acc: 0.968\n",
      "epoch: 8 iter:  42 loss: 0.08282 training acc: 0.97686 testing acc: 0.9678\n",
      "epoch: 8 iter:  43 loss: 0.08110 training acc: 0.97742 testing acc: 0.968\n",
      "epoch: 8 iter:  44 loss: 0.08875 training acc: 0.97454 testing acc: 0.9662\n",
      "epoch: 8 iter:  45 loss: 0.08385 training acc: 0.97630 testing acc: 0.967\n",
      "epoch: 8 iter:  46 loss: 0.07722 training acc: 0.97816 testing acc: 0.9698\n",
      "epoch: 8 iter:  47 loss: 0.07658 training acc: 0.97860 testing acc: 0.9698\n",
      "epoch: 8 iter:  48 loss: 0.08188 training acc: 0.97642 testing acc: 0.9692\n",
      "epoch: 8 iter:  49 loss: 0.08035 training acc: 0.97690 testing acc: 0.9686\n",
      "epoch: 8 iter:  50 loss: 0.08283 training acc: 0.97564 testing acc: 0.9664\n",
      "epoch: 8 iter:  51 loss: 0.08916 training acc: 0.97284 testing acc: 0.9654\n",
      "epoch: 8 iter:  52 loss: 0.07842 training acc: 0.97730 testing acc: 0.9666\n",
      "epoch: 8 iter:  53 loss: 0.09351 training acc: 0.97194 testing acc: 0.9614\n",
      "epoch: 8 iter:  54 loss: 0.08324 training acc: 0.97574 testing acc: 0.9658\n",
      "epoch: 8 iter:  55 loss: 0.08326 training acc: 0.97616 testing acc: 0.9668\n",
      "epoch: 8 iter:  56 loss: 0.08240 training acc: 0.97622 testing acc: 0.9668\n",
      "epoch: 8 iter:  57 loss: 0.08182 training acc: 0.97660 testing acc: 0.9672\n",
      "epoch: 8 iter:  58 loss: 0.08111 training acc: 0.97660 testing acc: 0.9666\n",
      "epoch: 8 iter:  59 loss: 0.08855 training acc: 0.97386 testing acc: 0.9658\n",
      "epoch: 8 iter:  60 loss: 0.08222 training acc: 0.97652 testing acc: 0.9664\n",
      "epoch: 8 iter:  61 loss: 0.08240 training acc: 0.97608 testing acc: 0.9654\n",
      "epoch: 8 iter:  62 loss: 0.08474 training acc: 0.97522 testing acc: 0.965\n",
      "epoch: 8 iter:  63 loss: 0.08421 training acc: 0.97564 testing acc: 0.9658\n",
      "epoch: 8 iter:  64 loss: 0.08729 training acc: 0.97424 testing acc: 0.9634\n",
      "epoch: 8 iter:  65 loss: 0.09332 training acc: 0.97300 testing acc: 0.9608\n",
      "epoch: 8 iter:  66 loss: 0.08599 training acc: 0.97502 testing acc: 0.9632\n",
      "epoch: 8 iter:  67 loss: 0.08218 training acc: 0.97588 testing acc: 0.9612\n",
      "epoch: 8 iter:  68 loss: 0.07897 training acc: 0.97722 testing acc: 0.9648\n",
      "epoch: 8 iter:  69 loss: 0.08598 training acc: 0.97448 testing acc: 0.962\n",
      "epoch: 8 iter:  70 loss: 0.08528 training acc: 0.97512 testing acc: 0.961\n",
      "epoch: 8 iter:  71 loss: 0.08405 training acc: 0.97538 testing acc: 0.9624\n",
      "epoch: 8 iter:  72 loss: 0.10810 training acc: 0.96698 testing acc: 0.9532\n",
      "epoch: 8 iter:  73 loss: 0.08908 training acc: 0.97394 testing acc: 0.962\n",
      "epoch: 8 iter:  74 loss: 0.08305 training acc: 0.97610 testing acc: 0.9642\n",
      "epoch: 8 iter:  75 loss: 0.08368 training acc: 0.97626 testing acc: 0.964\n",
      "epoch: 8 iter:  76 loss: 0.08636 training acc: 0.97506 testing acc: 0.9656\n",
      "epoch: 8 iter:  77 loss: 0.07928 training acc: 0.97782 testing acc: 0.9668\n",
      "epoch: 8 iter:  78 loss: 0.08744 training acc: 0.97424 testing acc: 0.9634\n",
      "epoch: 8 iter:  79 loss: 0.08608 training acc: 0.97512 testing acc: 0.9638\n",
      "epoch: 8 iter:  80 loss: 0.08139 training acc: 0.97690 testing acc: 0.9668\n",
      "epoch: 8 iter:  81 loss: 0.08463 training acc: 0.97584 testing acc: 0.9638\n",
      "epoch: 8 iter:  82 loss: 0.08532 training acc: 0.97556 testing acc: 0.9646\n",
      "epoch: 8 iter:  83 loss: 0.08091 training acc: 0.97666 testing acc: 0.9676\n",
      "epoch: 8 iter:  84 loss: 0.08329 training acc: 0.97636 testing acc: 0.9664\n",
      "epoch: 8 iter:  85 loss: 0.08214 training acc: 0.97628 testing acc: 0.966\n",
      "epoch: 8 iter:  86 loss: 0.07919 training acc: 0.97724 testing acc: 0.9672\n",
      "epoch: 8 iter:  87 loss: 0.07858 training acc: 0.97770 testing acc: 0.9678\n",
      "epoch: 8 iter:  88 loss: 0.07946 training acc: 0.97696 testing acc: 0.9674\n",
      "epoch: 8 iter:  89 loss: 0.07858 training acc: 0.97742 testing acc: 0.9674\n",
      "epoch: 8 iter:  90 loss: 0.07751 training acc: 0.97772 testing acc: 0.9672\n",
      "epoch: 8 iter:  91 loss: 0.07684 training acc: 0.97804 testing acc: 0.9686\n",
      "epoch: 8 iter:  92 loss: 0.07681 training acc: 0.97818 testing acc: 0.9698\n",
      "epoch: 8 iter:  93 loss: 0.07716 training acc: 0.97818 testing acc: 0.967\n",
      "epoch: 8 iter:  94 loss: 0.08112 training acc: 0.97692 testing acc: 0.967\n",
      "epoch: 8 iter:  95 loss: 0.07980 training acc: 0.97734 testing acc: 0.9676\n",
      "epoch: 8 iter:  96 loss: 0.08101 training acc: 0.97670 testing acc: 0.9648\n",
      "epoch: 8 iter:  97 loss: 0.08149 training acc: 0.97630 testing acc: 0.9648\n",
      "epoch: 8 iter:  98 loss: 0.08046 training acc: 0.97648 testing acc: 0.9664\n",
      "epoch: 8 iter:  99 loss: 0.07636 training acc: 0.97808 testing acc: 0.9674\n",
      "epoch: 8 iter: 100 loss: 0.07619 training acc: 0.97826 testing acc: 0.9658\n",
      "epoch: 8 iter: 101 loss: 0.07541 training acc: 0.97878 testing acc: 0.9666\n",
      "epoch: 8 iter: 102 loss: 0.07692 training acc: 0.97828 testing acc: 0.966\n",
      "epoch: 8 iter: 103 loss: 0.08068 training acc: 0.97676 testing acc: 0.9676\n",
      "epoch: 8 iter: 104 loss: 0.08014 training acc: 0.97632 testing acc: 0.9658\n",
      "epoch: 8 iter: 105 loss: 0.08820 training acc: 0.97296 testing acc: 0.9642\n",
      "epoch: 8 iter: 106 loss: 0.07937 training acc: 0.97664 testing acc: 0.9646\n",
      "epoch: 8 iter: 107 loss: 0.07900 training acc: 0.97680 testing acc: 0.9656\n",
      "epoch: 8 iter: 108 loss: 0.09269 training acc: 0.97150 testing acc: 0.9602\n",
      "epoch: 8 iter: 109 loss: 0.09852 training acc: 0.97026 testing acc: 0.959\n",
      "epoch: 8 iter: 110 loss: 0.09653 training acc: 0.97074 testing acc: 0.9582\n",
      "epoch: 8 iter: 111 loss: 0.08487 training acc: 0.97458 testing acc: 0.9616\n",
      "epoch: 8 iter: 112 loss: 0.08234 training acc: 0.97626 testing acc: 0.9652\n",
      "epoch: 8 iter: 113 loss: 0.08307 training acc: 0.97562 testing acc: 0.964\n",
      "epoch: 8 iter: 114 loss: 0.08340 training acc: 0.97562 testing acc: 0.9638\n",
      "epoch: 8 iter: 115 loss: 0.07771 training acc: 0.97768 testing acc: 0.9662\n",
      "epoch: 8 iter: 116 loss: 0.07887 training acc: 0.97716 testing acc: 0.965\n",
      "epoch: 8 iter: 117 loss: 0.08930 training acc: 0.97368 testing acc: 0.9638\n",
      "epoch: 8 iter: 118 loss: 0.07974 training acc: 0.97658 testing acc: 0.966\n",
      "epoch: 8 iter: 119 loss: 0.08106 training acc: 0.97586 testing acc: 0.963\n",
      "epoch: 8 iter: 120 loss: 0.07798 training acc: 0.97680 testing acc: 0.9658\n",
      "epoch: 8 iter: 121 loss: 0.07834 training acc: 0.97684 testing acc: 0.965\n",
      "epoch: 8 iter: 122 loss: 0.08010 training acc: 0.97586 testing acc: 0.9646\n",
      "epoch: 8 iter: 123 loss: 0.08236 training acc: 0.97474 testing acc: 0.9652\n",
      "epoch: 8 iter: 124 loss: 0.08290 training acc: 0.97500 testing acc: 0.9646\n",
      "epoch: 8 iter: 125 loss: 0.07797 training acc: 0.97674 testing acc: 0.9644\n",
      "epoch: 8 iter: 126 loss: 0.07847 training acc: 0.97688 testing acc: 0.9644\n",
      "epoch: 8 iter: 127 loss: 0.08307 training acc: 0.97536 testing acc: 0.9642\n",
      "epoch: 8 iter: 128 loss: 0.07769 training acc: 0.97750 testing acc: 0.9664\n",
      "epoch: 8 iter: 129 loss: 0.07724 training acc: 0.97768 testing acc: 0.9666\n",
      "epoch: 8 iter: 130 loss: 0.08667 training acc: 0.97368 testing acc: 0.9638\n",
      "epoch: 8 iter: 131 loss: 0.07990 training acc: 0.97670 testing acc: 0.967\n",
      "epoch: 8 iter: 132 loss: 0.08454 training acc: 0.97614 testing acc: 0.9634\n",
      "epoch: 8 iter: 133 loss: 0.08692 training acc: 0.97498 testing acc: 0.9618\n",
      "epoch: 8 iter: 134 loss: 0.08342 training acc: 0.97626 testing acc: 0.9644\n",
      "epoch: 8 iter: 135 loss: 0.08033 training acc: 0.97668 testing acc: 0.9664\n",
      "epoch: 8 iter: 136 loss: 0.08046 training acc: 0.97754 testing acc: 0.9648\n",
      "epoch: 8 iter: 137 loss: 0.07905 training acc: 0.97766 testing acc: 0.9648\n",
      "epoch: 8 iter: 138 loss: 0.07775 training acc: 0.97808 testing acc: 0.967\n",
      "epoch: 8 iter: 139 loss: 0.08573 training acc: 0.97454 testing acc: 0.9606\n",
      "epoch: 8 iter: 140 loss: 0.08792 training acc: 0.97340 testing acc: 0.9614\n",
      "epoch: 8 iter: 141 loss: 0.08028 training acc: 0.97690 testing acc: 0.9658\n",
      "epoch: 8 iter: 142 loss: 0.08151 training acc: 0.97630 testing acc: 0.965\n",
      "epoch: 8 iter: 143 loss: 0.07983 training acc: 0.97722 testing acc: 0.9656\n",
      "epoch: 8 iter: 144 loss: 0.07797 training acc: 0.97802 testing acc: 0.9678\n",
      "epoch: 8 iter: 145 loss: 0.08447 training acc: 0.97562 testing acc: 0.9656\n",
      "epoch: 8 iter: 146 loss: 0.08033 training acc: 0.97670 testing acc: 0.966\n",
      "epoch: 8 iter: 147 loss: 0.09951 training acc: 0.96944 testing acc: 0.9572\n",
      "epoch: 8 iter: 148 loss: 0.08973 training acc: 0.97364 testing acc: 0.9594\n",
      "epoch: 8 iter: 149 loss: 0.07937 training acc: 0.97680 testing acc: 0.9642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 iter: 150 loss: 0.07725 training acc: 0.97794 testing acc: 0.9674\n",
      "epoch: 8 iter: 151 loss: 0.07995 training acc: 0.97694 testing acc: 0.9656\n",
      "epoch: 8 iter: 152 loss: 0.07913 training acc: 0.97720 testing acc: 0.9674\n",
      "epoch: 8 iter: 153 loss: 0.08584 training acc: 0.97398 testing acc: 0.9644\n",
      "epoch: 8 iter: 154 loss: 0.08277 training acc: 0.97504 testing acc: 0.9654\n",
      "epoch: 8 iter: 155 loss: 0.07901 training acc: 0.97614 testing acc: 0.9666\n",
      "epoch: 8 iter: 156 loss: 0.07758 training acc: 0.97706 testing acc: 0.9662\n",
      "epoch: 8 iter: 157 loss: 0.07440 training acc: 0.97842 testing acc: 0.968\n",
      "epoch: 8 iter: 158 loss: 0.08783 training acc: 0.97392 testing acc: 0.9638\n",
      "epoch: 8 iter: 159 loss: 0.07742 training acc: 0.97736 testing acc: 0.9676\n",
      "epoch: 8 iter: 160 loss: 0.07714 training acc: 0.97752 testing acc: 0.968\n",
      "epoch: 8 iter: 161 loss: 0.07754 training acc: 0.97714 testing acc: 0.9674\n",
      "epoch: 8 iter: 162 loss: 0.07851 training acc: 0.97690 testing acc: 0.9664\n",
      "epoch: 8 iter: 163 loss: 0.08205 training acc: 0.97482 testing acc: 0.9654\n",
      "epoch: 8 iter: 164 loss: 0.08334 training acc: 0.97516 testing acc: 0.9664\n",
      "epoch: 8 iter: 165 loss: 0.07591 training acc: 0.97822 testing acc: 0.9692\n",
      "epoch: 8 iter: 166 loss: 0.07782 training acc: 0.97746 testing acc: 0.9688\n",
      "epoch: 8 iter: 167 loss: 0.08006 training acc: 0.97664 testing acc: 0.9682\n",
      "epoch: 8 iter: 168 loss: 0.08189 training acc: 0.97520 testing acc: 0.9654\n",
      "epoch: 8 iter: 169 loss: 0.08205 training acc: 0.97534 testing acc: 0.9658\n",
      "epoch: 8 iter: 170 loss: 0.07788 training acc: 0.97734 testing acc: 0.9684\n",
      "epoch: 8 iter: 171 loss: 0.07810 training acc: 0.97746 testing acc: 0.9694\n",
      "epoch: 8 iter: 172 loss: 0.12385 training acc: 0.96240 testing acc: 0.9522\n",
      "epoch: 8 iter: 173 loss: 0.08670 training acc: 0.97450 testing acc: 0.9654\n",
      "epoch: 8 iter: 174 loss: 0.08293 training acc: 0.97594 testing acc: 0.966\n",
      "epoch: 8 iter: 175 loss: 0.08254 training acc: 0.97554 testing acc: 0.9652\n",
      "epoch: 8 iter: 176 loss: 0.09215 training acc: 0.97200 testing acc: 0.9614\n",
      "epoch: 8 iter: 177 loss: 0.08919 training acc: 0.97318 testing acc: 0.9628\n",
      "epoch: 8 iter: 178 loss: 0.08527 training acc: 0.97474 testing acc: 0.9642\n",
      "epoch: 8 iter: 179 loss: 0.08833 training acc: 0.97372 testing acc: 0.964\n",
      "epoch: 8 iter: 180 loss: 0.08169 training acc: 0.97646 testing acc: 0.9662\n",
      "epoch: 8 iter: 181 loss: 0.08448 training acc: 0.97496 testing acc: 0.9652\n",
      "epoch: 8 iter: 182 loss: 0.08213 training acc: 0.97548 testing acc: 0.9656\n",
      "epoch: 8 iter: 183 loss: 0.07642 training acc: 0.97778 testing acc: 0.9664\n",
      "epoch: 8 iter: 184 loss: 0.07572 training acc: 0.97806 testing acc: 0.9672\n",
      "epoch: 8 iter: 185 loss: 0.07611 training acc: 0.97812 testing acc: 0.969\n",
      "epoch: 8 iter: 186 loss: 0.07445 training acc: 0.97878 testing acc: 0.9696\n",
      "epoch: 8 iter: 187 loss: 0.07761 training acc: 0.97744 testing acc: 0.9674\n",
      "epoch: 8 iter: 188 loss: 0.08323 training acc: 0.97550 testing acc: 0.9668\n",
      "epoch: 8 iter: 189 loss: 0.08726 training acc: 0.97410 testing acc: 0.9642\n",
      "epoch: 8 iter: 190 loss: 0.07838 training acc: 0.97716 testing acc: 0.9668\n",
      "epoch: 8 iter: 191 loss: 0.08697 training acc: 0.97410 testing acc: 0.9636\n",
      "epoch: 8 iter: 192 loss: 0.07945 training acc: 0.97658 testing acc: 0.966\n",
      "epoch: 8 iter: 193 loss: 0.07729 training acc: 0.97698 testing acc: 0.9658\n",
      "epoch: 8 iter: 194 loss: 0.07815 training acc: 0.97700 testing acc: 0.9646\n",
      "epoch: 8 iter: 195 loss: 0.07753 training acc: 0.97728 testing acc: 0.9646\n",
      "epoch: 8 iter: 196 loss: 0.07679 training acc: 0.97750 testing acc: 0.9656\n",
      "epoch: 8 iter: 197 loss: 0.07707 training acc: 0.97746 testing acc: 0.964\n",
      "epoch: 8 iter: 198 loss: 0.07645 training acc: 0.97740 testing acc: 0.9662\n",
      "epoch: 8 iter: 199 loss: 0.07879 training acc: 0.97576 testing acc: 0.965\n",
      "epoch: 8 iter: 200 loss: 0.08017 training acc: 0.97548 testing acc: 0.9648\n",
      "epoch: 8 iter: 201 loss: 0.08596 training acc: 0.97408 testing acc: 0.9634\n",
      "epoch: 8 iter: 202 loss: 0.12024 training acc: 0.96238 testing acc: 0.9552\n",
      "epoch: 8 iter: 203 loss: 0.11175 training acc: 0.96564 testing acc: 0.9584\n",
      "epoch: 8 iter: 204 loss: 0.08708 training acc: 0.97400 testing acc: 0.9658\n",
      "epoch: 8 iter: 205 loss: 0.08252 training acc: 0.97538 testing acc: 0.9656\n",
      "epoch: 8 iter: 206 loss: 0.08175 training acc: 0.97590 testing acc: 0.9672\n",
      "epoch: 8 iter: 207 loss: 0.09109 training acc: 0.97092 testing acc: 0.9612\n",
      "epoch: 8 iter: 208 loss: 0.09267 training acc: 0.97154 testing acc: 0.9614\n",
      "epoch: 8 iter: 209 loss: 0.07858 training acc: 0.97694 testing acc: 0.9644\n",
      "epoch: 8 iter: 210 loss: 0.07829 training acc: 0.97668 testing acc: 0.967\n",
      "epoch: 8 iter: 211 loss: 0.07817 training acc: 0.97674 testing acc: 0.9656\n",
      "epoch: 8 iter: 212 loss: 0.07767 training acc: 0.97694 testing acc: 0.9666\n",
      "epoch: 8 iter: 213 loss: 0.08200 training acc: 0.97602 testing acc: 0.9672\n",
      "epoch: 8 iter: 214 loss: 0.07961 training acc: 0.97718 testing acc: 0.9658\n",
      "epoch: 8 iter: 215 loss: 0.08103 training acc: 0.97608 testing acc: 0.964\n",
      "epoch: 8 iter: 216 loss: 0.08229 training acc: 0.97538 testing acc: 0.9638\n",
      "epoch: 8 iter: 217 loss: 0.08445 training acc: 0.97490 testing acc: 0.9654\n",
      "epoch: 8 iter: 218 loss: 0.08102 training acc: 0.97614 testing acc: 0.9648\n",
      "epoch: 8 iter: 219 loss: 0.08607 training acc: 0.97388 testing acc: 0.964\n",
      "epoch: 8 iter: 220 loss: 0.08322 training acc: 0.97532 testing acc: 0.9648\n",
      "epoch: 8 iter: 221 loss: 0.08332 training acc: 0.97572 testing acc: 0.964\n",
      "epoch: 8 iter: 222 loss: 0.08231 training acc: 0.97606 testing acc: 0.9674\n",
      "epoch: 8 iter: 223 loss: 0.07934 training acc: 0.97682 testing acc: 0.9674\n",
      "epoch: 8 iter: 224 loss: 0.07847 training acc: 0.97688 testing acc: 0.969\n",
      "epoch: 8 iter: 225 loss: 0.07840 training acc: 0.97712 testing acc: 0.9686\n",
      "epoch: 8 iter: 226 loss: 0.07831 training acc: 0.97710 testing acc: 0.968\n",
      "epoch: 8 iter: 227 loss: 0.07939 training acc: 0.97664 testing acc: 0.9682\n",
      "epoch: 8 iter: 228 loss: 0.08254 training acc: 0.97534 testing acc: 0.9674\n",
      "epoch: 8 iter: 229 loss: 0.08359 training acc: 0.97510 testing acc: 0.9654\n",
      "epoch: 8 iter: 230 loss: 0.07894 training acc: 0.97656 testing acc: 0.9678\n",
      "epoch: 8 iter: 231 loss: 0.07989 training acc: 0.97624 testing acc: 0.967\n",
      "epoch: 8 iter: 232 loss: 0.07810 training acc: 0.97680 testing acc: 0.9672\n",
      "epoch: 8 iter: 233 loss: 0.07885 training acc: 0.97648 testing acc: 0.9676\n",
      "epoch: 8 iter: 234 loss: 0.07673 training acc: 0.97720 testing acc: 0.9682\n",
      "epoch: 8 iter: 235 loss: 0.07651 training acc: 0.97760 testing acc: 0.9688\n",
      "epoch: 8 iter: 236 loss: 0.08085 training acc: 0.97594 testing acc: 0.966\n",
      "epoch: 8 iter: 237 loss: 0.07949 training acc: 0.97614 testing acc: 0.967\n",
      "epoch: 8 iter: 238 loss: 0.07905 training acc: 0.97628 testing acc: 0.9676\n",
      "epoch: 8 iter: 239 loss: 0.07750 training acc: 0.97702 testing acc: 0.9668\n",
      "epoch: 8 iter: 240 loss: 0.07749 training acc: 0.97722 testing acc: 0.9666\n",
      "epoch: 8 iter: 241 loss: 0.07661 training acc: 0.97746 testing acc: 0.9668\n",
      "epoch: 8 iter: 242 loss: 0.08410 training acc: 0.97438 testing acc: 0.9638\n",
      "epoch: 8 iter: 243 loss: 0.08981 training acc: 0.97268 testing acc: 0.9622\n",
      "epoch: 8 iter: 244 loss: 0.08637 training acc: 0.97408 testing acc: 0.9634\n",
      "epoch: 8 iter: 245 loss: 0.10206 training acc: 0.96724 testing acc: 0.9578\n",
      "epoch: 8 iter: 246 loss: 0.09149 training acc: 0.97192 testing acc: 0.9616\n",
      "epoch: 8 iter: 247 loss: 0.07751 training acc: 0.97724 testing acc: 0.967\n",
      "epoch: 8 iter: 248 loss: 0.08322 training acc: 0.97506 testing acc: 0.9654\n",
      "epoch: 8 iter: 249 loss: 0.10193 training acc: 0.96940 testing acc: 0.9596\n",
      "epoch: 8 iter: 250 loss: 0.09314 training acc: 0.97224 testing acc: 0.9628\n",
      "epoch: 8 iter: 251 loss: 0.08407 training acc: 0.97480 testing acc: 0.964\n",
      "epoch: 8 iter: 252 loss: 0.09165 training acc: 0.97190 testing acc: 0.9626\n",
      "epoch: 8 iter: 253 loss: 0.08043 training acc: 0.97616 testing acc: 0.967\n",
      "epoch: 8 iter: 254 loss: 0.11383 training acc: 0.96452 testing acc: 0.9536\n",
      "epoch: 8 iter: 255 loss: 0.10464 training acc: 0.96874 testing acc: 0.9564\n",
      "epoch: 8 iter: 256 loss: 0.09729 training acc: 0.97104 testing acc: 0.9588\n",
      "epoch: 8 iter: 257 loss: 0.09353 training acc: 0.97230 testing acc: 0.9584\n",
      "epoch: 8 iter: 258 loss: 0.08410 training acc: 0.97488 testing acc: 0.9612\n",
      "epoch: 8 iter: 259 loss: 0.08512 training acc: 0.97378 testing acc: 0.9594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 iter: 260 loss: 0.08681 training acc: 0.97324 testing acc: 0.9612\n",
      "epoch: 8 iter: 261 loss: 0.09531 training acc: 0.97064 testing acc: 0.9596\n",
      "epoch: 8 iter: 262 loss: 0.09414 training acc: 0.97106 testing acc: 0.9616\n",
      "epoch: 8 iter: 263 loss: 0.09405 training acc: 0.97132 testing acc: 0.9612\n",
      "epoch: 8 iter: 264 loss: 0.09224 training acc: 0.97184 testing acc: 0.9608\n",
      "epoch: 8 iter: 265 loss: 0.08942 training acc: 0.97326 testing acc: 0.9628\n",
      "epoch: 8 iter: 266 loss: 0.08509 training acc: 0.97480 testing acc: 0.9646\n",
      "epoch: 8 iter: 267 loss: 0.07712 training acc: 0.97718 testing acc: 0.9648\n",
      "epoch: 8 iter: 268 loss: 0.07710 training acc: 0.97748 testing acc: 0.9658\n",
      "epoch: 8 iter: 269 loss: 0.08862 training acc: 0.97370 testing acc: 0.9624\n",
      "epoch: 8 iter: 270 loss: 0.08503 training acc: 0.97500 testing acc: 0.9642\n",
      "epoch: 8 iter: 271 loss: 0.07939 training acc: 0.97752 testing acc: 0.9654\n",
      "epoch: 8 iter: 272 loss: 0.07527 training acc: 0.97872 testing acc: 0.9688\n",
      "epoch: 8 iter: 273 loss: 0.07495 training acc: 0.97884 testing acc: 0.9682\n",
      "epoch: 8 iter: 274 loss: 0.07595 training acc: 0.97808 testing acc: 0.9682\n",
      "epoch: 8 iter: 275 loss: 0.07643 training acc: 0.97820 testing acc: 0.968\n",
      "epoch: 8 iter: 276 loss: 0.07711 training acc: 0.97784 testing acc: 0.9678\n",
      "epoch: 8 iter: 277 loss: 0.07885 training acc: 0.97744 testing acc: 0.967\n",
      "epoch: 8 iter: 278 loss: 0.07841 training acc: 0.97692 testing acc: 0.9656\n",
      "epoch: 8 iter: 279 loss: 0.07805 training acc: 0.97706 testing acc: 0.9652\n",
      "epoch: 8 iter: 280 loss: 0.07561 training acc: 0.97814 testing acc: 0.966\n",
      "epoch: 8 iter: 281 loss: 0.09621 training acc: 0.96940 testing acc: 0.9594\n",
      "epoch: 8 iter: 282 loss: 0.08834 training acc: 0.97260 testing acc: 0.9624\n",
      "epoch: 8 iter: 283 loss: 0.07422 training acc: 0.97864 testing acc: 0.9654\n",
      "epoch: 8 iter: 284 loss: 0.07754 training acc: 0.97756 testing acc: 0.9668\n",
      "epoch: 8 iter: 285 loss: 0.07766 training acc: 0.97752 testing acc: 0.9664\n",
      "epoch: 8 iter: 286 loss: 0.07624 training acc: 0.97816 testing acc: 0.9688\n",
      "epoch: 8 iter: 287 loss: 0.08009 training acc: 0.97674 testing acc: 0.966\n",
      "epoch: 8 iter: 288 loss: 0.07931 training acc: 0.97710 testing acc: 0.9658\n",
      "epoch: 8 iter: 289 loss: 0.07563 training acc: 0.97782 testing acc: 0.9682\n",
      "epoch: 8 iter: 290 loss: 0.07543 training acc: 0.97814 testing acc: 0.9696\n",
      "epoch: 8 iter: 291 loss: 0.07595 training acc: 0.97820 testing acc: 0.9686\n",
      "epoch: 8 iter: 292 loss: 0.07800 training acc: 0.97722 testing acc: 0.9656\n",
      "epoch: 8 iter: 293 loss: 0.07711 training acc: 0.97754 testing acc: 0.966\n",
      "epoch: 8 iter: 294 loss: 0.07743 training acc: 0.97754 testing acc: 0.9664\n",
      "epoch: 8 iter: 295 loss: 0.07644 training acc: 0.97800 testing acc: 0.9668\n",
      "epoch: 8 iter: 296 loss: 0.08177 training acc: 0.97612 testing acc: 0.9676\n",
      "epoch: 8 iter: 297 loss: 0.08046 training acc: 0.97644 testing acc: 0.9664\n",
      "epoch: 8 iter: 298 loss: 0.07674 training acc: 0.97810 testing acc: 0.9694\n",
      "epoch: 8 iter: 299 loss: 0.07626 training acc: 0.97800 testing acc: 0.9696\n",
      "epoch: 8 iter: 300 loss: 0.07631 training acc: 0.97810 testing acc: 0.969\n",
      "epoch: 8 iter: 301 loss: 0.08562 training acc: 0.97462 testing acc: 0.9658\n",
      "epoch: 8 iter: 302 loss: 0.08747 training acc: 0.97384 testing acc: 0.9664\n",
      "epoch: 8 iter: 303 loss: 0.08894 training acc: 0.97366 testing acc: 0.9652\n",
      "epoch: 8 iter: 304 loss: 0.08071 training acc: 0.97692 testing acc: 0.9668\n",
      "epoch: 8 iter: 305 loss: 0.08073 training acc: 0.97688 testing acc: 0.9664\n",
      "epoch: 8 iter: 306 loss: 0.07909 training acc: 0.97774 testing acc: 0.966\n",
      "epoch: 8 iter: 307 loss: 0.07279 training acc: 0.97912 testing acc: 0.9684\n",
      "epoch: 8 iter: 308 loss: 0.07625 training acc: 0.97838 testing acc: 0.9674\n",
      "epoch: 8 iter: 309 loss: 0.07267 training acc: 0.97968 testing acc: 0.9692\n",
      "epoch: 8 iter: 310 loss: 0.07132 training acc: 0.97988 testing acc: 0.9684\n",
      "epoch: 8 iter: 311 loss: 0.07232 training acc: 0.97950 testing acc: 0.9688\n",
      "epoch: 8 iter: 312 loss: 0.07258 training acc: 0.97968 testing acc: 0.9686\n",
      "epoch: 8 iter: 313 loss: 0.07277 training acc: 0.97942 testing acc: 0.969\n",
      "epoch: 8 iter: 314 loss: 0.07346 training acc: 0.97896 testing acc: 0.9688\n",
      "epoch: 8 iter: 315 loss: 0.07693 training acc: 0.97740 testing acc: 0.9672\n",
      "epoch: 8 iter: 316 loss: 0.07317 training acc: 0.97892 testing acc: 0.9688\n",
      "epoch: 8 iter: 317 loss: 0.07744 training acc: 0.97796 testing acc: 0.9666\n",
      "epoch: 8 iter: 318 loss: 0.08618 training acc: 0.97384 testing acc: 0.9616\n",
      "epoch: 8 iter: 319 loss: 0.08248 training acc: 0.97566 testing acc: 0.9628\n",
      "epoch: 8 iter: 320 loss: 0.07781 training acc: 0.97746 testing acc: 0.9662\n",
      "epoch: 8 iter: 321 loss: 0.08245 training acc: 0.97560 testing acc: 0.9638\n",
      "epoch: 8 iter: 322 loss: 0.08671 training acc: 0.97368 testing acc: 0.9636\n",
      "epoch: 8 iter: 323 loss: 0.07973 training acc: 0.97580 testing acc: 0.9654\n",
      "epoch: 8 iter: 324 loss: 0.07496 training acc: 0.97882 testing acc: 0.9668\n",
      "epoch: 8 iter: 325 loss: 0.07565 training acc: 0.97834 testing acc: 0.9668\n",
      "epoch: 8 iter: 326 loss: 0.08129 training acc: 0.97584 testing acc: 0.9642\n",
      "epoch: 8 iter: 327 loss: 0.07608 training acc: 0.97768 testing acc: 0.9652\n",
      "epoch: 8 iter: 328 loss: 0.07979 training acc: 0.97658 testing acc: 0.9672\n",
      "epoch: 8 iter: 329 loss: 0.07918 training acc: 0.97710 testing acc: 0.9668\n",
      "epoch: 8 iter: 330 loss: 0.07952 training acc: 0.97708 testing acc: 0.9666\n",
      "epoch: 8 iter: 331 loss: 0.08005 training acc: 0.97708 testing acc: 0.9664\n",
      "epoch: 8 iter: 332 loss: 0.07449 training acc: 0.97954 testing acc: 0.9682\n",
      "epoch: 8 iter: 333 loss: 0.07258 training acc: 0.97988 testing acc: 0.969\n",
      "epoch: 8 iter: 334 loss: 0.07198 training acc: 0.97994 testing acc: 0.9688\n",
      "epoch: 8 iter: 335 loss: 0.07361 training acc: 0.97924 testing acc: 0.9688\n",
      "epoch: 8 iter: 336 loss: 0.07263 training acc: 0.97960 testing acc: 0.969\n",
      "epoch: 8 iter: 337 loss: 0.07627 training acc: 0.97824 testing acc: 0.9676\n",
      "epoch: 8 iter: 338 loss: 0.07612 training acc: 0.97814 testing acc: 0.9696\n",
      "epoch: 8 iter: 339 loss: 0.07772 training acc: 0.97814 testing acc: 0.9684\n",
      "epoch: 8 iter: 340 loss: 0.07664 training acc: 0.97812 testing acc: 0.9678\n",
      "epoch: 8 iter: 341 loss: 0.07783 training acc: 0.97794 testing acc: 0.966\n",
      "epoch: 8 iter: 342 loss: 0.07644 training acc: 0.97800 testing acc: 0.9676\n",
      "epoch: 8 iter: 343 loss: 0.08016 training acc: 0.97698 testing acc: 0.9668\n",
      "epoch: 8 iter: 344 loss: 0.08255 training acc: 0.97600 testing acc: 0.9664\n",
      "epoch: 8 iter: 345 loss: 0.08347 training acc: 0.97570 testing acc: 0.9658\n",
      "epoch: 8 iter: 346 loss: 0.08113 training acc: 0.97648 testing acc: 0.9676\n",
      "epoch: 8 iter: 347 loss: 0.07831 training acc: 0.97772 testing acc: 0.969\n",
      "epoch: 8 iter: 348 loss: 0.07558 training acc: 0.97876 testing acc: 0.9702\n",
      "epoch: 8 iter: 349 loss: 0.07425 training acc: 0.97934 testing acc: 0.97\n",
      "epoch: 8 iter: 350 loss: 0.07850 training acc: 0.97752 testing acc: 0.9684\n",
      "epoch: 8 iter: 351 loss: 0.10807 training acc: 0.96604 testing acc: 0.954\n",
      "epoch: 8 iter: 352 loss: 0.07957 training acc: 0.97648 testing acc: 0.966\n",
      "epoch: 8 iter: 353 loss: 0.07751 training acc: 0.97748 testing acc: 0.966\n",
      "epoch: 8 iter: 354 loss: 0.07826 training acc: 0.97754 testing acc: 0.9662\n",
      "epoch: 8 iter: 355 loss: 0.07434 training acc: 0.97874 testing acc: 0.9682\n",
      "epoch: 8 iter: 356 loss: 0.07689 training acc: 0.97770 testing acc: 0.9672\n",
      "epoch: 8 iter: 357 loss: 0.07607 training acc: 0.97772 testing acc: 0.967\n",
      "epoch: 8 iter: 358 loss: 0.07539 training acc: 0.97816 testing acc: 0.9658\n",
      "epoch: 8 iter: 359 loss: 0.07472 training acc: 0.97840 testing acc: 0.9678\n",
      "epoch: 8 iter: 360 loss: 0.07505 training acc: 0.97810 testing acc: 0.9676\n",
      "epoch: 8 iter: 361 loss: 0.07407 training acc: 0.97846 testing acc: 0.9664\n",
      "epoch: 8 iter: 362 loss: 0.07602 training acc: 0.97734 testing acc: 0.9654\n",
      "epoch: 8 iter: 363 loss: 0.07465 training acc: 0.97828 testing acc: 0.9666\n",
      "epoch: 8 iter: 364 loss: 0.07349 training acc: 0.97914 testing acc: 0.9678\n",
      "epoch: 8 iter: 365 loss: 0.07518 training acc: 0.97830 testing acc: 0.9668\n",
      "epoch: 8 iter: 366 loss: 0.07504 training acc: 0.97852 testing acc: 0.9674\n",
      "epoch: 8 iter: 367 loss: 0.07587 training acc: 0.97794 testing acc: 0.9676\n",
      "epoch: 8 iter: 368 loss: 0.07955 training acc: 0.97620 testing acc: 0.9646\n",
      "epoch: 8 iter: 369 loss: 0.07754 training acc: 0.97684 testing acc: 0.9642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 iter: 370 loss: 0.07900 training acc: 0.97638 testing acc: 0.9642\n",
      "epoch: 8 iter: 371 loss: 0.07918 training acc: 0.97696 testing acc: 0.9638\n",
      "epoch: 8 iter: 372 loss: 0.07461 training acc: 0.97858 testing acc: 0.9678\n",
      "epoch: 8 iter: 373 loss: 0.07406 training acc: 0.97904 testing acc: 0.9686\n",
      "epoch: 8 iter: 374 loss: 0.07210 training acc: 0.97906 testing acc: 0.9688\n",
      "epoch: 8 iter: 375 loss: 0.07620 training acc: 0.97774 testing acc: 0.967\n",
      "epoch: 8 iter: 376 loss: 0.09154 training acc: 0.97204 testing acc: 0.962\n",
      "epoch: 8 iter: 377 loss: 0.09137 training acc: 0.97174 testing acc: 0.9616\n",
      "epoch: 8 iter: 378 loss: 0.08722 training acc: 0.97318 testing acc: 0.9642\n",
      "epoch: 8 iter: 379 loss: 0.08597 training acc: 0.97398 testing acc: 0.9646\n",
      "epoch: 8 iter: 380 loss: 0.08099 training acc: 0.97568 testing acc: 0.9658\n",
      "epoch: 8 iter: 381 loss: 0.08085 training acc: 0.97548 testing acc: 0.966\n",
      "epoch: 8 iter: 382 loss: 0.07999 training acc: 0.97582 testing acc: 0.9664\n",
      "epoch: 8 iter: 383 loss: 0.07442 training acc: 0.97868 testing acc: 0.9676\n",
      "epoch: 8 iter: 384 loss: 0.07353 training acc: 0.97848 testing acc: 0.9682\n",
      "epoch: 8 iter: 385 loss: 0.07684 training acc: 0.97714 testing acc: 0.9668\n",
      "epoch: 8 iter: 386 loss: 0.07578 training acc: 0.97814 testing acc: 0.9676\n",
      "epoch: 8 iter: 387 loss: 0.07934 training acc: 0.97704 testing acc: 0.9666\n",
      "epoch: 8 iter: 388 loss: 0.08040 training acc: 0.97662 testing acc: 0.9666\n",
      "epoch: 8 iter: 389 loss: 0.07713 training acc: 0.97802 testing acc: 0.969\n",
      "epoch: 8 iter: 390 loss: 0.07933 training acc: 0.97642 testing acc: 0.966\n",
      "epoch: 8 iter: 391 loss: 0.08043 training acc: 0.97626 testing acc: 0.966\n",
      "epoch: 8 iter: 392 loss: 0.08482 training acc: 0.97444 testing acc: 0.9618\n",
      "epoch: 8 iter: 393 loss: 0.07658 training acc: 0.97728 testing acc: 0.9662\n",
      "epoch: 8 iter: 394 loss: 0.07924 training acc: 0.97612 testing acc: 0.9656\n",
      "epoch: 8 iter: 395 loss: 0.08011 training acc: 0.97622 testing acc: 0.9656\n",
      "epoch: 8 iter: 396 loss: 0.07904 training acc: 0.97648 testing acc: 0.9664\n",
      "epoch: 8 iter: 397 loss: 0.08792 training acc: 0.97326 testing acc: 0.964\n",
      "epoch: 8 iter: 398 loss: 0.08560 training acc: 0.97404 testing acc: 0.9646\n",
      "epoch: 8 iter: 399 loss: 0.07719 training acc: 0.97766 testing acc: 0.9694\n",
      "epoch: 8 iter: 400 loss: 0.07650 training acc: 0.97800 testing acc: 0.97\n",
      "epoch: 8 iter: 401 loss: 0.07276 training acc: 0.97958 testing acc: 0.9714\n",
      "epoch: 8 iter: 402 loss: 0.07088 training acc: 0.97984 testing acc: 0.971\n",
      "epoch: 8 iter: 403 loss: 0.08189 training acc: 0.97608 testing acc: 0.965\n",
      "epoch: 8 iter: 404 loss: 0.08003 training acc: 0.97724 testing acc: 0.9652\n",
      "epoch: 8 iter: 405 loss: 0.08807 training acc: 0.97520 testing acc: 0.966\n",
      "epoch: 8 iter: 406 loss: 0.08031 training acc: 0.97780 testing acc: 0.9688\n",
      "epoch: 8 iter: 407 loss: 0.08271 training acc: 0.97674 testing acc: 0.9676\n",
      "epoch: 8 iter: 408 loss: 0.07479 training acc: 0.97874 testing acc: 0.9704\n",
      "epoch: 8 iter: 409 loss: 0.07275 training acc: 0.97922 testing acc: 0.9708\n",
      "epoch: 8 iter: 410 loss: 0.07859 training acc: 0.97688 testing acc: 0.9672\n",
      "epoch: 8 iter: 411 loss: 0.08414 training acc: 0.97488 testing acc: 0.9658\n",
      "epoch: 8 iter: 412 loss: 0.07979 training acc: 0.97644 testing acc: 0.9672\n",
      "epoch: 8 iter: 413 loss: 0.08097 training acc: 0.97598 testing acc: 0.966\n",
      "epoch: 8 iter: 414 loss: 0.08171 training acc: 0.97616 testing acc: 0.9636\n",
      "epoch: 8 iter: 415 loss: 0.08086 training acc: 0.97660 testing acc: 0.965\n",
      "epoch: 8 iter: 416 loss: 0.08015 training acc: 0.97694 testing acc: 0.9662\n",
      "epoch: 8 iter: 417 loss: 0.07700 training acc: 0.97798 testing acc: 0.9678\n",
      "epoch: 8 iter: 418 loss: 0.07568 training acc: 0.97834 testing acc: 0.9668\n",
      "epoch: 8 iter: 419 loss: 0.07600 training acc: 0.97812 testing acc: 0.9658\n",
      "epoch: 8 iter: 420 loss: 0.07864 training acc: 0.97770 testing acc: 0.966\n",
      "epoch: 8 iter: 421 loss: 0.07518 training acc: 0.97862 testing acc: 0.9684\n",
      "epoch: 8 iter: 422 loss: 0.07249 training acc: 0.97926 testing acc: 0.9682\n",
      "epoch: 8 iter: 423 loss: 0.07859 training acc: 0.97720 testing acc: 0.9678\n",
      "epoch: 8 iter: 424 loss: 0.07399 training acc: 0.97938 testing acc: 0.971\n",
      "epoch: 8 iter: 425 loss: 0.07315 training acc: 0.97954 testing acc: 0.9698\n",
      "epoch: 8 iter: 426 loss: 0.07275 training acc: 0.97952 testing acc: 0.97\n",
      "epoch: 8 iter: 427 loss: 0.07738 training acc: 0.97800 testing acc: 0.9672\n",
      "epoch: 8 iter: 428 loss: 0.07756 training acc: 0.97782 testing acc: 0.9684\n",
      "epoch: 8 iter: 429 loss: 0.07410 training acc: 0.97954 testing acc: 0.97\n",
      "epoch: 8 iter: 430 loss: 0.07186 training acc: 0.97990 testing acc: 0.971\n",
      "epoch: 8 iter: 431 loss: 0.07165 training acc: 0.98002 testing acc: 0.9712\n",
      "epoch: 8 iter: 432 loss: 0.07498 training acc: 0.97876 testing acc: 0.9706\n",
      "epoch: 8 iter: 433 loss: 0.07689 training acc: 0.97754 testing acc: 0.9692\n",
      "epoch: 8 iter: 434 loss: 0.07762 training acc: 0.97714 testing acc: 0.9686\n",
      "epoch: 8 iter: 435 loss: 0.07734 training acc: 0.97720 testing acc: 0.9688\n",
      "epoch: 8 iter: 436 loss: 0.07426 training acc: 0.97876 testing acc: 0.969\n",
      "epoch: 8 iter: 437 loss: 0.07455 training acc: 0.97834 testing acc: 0.9684\n",
      "epoch: 8 iter: 438 loss: 0.07535 training acc: 0.97828 testing acc: 0.9684\n",
      "epoch: 8 iter: 439 loss: 0.08966 training acc: 0.97338 testing acc: 0.9642\n",
      "epoch: 8 iter: 440 loss: 0.07707 training acc: 0.97798 testing acc: 0.9684\n",
      "epoch: 8 iter: 441 loss: 0.07648 training acc: 0.97824 testing acc: 0.969\n",
      "epoch: 8 iter: 442 loss: 0.07678 training acc: 0.97812 testing acc: 0.9682\n",
      "epoch: 8 iter: 443 loss: 0.07473 training acc: 0.97892 testing acc: 0.9688\n",
      "epoch: 8 iter: 444 loss: 0.07319 training acc: 0.97930 testing acc: 0.9692\n",
      "epoch: 8 iter: 445 loss: 0.07315 training acc: 0.97954 testing acc: 0.9702\n",
      "epoch: 8 iter: 446 loss: 0.07293 training acc: 0.97966 testing acc: 0.971\n",
      "epoch: 8 iter: 447 loss: 0.07468 training acc: 0.97886 testing acc: 0.9688\n",
      "epoch: 8 iter: 448 loss: 0.07580 training acc: 0.97776 testing acc: 0.968\n",
      "epoch: 8 iter: 449 loss: 0.07707 training acc: 0.97796 testing acc: 0.9682\n",
      "epoch: 8 iter: 450 loss: 0.07764 training acc: 0.97786 testing acc: 0.9688\n",
      "epoch: 8 iter: 451 loss: 0.07511 training acc: 0.97816 testing acc: 0.9688\n",
      "epoch: 8 iter: 452 loss: 0.08164 training acc: 0.97576 testing acc: 0.9674\n",
      "epoch: 8 iter: 453 loss: 0.07920 training acc: 0.97740 testing acc: 0.9674\n",
      "epoch: 8 iter: 454 loss: 0.13058 training acc: 0.95808 testing acc: 0.949\n",
      "epoch: 8 iter: 455 loss: 0.07681 training acc: 0.97746 testing acc: 0.9666\n",
      "epoch: 8 iter: 456 loss: 0.07155 training acc: 0.97966 testing acc: 0.9686\n",
      "epoch: 8 iter: 457 loss: 0.08201 training acc: 0.97636 testing acc: 0.9646\n",
      "epoch: 8 iter: 458 loss: 0.07953 training acc: 0.97738 testing acc: 0.9658\n",
      "epoch: 8 iter: 459 loss: 0.07815 training acc: 0.97748 testing acc: 0.9674\n",
      "epoch: 8 iter: 460 loss: 0.07713 training acc: 0.97778 testing acc: 0.9676\n",
      "epoch: 8 iter: 461 loss: 0.07558 training acc: 0.97826 testing acc: 0.9684\n",
      "epoch: 8 iter: 462 loss: 0.07651 training acc: 0.97804 testing acc: 0.9676\n",
      "epoch: 8 iter: 463 loss: 0.07337 training acc: 0.97916 testing acc: 0.969\n",
      "epoch: 8 iter: 464 loss: 0.07564 training acc: 0.97784 testing acc: 0.969\n",
      "epoch: 8 iter: 465 loss: 0.07472 training acc: 0.97820 testing acc: 0.97\n",
      "epoch: 8 iter: 466 loss: 0.07339 training acc: 0.97908 testing acc: 0.9708\n",
      "epoch: 8 iter: 467 loss: 0.07544 training acc: 0.97848 testing acc: 0.9704\n",
      "epoch: 8 iter: 468 loss: 0.08127 training acc: 0.97644 testing acc: 0.969\n",
      "epoch: 8 iter: 469 loss: 0.07644 training acc: 0.97786 testing acc: 0.9678\n",
      "epoch: 8 iter: 470 loss: 0.07670 training acc: 0.97766 testing acc: 0.9674\n",
      "epoch: 8 iter: 471 loss: 0.07564 training acc: 0.97808 testing acc: 0.968\n",
      "epoch: 8 iter: 472 loss: 0.07241 training acc: 0.97920 testing acc: 0.9674\n",
      "epoch: 8 iter: 473 loss: 0.07954 training acc: 0.97664 testing acc: 0.9612\n",
      "epoch: 8 iter: 474 loss: 0.07912 training acc: 0.97664 testing acc: 0.9624\n",
      "epoch: 8 iter: 475 loss: 0.08240 training acc: 0.97582 testing acc: 0.9646\n",
      "epoch: 8 iter: 476 loss: 0.07330 training acc: 0.97930 testing acc: 0.969\n",
      "epoch: 8 iter: 477 loss: 0.07508 training acc: 0.97858 testing acc: 0.967\n",
      "epoch: 8 iter: 478 loss: 0.09704 training acc: 0.96974 testing acc: 0.9574\n",
      "epoch: 8 iter: 479 loss: 0.08221 training acc: 0.97490 testing acc: 0.9636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 iter: 480 loss: 0.07516 training acc: 0.97760 testing acc: 0.9668\n",
      "epoch: 8 iter: 481 loss: 0.07509 training acc: 0.97740 testing acc: 0.9668\n",
      "epoch: 8 iter: 482 loss: 0.07205 training acc: 0.97950 testing acc: 0.9694\n",
      "epoch: 8 iter: 483 loss: 0.07234 training acc: 0.97934 testing acc: 0.9678\n",
      "epoch: 8 iter: 484 loss: 0.07348 training acc: 0.97874 testing acc: 0.967\n",
      "epoch: 8 iter: 485 loss: 0.06977 training acc: 0.98024 testing acc: 0.9688\n",
      "epoch: 8 iter: 486 loss: 0.07699 training acc: 0.97808 testing acc: 0.9678\n",
      "epoch: 8 iter: 487 loss: 0.07516 training acc: 0.97888 testing acc: 0.9666\n",
      "epoch: 8 iter: 488 loss: 0.07283 training acc: 0.97952 testing acc: 0.9678\n",
      "epoch: 8 iter: 489 loss: 0.07204 training acc: 0.97988 testing acc: 0.9682\n",
      "epoch: 8 iter: 490 loss: 0.07397 training acc: 0.97898 testing acc: 0.9686\n",
      "epoch: 8 iter: 491 loss: 0.07462 training acc: 0.97794 testing acc: 0.9682\n",
      "epoch: 8 iter: 492 loss: 0.07774 training acc: 0.97704 testing acc: 0.9662\n",
      "epoch: 8 iter: 493 loss: 0.08195 training acc: 0.97560 testing acc: 0.964\n",
      "epoch: 8 iter: 494 loss: 0.07861 training acc: 0.97702 testing acc: 0.9646\n",
      "epoch: 8 iter: 495 loss: 0.07545 training acc: 0.97878 testing acc: 0.969\n",
      "epoch: 8 iter: 496 loss: 0.07887 training acc: 0.97718 testing acc: 0.9682\n",
      "epoch: 8 iter: 497 loss: 0.08303 training acc: 0.97512 testing acc: 0.965\n",
      "epoch: 8 iter: 498 loss: 0.08056 training acc: 0.97620 testing acc: 0.9656\n",
      "epoch: 8 iter: 499 loss: 0.07853 training acc: 0.97696 testing acc: 0.9664\n",
      "epoch: 8 iter: 500 loss: 0.08058 training acc: 0.97668 testing acc: 0.9666\n",
      "epoch: 8 iter: 501 loss: 0.07700 training acc: 0.97794 testing acc: 0.9686\n",
      "epoch: 8 iter: 502 loss: 0.12680 training acc: 0.96024 testing acc: 0.95\n",
      "epoch: 8 iter: 503 loss: 0.08866 training acc: 0.97378 testing acc: 0.9636\n",
      "epoch: 8 iter: 504 loss: 0.08454 training acc: 0.97570 testing acc: 0.964\n",
      "epoch: 8 iter: 505 loss: 0.08397 training acc: 0.97564 testing acc: 0.9638\n",
      "epoch: 8 iter: 506 loss: 0.08108 training acc: 0.97674 testing acc: 0.964\n",
      "epoch: 8 iter: 507 loss: 0.09562 training acc: 0.97042 testing acc: 0.96\n",
      "epoch: 8 iter: 508 loss: 0.09580 training acc: 0.97092 testing acc: 0.9582\n",
      "epoch: 8 iter: 509 loss: 0.09988 training acc: 0.96926 testing acc: 0.9574\n",
      "epoch: 8 iter: 510 loss: 0.09601 training acc: 0.97074 testing acc: 0.959\n",
      "epoch: 8 iter: 511 loss: 0.08301 training acc: 0.97558 testing acc: 0.9678\n",
      "epoch: 8 iter: 512 loss: 0.07846 training acc: 0.97756 testing acc: 0.967\n",
      "epoch: 8 iter: 513 loss: 0.07845 training acc: 0.97820 testing acc: 0.9678\n",
      "epoch: 8 iter: 514 loss: 0.07604 training acc: 0.97878 testing acc: 0.9676\n",
      "epoch: 8 iter: 515 loss: 0.07491 training acc: 0.97950 testing acc: 0.9692\n",
      "epoch: 8 iter: 516 loss: 0.07444 training acc: 0.97946 testing acc: 0.9684\n",
      "epoch: 8 iter: 517 loss: 0.07405 training acc: 0.97938 testing acc: 0.969\n",
      "epoch: 8 iter: 518 loss: 0.07932 training acc: 0.97680 testing acc: 0.9658\n",
      "epoch: 8 iter: 519 loss: 0.07756 training acc: 0.97826 testing acc: 0.9676\n",
      "epoch: 8 iter: 520 loss: 0.07320 training acc: 0.97926 testing acc: 0.9682\n",
      "epoch: 8 iter: 521 loss: 0.07637 training acc: 0.97772 testing acc: 0.967\n",
      "epoch: 8 iter: 522 loss: 0.07622 training acc: 0.97834 testing acc: 0.9674\n",
      "epoch: 8 iter: 523 loss: 0.07526 training acc: 0.97842 testing acc: 0.9674\n",
      "epoch: 8 iter: 524 loss: 0.08466 training acc: 0.97440 testing acc: 0.9624\n",
      "epoch: 8 iter: 525 loss: 0.08210 training acc: 0.97556 testing acc: 0.9634\n",
      "epoch: 8 iter: 526 loss: 0.08079 training acc: 0.97684 testing acc: 0.9674\n",
      "epoch: 8 iter: 527 loss: 0.07998 training acc: 0.97678 testing acc: 0.9668\n",
      "epoch: 8 iter: 528 loss: 0.07857 training acc: 0.97762 testing acc: 0.9672\n",
      "epoch: 8 iter: 529 loss: 0.08090 training acc: 0.97678 testing acc: 0.9652\n",
      "epoch: 8 iter: 530 loss: 0.08377 training acc: 0.97562 testing acc: 0.9642\n",
      "epoch: 8 iter: 531 loss: 0.08082 training acc: 0.97684 testing acc: 0.9636\n",
      "epoch: 8 iter: 532 loss: 0.08328 training acc: 0.97578 testing acc: 0.9636\n",
      "epoch: 8 iter: 533 loss: 0.07867 training acc: 0.97694 testing acc: 0.9654\n",
      "epoch: 8 iter: 534 loss: 0.08724 training acc: 0.97388 testing acc: 0.964\n",
      "epoch: 8 iter: 535 loss: 0.08244 training acc: 0.97600 testing acc: 0.9654\n",
      "epoch: 8 iter: 536 loss: 0.08857 training acc: 0.97354 testing acc: 0.9652\n",
      "epoch: 8 iter: 537 loss: 0.08056 training acc: 0.97706 testing acc: 0.968\n",
      "epoch: 8 iter: 538 loss: 0.09010 training acc: 0.97346 testing acc: 0.9664\n",
      "epoch: 8 iter: 539 loss: 0.08405 training acc: 0.97540 testing acc: 0.969\n",
      "epoch: 8 iter: 540 loss: 0.08316 training acc: 0.97588 testing acc: 0.9698\n",
      "epoch: 8 iter: 541 loss: 0.08288 training acc: 0.97586 testing acc: 0.9694\n",
      "epoch: 8 iter: 542 loss: 0.07478 training acc: 0.97858 testing acc: 0.9686\n",
      "epoch: 8 iter: 543 loss: 0.08112 training acc: 0.97626 testing acc: 0.9662\n",
      "epoch: 8 iter: 544 loss: 0.09721 training acc: 0.97080 testing acc: 0.9614\n",
      "epoch: 8 iter: 545 loss: 0.07863 training acc: 0.97776 testing acc: 0.9656\n",
      "epoch: 8 iter: 546 loss: 0.07930 training acc: 0.97650 testing acc: 0.9686\n",
      "epoch: 8 iter: 547 loss: 0.08260 training acc: 0.97554 testing acc: 0.966\n",
      "epoch: 8 iter: 548 loss: 0.07844 training acc: 0.97676 testing acc: 0.9676\n",
      "epoch: 8 iter: 549 loss: 0.07848 training acc: 0.97736 testing acc: 0.9672\n",
      "epoch: 8 iter: 550 loss: 0.07924 training acc: 0.97750 testing acc: 0.966\n",
      "epoch: 8 iter: 551 loss: 0.09003 training acc: 0.97292 testing acc: 0.9614\n",
      "epoch: 8 iter: 552 loss: 0.08600 training acc: 0.97488 testing acc: 0.9614\n",
      "epoch: 8 iter: 553 loss: 0.08479 training acc: 0.97514 testing acc: 0.963\n",
      "epoch: 8 iter: 554 loss: 0.09107 training acc: 0.97294 testing acc: 0.9614\n",
      "epoch: 8 iter: 555 loss: 0.08306 training acc: 0.97556 testing acc: 0.9654\n",
      "epoch: 8 iter: 556 loss: 0.08311 training acc: 0.97556 testing acc: 0.9616\n",
      "epoch: 8 iter: 557 loss: 0.07793 training acc: 0.97770 testing acc: 0.9646\n",
      "epoch: 8 iter: 558 loss: 0.08696 training acc: 0.97460 testing acc: 0.962\n",
      "epoch: 8 iter: 559 loss: 0.07482 training acc: 0.97846 testing acc: 0.9662\n",
      "epoch: 8 iter: 560 loss: 0.08075 training acc: 0.97674 testing acc: 0.9664\n",
      "epoch: 8 iter: 561 loss: 0.07329 training acc: 0.97928 testing acc: 0.9686\n",
      "epoch: 8 iter: 562 loss: 0.07277 training acc: 0.97966 testing acc: 0.9682\n",
      "epoch: 8 iter: 563 loss: 0.07196 training acc: 0.97950 testing acc: 0.9692\n",
      "epoch: 8 iter: 564 loss: 0.07594 training acc: 0.97818 testing acc: 0.9684\n",
      "epoch: 8 iter: 565 loss: 0.07635 training acc: 0.97834 testing acc: 0.9678\n",
      "epoch: 8 iter: 566 loss: 0.10465 training acc: 0.96802 testing acc: 0.9594\n",
      "epoch: 8 iter: 567 loss: 0.08147 training acc: 0.97672 testing acc: 0.9658\n",
      "epoch: 8 iter: 568 loss: 0.07493 training acc: 0.97902 testing acc: 0.9684\n",
      "epoch: 8 iter: 569 loss: 0.07859 training acc: 0.97774 testing acc: 0.968\n",
      "epoch: 8 iter: 570 loss: 0.08106 training acc: 0.97700 testing acc: 0.9678\n",
      "epoch: 8 iter: 571 loss: 0.08787 training acc: 0.97444 testing acc: 0.9638\n",
      "epoch: 8 iter: 572 loss: 0.08175 training acc: 0.97628 testing acc: 0.966\n",
      "epoch: 8 iter: 573 loss: 0.07955 training acc: 0.97740 testing acc: 0.9658\n",
      "epoch: 8 iter: 574 loss: 0.08731 training acc: 0.97448 testing acc: 0.9624\n",
      "epoch: 8 iter: 575 loss: 0.07877 training acc: 0.97750 testing acc: 0.9656\n",
      "epoch: 8 iter: 576 loss: 0.08117 training acc: 0.97672 testing acc: 0.9652\n",
      "epoch: 8 iter: 577 loss: 0.08216 training acc: 0.97554 testing acc: 0.9644\n",
      "epoch: 8 iter: 578 loss: 0.08189 training acc: 0.97626 testing acc: 0.9644\n",
      "epoch: 8 iter: 579 loss: 0.09090 training acc: 0.97182 testing acc: 0.9616\n",
      "epoch: 8 iter: 580 loss: 0.08562 training acc: 0.97414 testing acc: 0.9658\n",
      "epoch: 8 iter: 581 loss: 0.10287 training acc: 0.96842 testing acc: 0.9612\n",
      "epoch: 8 iter: 582 loss: 0.08866 training acc: 0.97306 testing acc: 0.9638\n",
      "epoch: 8 iter: 583 loss: 0.08412 training acc: 0.97442 testing acc: 0.9652\n",
      "epoch: 8 iter: 584 loss: 0.08612 training acc: 0.97396 testing acc: 0.9636\n",
      "epoch: 8 iter: 585 loss: 0.08339 training acc: 0.97498 testing acc: 0.9634\n",
      "epoch: 8 iter: 586 loss: 0.08309 training acc: 0.97546 testing acc: 0.9634\n",
      "epoch: 8 iter: 587 loss: 0.08093 training acc: 0.97612 testing acc: 0.9646\n",
      "epoch: 8 iter: 588 loss: 0.08132 training acc: 0.97608 testing acc: 0.9644\n",
      "epoch: 8 iter: 589 loss: 0.09213 training acc: 0.97272 testing acc: 0.962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 iter: 590 loss: 0.08461 training acc: 0.97530 testing acc: 0.9634\n",
      "epoch: 8 iter: 591 loss: 0.08470 training acc: 0.97472 testing acc: 0.9628\n",
      "epoch: 8 iter: 592 loss: 0.07799 training acc: 0.97734 testing acc: 0.9658\n",
      "epoch: 8 iter: 593 loss: 0.07458 training acc: 0.97864 testing acc: 0.9656\n",
      "epoch: 8 iter: 594 loss: 0.07338 training acc: 0.97954 testing acc: 0.9708\n",
      "epoch: 8 iter: 595 loss: 0.07376 training acc: 0.97936 testing acc: 0.9712\n",
      "epoch: 8 iter: 596 loss: 0.07298 training acc: 0.97938 testing acc: 0.971\n",
      "epoch: 8 iter: 597 loss: 0.07155 training acc: 0.97972 testing acc: 0.9698\n",
      "epoch: 8 iter: 598 loss: 0.07266 training acc: 0.97904 testing acc: 0.9696\n",
      "epoch: 8 iter: 599 loss: 0.07218 training acc: 0.97942 testing acc: 0.9696\n",
      "epoch: 8 iter: 600 loss: 0.07435 training acc: 0.97818 testing acc: 0.9674\n",
      "epoch: 8 iter: 601 loss: 0.07998 training acc: 0.97604 testing acc: 0.9658\n",
      "epoch: 8 iter: 602 loss: 0.08460 training acc: 0.97484 testing acc: 0.9646\n",
      "epoch: 8 iter: 603 loss: 0.08641 training acc: 0.97394 testing acc: 0.9634\n",
      "epoch: 8 iter: 604 loss: 0.09120 training acc: 0.97272 testing acc: 0.9614\n",
      "epoch: 8 iter: 605 loss: 0.07737 training acc: 0.97816 testing acc: 0.967\n",
      "epoch: 8 iter: 606 loss: 0.07647 training acc: 0.97872 testing acc: 0.9682\n",
      "epoch: 8 iter: 607 loss: 0.07640 training acc: 0.97838 testing acc: 0.968\n",
      "epoch: 8 iter: 608 loss: 0.07643 training acc: 0.97788 testing acc: 0.9666\n",
      "epoch: 8 iter: 609 loss: 0.07563 training acc: 0.97828 testing acc: 0.967\n",
      "epoch: 8 iter: 610 loss: 0.09160 training acc: 0.97240 testing acc: 0.9606\n",
      "epoch: 8 iter: 611 loss: 0.08414 training acc: 0.97456 testing acc: 0.9632\n",
      "epoch: 8 iter: 612 loss: 0.08972 training acc: 0.97322 testing acc: 0.963\n",
      "epoch: 8 iter: 613 loss: 0.08695 training acc: 0.97440 testing acc: 0.964\n",
      "epoch: 8 iter: 614 loss: 0.08768 training acc: 0.97388 testing acc: 0.9642\n",
      "epoch: 8 iter: 615 loss: 0.07676 training acc: 0.97824 testing acc: 0.968\n",
      "epoch: 8 iter: 616 loss: 0.08682 training acc: 0.97472 testing acc: 0.963\n",
      "epoch: 8 iter: 617 loss: 0.08054 training acc: 0.97652 testing acc: 0.9634\n",
      "epoch: 8 iter: 618 loss: 0.07884 training acc: 0.97732 testing acc: 0.964\n",
      "epoch: 8 iter: 619 loss: 0.08288 training acc: 0.97524 testing acc: 0.9626\n",
      "epoch: 8 iter: 620 loss: 0.08144 training acc: 0.97646 testing acc: 0.964\n",
      "epoch: 8 iter: 621 loss: 0.07611 training acc: 0.97866 testing acc: 0.9686\n",
      "epoch: 8 iter: 622 loss: 0.07866 training acc: 0.97700 testing acc: 0.9664\n",
      "epoch: 8 iter: 623 loss: 0.07965 training acc: 0.97652 testing acc: 0.9662\n",
      "epoch: 8 iter: 624 loss: 0.07261 training acc: 0.97984 testing acc: 0.9688\n",
      "epoch: 8 iter: 625 loss: 0.07306 training acc: 0.97926 testing acc: 0.9682\n",
      "epoch: 8 iter: 626 loss: 0.07308 training acc: 0.97954 testing acc: 0.9674\n",
      "epoch: 8 iter: 627 loss: 0.07695 training acc: 0.97806 testing acc: 0.9684\n",
      "epoch: 8 iter: 628 loss: 0.07620 training acc: 0.97814 testing acc: 0.9684\n",
      "epoch: 8 iter: 629 loss: 0.07447 training acc: 0.97890 testing acc: 0.969\n",
      "epoch: 8 iter: 630 loss: 0.07344 training acc: 0.97900 testing acc: 0.97\n",
      "epoch: 8 iter: 631 loss: 0.07471 training acc: 0.97880 testing acc: 0.9688\n",
      "epoch: 8 iter: 632 loss: 0.07291 training acc: 0.97968 testing acc: 0.9698\n",
      "epoch: 8 iter: 633 loss: 0.09097 training acc: 0.97336 testing acc: 0.9638\n",
      "epoch: 8 iter: 634 loss: 0.08262 training acc: 0.97592 testing acc: 0.9656\n",
      "epoch: 8 iter: 635 loss: 0.07804 training acc: 0.97756 testing acc: 0.9674\n",
      "epoch: 8 iter: 636 loss: 0.08562 training acc: 0.97476 testing acc: 0.961\n",
      "epoch: 8 iter: 637 loss: 0.09336 training acc: 0.97130 testing acc: 0.957\n",
      "epoch: 8 iter: 638 loss: 0.07389 training acc: 0.97886 testing acc: 0.9656\n",
      "epoch: 8 iter: 639 loss: 0.07293 training acc: 0.97946 testing acc: 0.9654\n",
      "epoch: 8 iter: 640 loss: 0.06974 training acc: 0.98064 testing acc: 0.9686\n",
      "epoch: 8 iter: 641 loss: 0.07252 training acc: 0.97984 testing acc: 0.966\n",
      "epoch: 8 iter: 642 loss: 0.07306 training acc: 0.97968 testing acc: 0.9656\n",
      "epoch: 8 iter: 643 loss: 0.07448 training acc: 0.97928 testing acc: 0.9672\n",
      "epoch: 8 iter: 644 loss: 0.07262 training acc: 0.97994 testing acc: 0.967\n",
      "epoch: 8 iter: 645 loss: 0.07670 training acc: 0.97848 testing acc: 0.9672\n",
      "epoch: 8 iter: 646 loss: 0.07566 training acc: 0.97858 testing acc: 0.9668\n",
      "epoch: 8 iter: 647 loss: 0.07487 training acc: 0.97880 testing acc: 0.9672\n",
      "epoch: 8 iter: 648 loss: 0.07505 training acc: 0.97910 testing acc: 0.967\n",
      "epoch: 8 iter: 649 loss: 0.07346 training acc: 0.97964 testing acc: 0.9676\n",
      "epoch: 8 iter: 650 loss: 0.07324 training acc: 0.97978 testing acc: 0.9674\n",
      "epoch: 8 iter: 651 loss: 0.07269 training acc: 0.97972 testing acc: 0.9682\n",
      "epoch: 8 iter: 652 loss: 0.07243 training acc: 0.97964 testing acc: 0.9678\n",
      "epoch: 8 iter: 653 loss: 0.07531 training acc: 0.97870 testing acc: 0.967\n",
      "epoch: 8 iter: 654 loss: 0.07894 training acc: 0.97742 testing acc: 0.9644\n",
      "epoch: 8 iter: 655 loss: 0.07719 training acc: 0.97800 testing acc: 0.9656\n",
      "epoch: 8 iter: 656 loss: 0.07128 training acc: 0.97984 testing acc: 0.9666\n",
      "epoch: 8 iter: 657 loss: 0.07548 training acc: 0.97802 testing acc: 0.9656\n",
      "epoch: 8 iter: 658 loss: 0.07214 training acc: 0.97946 testing acc: 0.9664\n",
      "epoch: 8 iter: 659 loss: 0.08445 training acc: 0.97462 testing acc: 0.9618\n",
      "epoch: 8 iter: 660 loss: 0.07814 training acc: 0.97698 testing acc: 0.9648\n",
      "epoch: 8 iter: 661 loss: 0.07224 training acc: 0.97946 testing acc: 0.9662\n",
      "epoch: 8 iter: 662 loss: 0.07272 training acc: 0.97902 testing acc: 0.9664\n",
      "epoch: 8 iter: 663 loss: 0.08371 training acc: 0.97520 testing acc: 0.9632\n",
      "epoch: 8 iter: 664 loss: 0.08342 training acc: 0.97538 testing acc: 0.9644\n",
      "epoch: 8 iter: 665 loss: 0.08285 training acc: 0.97564 testing acc: 0.9652\n",
      "epoch: 8 iter: 666 loss: 0.07788 training acc: 0.97730 testing acc: 0.9642\n",
      "epoch: 8 iter: 667 loss: 0.07044 training acc: 0.97992 testing acc: 0.9682\n",
      "epoch: 8 iter: 668 loss: 0.07011 training acc: 0.98022 testing acc: 0.9678\n",
      "epoch: 8 iter: 669 loss: 0.06956 training acc: 0.98022 testing acc: 0.9676\n",
      "epoch: 8 iter: 670 loss: 0.06840 training acc: 0.98088 testing acc: 0.9674\n",
      "epoch: 8 iter: 671 loss: 0.08063 training acc: 0.97608 testing acc: 0.963\n",
      "epoch: 8 iter: 672 loss: 0.07634 training acc: 0.97812 testing acc: 0.9646\n",
      "epoch: 8 iter: 673 loss: 0.07055 training acc: 0.98046 testing acc: 0.967\n",
      "epoch: 8 iter: 674 loss: 0.07165 training acc: 0.98014 testing acc: 0.9668\n",
      "epoch: 8 iter: 675 loss: 0.07195 training acc: 0.97986 testing acc: 0.9658\n",
      "epoch: 8 iter: 676 loss: 0.07728 training acc: 0.97786 testing acc: 0.9654\n",
      "epoch: 8 iter: 677 loss: 0.07884 training acc: 0.97672 testing acc: 0.9638\n",
      "epoch: 8 iter: 678 loss: 0.07935 training acc: 0.97640 testing acc: 0.9646\n",
      "epoch: 8 iter: 679 loss: 0.08007 training acc: 0.97562 testing acc: 0.9654\n",
      "epoch: 8 iter: 680 loss: 0.07314 training acc: 0.97890 testing acc: 0.967\n",
      "epoch: 8 iter: 681 loss: 0.07283 training acc: 0.97906 testing acc: 0.9678\n",
      "epoch: 8 iter: 682 loss: 0.07104 training acc: 0.97974 testing acc: 0.9694\n",
      "epoch: 8 iter: 683 loss: 0.07245 training acc: 0.97948 testing acc: 0.9688\n",
      "epoch: 8 iter: 684 loss: 0.07026 training acc: 0.98016 testing acc: 0.9682\n",
      "epoch: 8 iter: 685 loss: 0.07051 training acc: 0.97994 testing acc: 0.9682\n",
      "epoch: 8 iter: 686 loss: 0.06946 training acc: 0.98062 testing acc: 0.9686\n",
      "epoch: 8 iter: 687 loss: 0.07217 training acc: 0.97942 testing acc: 0.9678\n",
      "epoch: 8 iter: 688 loss: 0.08156 training acc: 0.97612 testing acc: 0.9654\n",
      "epoch: 8 iter: 689 loss: 0.07813 training acc: 0.97720 testing acc: 0.965\n",
      "epoch: 8 iter: 690 loss: 0.07431 training acc: 0.97888 testing acc: 0.9656\n",
      "epoch: 8 iter: 691 loss: 0.07175 training acc: 0.97934 testing acc: 0.967\n",
      "epoch: 8 iter: 692 loss: 0.07104 training acc: 0.97994 testing acc: 0.9672\n",
      "epoch: 8 iter: 693 loss: 0.07577 training acc: 0.97814 testing acc: 0.966\n",
      "epoch: 8 iter: 694 loss: 0.07529 training acc: 0.97796 testing acc: 0.966\n",
      "epoch: 8 iter: 695 loss: 0.08485 training acc: 0.97508 testing acc: 0.9636\n",
      "epoch: 8 iter: 696 loss: 0.07547 training acc: 0.97872 testing acc: 0.9664\n",
      "epoch: 8 iter: 697 loss: 0.07698 training acc: 0.97772 testing acc: 0.9662\n",
      "epoch: 8 iter: 698 loss: 0.07647 training acc: 0.97764 testing acc: 0.965\n",
      "epoch: 8 iter: 699 loss: 0.08547 training acc: 0.97474 testing acc: 0.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 iter: 700 loss: 0.08357 training acc: 0.97506 testing acc: 0.9632\n",
      "epoch: 8 iter: 701 loss: 0.08999 training acc: 0.97338 testing acc: 0.9616\n",
      "epoch: 8 iter: 702 loss: 0.07408 training acc: 0.97810 testing acc: 0.9686\n",
      "epoch: 8 iter: 703 loss: 0.07239 training acc: 0.97968 testing acc: 0.9692\n",
      "epoch: 8 iter: 704 loss: 0.07613 training acc: 0.97854 testing acc: 0.9668\n",
      "epoch: 8 iter: 705 loss: 0.08625 training acc: 0.97484 testing acc: 0.963\n",
      "epoch: 8 iter: 706 loss: 0.07661 training acc: 0.97854 testing acc: 0.9664\n",
      "epoch: 8 iter: 707 loss: 0.07813 training acc: 0.97790 testing acc: 0.967\n",
      "epoch: 8 iter: 708 loss: 0.08054 training acc: 0.97702 testing acc: 0.9656\n",
      "epoch: 8 iter: 709 loss: 0.07739 training acc: 0.97824 testing acc: 0.9678\n",
      "epoch: 8 iter: 710 loss: 0.07530 training acc: 0.97846 testing acc: 0.967\n",
      "epoch: 8 iter: 711 loss: 0.07059 training acc: 0.98034 testing acc: 0.9678\n",
      "epoch: 8 iter: 712 loss: 0.07303 training acc: 0.97936 testing acc: 0.9652\n",
      "epoch: 8 iter: 713 loss: 0.07013 training acc: 0.98040 testing acc: 0.9684\n",
      "epoch: 8 iter: 714 loss: 0.07039 training acc: 0.98016 testing acc: 0.9696\n",
      "epoch: 8 iter: 715 loss: 0.07169 training acc: 0.97988 testing acc: 0.969\n",
      "epoch: 8 iter: 716 loss: 0.07101 training acc: 0.98032 testing acc: 0.9696\n",
      "epoch: 8 iter: 717 loss: 0.07269 training acc: 0.97976 testing acc: 0.9666\n",
      "epoch: 8 iter: 718 loss: 0.07446 training acc: 0.97912 testing acc: 0.9662\n",
      "epoch: 8 iter: 719 loss: 0.07515 training acc: 0.97884 testing acc: 0.966\n",
      "epoch: 8 iter: 720 loss: 0.07253 training acc: 0.97990 testing acc: 0.9662\n",
      "epoch: 8 iter: 721 loss: 0.07495 training acc: 0.97930 testing acc: 0.9682\n",
      "epoch: 8 iter: 722 loss: 0.07568 training acc: 0.97894 testing acc: 0.9688\n",
      "epoch: 8 iter: 723 loss: 0.07429 training acc: 0.97908 testing acc: 0.9686\n",
      "epoch: 8 iter: 724 loss: 0.08794 training acc: 0.97416 testing acc: 0.9628\n",
      "epoch: 8 iter: 725 loss: 0.08350 training acc: 0.97594 testing acc: 0.9662\n",
      "epoch: 8 iter: 726 loss: 0.08152 training acc: 0.97586 testing acc: 0.9648\n",
      "epoch: 8 iter: 727 loss: 0.07181 training acc: 0.97952 testing acc: 0.9688\n",
      "epoch: 8 iter: 728 loss: 0.07157 training acc: 0.97970 testing acc: 0.9692\n",
      "epoch: 8 iter: 729 loss: 0.07508 training acc: 0.97856 testing acc: 0.9678\n",
      "epoch: 8 iter: 730 loss: 0.07518 training acc: 0.97842 testing acc: 0.9694\n",
      "epoch: 8 iter: 731 loss: 0.07327 training acc: 0.97844 testing acc: 0.9684\n",
      "epoch: 8 iter: 732 loss: 0.08300 training acc: 0.97496 testing acc: 0.9644\n",
      "epoch: 8 iter: 733 loss: 0.08087 training acc: 0.97640 testing acc: 0.9676\n",
      "epoch: 8 iter: 734 loss: 0.07004 training acc: 0.98048 testing acc: 0.9684\n",
      "epoch: 8 iter: 735 loss: 0.07206 training acc: 0.97952 testing acc: 0.9682\n",
      "epoch: 8 iter: 736 loss: 0.07724 training acc: 0.97762 testing acc: 0.965\n",
      "epoch: 8 iter: 737 loss: 0.07445 training acc: 0.97860 testing acc: 0.9658\n",
      "epoch: 8 iter: 738 loss: 0.07035 training acc: 0.98022 testing acc: 0.9694\n",
      "epoch: 8 iter: 739 loss: 0.06799 training acc: 0.98118 testing acc: 0.9692\n",
      "epoch: 8 iter: 740 loss: 0.06783 training acc: 0.98086 testing acc: 0.969\n",
      "epoch: 8 iter: 741 loss: 0.06861 training acc: 0.98040 testing acc: 0.9668\n",
      "epoch: 8 iter: 742 loss: 0.07627 training acc: 0.97720 testing acc: 0.9632\n",
      "epoch: 8 iter: 743 loss: 0.06888 training acc: 0.98038 testing acc: 0.9662\n",
      "epoch: 8 iter: 744 loss: 0.07514 training acc: 0.97764 testing acc: 0.9656\n",
      "epoch: 8 iter: 745 loss: 0.07101 training acc: 0.97950 testing acc: 0.9666\n",
      "epoch: 8 iter: 746 loss: 0.07700 training acc: 0.97704 testing acc: 0.965\n",
      "epoch: 8 iter: 747 loss: 0.07912 training acc: 0.97604 testing acc: 0.9634\n",
      "epoch: 8 iter: 748 loss: 0.09545 training acc: 0.97018 testing acc: 0.9582\n",
      "epoch: 8 iter: 749 loss: 0.07585 training acc: 0.97814 testing acc: 0.9652\n",
      "epoch: 8 iter: 750 loss: 0.08406 training acc: 0.97534 testing acc: 0.9642\n",
      "epoch: 8 iter: 751 loss: 0.08111 training acc: 0.97590 testing acc: 0.9654\n",
      "epoch: 8 iter: 752 loss: 0.07862 training acc: 0.97684 testing acc: 0.9656\n",
      "epoch: 8 iter: 753 loss: 0.09544 training acc: 0.97104 testing acc: 0.9596\n",
      "epoch: 8 iter: 754 loss: 0.07949 training acc: 0.97696 testing acc: 0.9648\n",
      "epoch: 8 iter: 755 loss: 0.11997 training acc: 0.96060 testing acc: 0.9506\n",
      "epoch: 8 iter: 756 loss: 0.07817 training acc: 0.97668 testing acc: 0.9648\n",
      "epoch: 8 iter: 757 loss: 0.07397 training acc: 0.97794 testing acc: 0.9658\n",
      "epoch: 8 iter: 758 loss: 0.07687 training acc: 0.97728 testing acc: 0.9656\n",
      "epoch: 8 iter: 759 loss: 0.07429 training acc: 0.97814 testing acc: 0.9676\n",
      "epoch: 8 iter: 760 loss: 0.07378 training acc: 0.97826 testing acc: 0.9664\n",
      "epoch: 8 iter: 761 loss: 0.07025 training acc: 0.97992 testing acc: 0.9678\n",
      "epoch: 8 iter: 762 loss: 0.08031 training acc: 0.97622 testing acc: 0.9656\n",
      "epoch: 8 iter: 763 loss: 0.07827 training acc: 0.97720 testing acc: 0.9668\n",
      "epoch: 8 iter: 764 loss: 0.08715 training acc: 0.97340 testing acc: 0.9638\n",
      "epoch: 8 iter: 765 loss: 0.08534 training acc: 0.97386 testing acc: 0.9642\n",
      "epoch: 8 iter: 766 loss: 0.07608 training acc: 0.97792 testing acc: 0.9656\n",
      "epoch: 8 iter: 767 loss: 0.07586 training acc: 0.97782 testing acc: 0.9674\n",
      "epoch: 8 iter: 768 loss: 0.07447 training acc: 0.97904 testing acc: 0.9674\n",
      "epoch: 8 iter: 769 loss: 0.07384 training acc: 0.97910 testing acc: 0.9674\n",
      "epoch: 8 iter: 770 loss: 0.09069 training acc: 0.97332 testing acc: 0.9624\n",
      "epoch: 8 iter: 771 loss: 0.09220 training acc: 0.97280 testing acc: 0.9636\n",
      "epoch: 8 iter: 772 loss: 0.07967 training acc: 0.97696 testing acc: 0.9682\n",
      "epoch: 8 iter: 773 loss: 0.07847 training acc: 0.97674 testing acc: 0.9674\n",
      "epoch: 8 iter: 774 loss: 0.07986 training acc: 0.97622 testing acc: 0.963\n",
      "epoch: 8 iter: 775 loss: 0.08055 training acc: 0.97566 testing acc: 0.9646\n",
      "epoch: 8 iter: 776 loss: 0.08141 training acc: 0.97540 testing acc: 0.9628\n",
      "epoch: 8 iter: 777 loss: 0.07715 training acc: 0.97718 testing acc: 0.965\n",
      "epoch: 8 iter: 778 loss: 0.07687 training acc: 0.97726 testing acc: 0.9666\n",
      "epoch: 8 iter: 779 loss: 0.07997 training acc: 0.97544 testing acc: 0.965\n",
      "epoch: 8 iter: 780 loss: 0.07538 training acc: 0.97752 testing acc: 0.9666\n",
      "epoch: 8 iter: 781 loss: 0.07496 training acc: 0.97772 testing acc: 0.9662\n",
      "epoch: 8 iter: 782 loss: 0.07564 training acc: 0.97732 testing acc: 0.9654\n",
      "epoch: 8 iter: 783 loss: 0.07435 training acc: 0.97812 testing acc: 0.9668\n",
      "epoch: 8 iter: 784 loss: 0.07302 training acc: 0.97812 testing acc: 0.9666\n",
      "epoch: 8 iter: 785 loss: 0.07615 training acc: 0.97784 testing acc: 0.9658\n",
      "epoch: 8 iter: 786 loss: 0.07410 training acc: 0.97864 testing acc: 0.9676\n",
      "epoch: 8 iter: 787 loss: 0.08047 training acc: 0.97548 testing acc: 0.963\n",
      "epoch: 8 iter: 788 loss: 0.07257 training acc: 0.97958 testing acc: 0.9654\n",
      "epoch: 8 iter: 789 loss: 0.07899 training acc: 0.97666 testing acc: 0.963\n",
      "epoch: 8 iter: 790 loss: 0.09143 training acc: 0.97266 testing acc: 0.9606\n",
      "epoch: 8 iter: 791 loss: 0.08149 training acc: 0.97572 testing acc: 0.9632\n",
      "epoch: 8 iter: 792 loss: 0.08094 training acc: 0.97604 testing acc: 0.961\n",
      "epoch: 8 iter: 793 loss: 0.07449 training acc: 0.97792 testing acc: 0.9634\n",
      "epoch: 8 iter: 794 loss: 0.07561 training acc: 0.97794 testing acc: 0.9632\n",
      "epoch: 8 iter: 795 loss: 0.07391 training acc: 0.97842 testing acc: 0.964\n",
      "epoch: 8 iter: 796 loss: 0.07252 training acc: 0.97858 testing acc: 0.9666\n",
      "epoch: 8 iter: 797 loss: 0.07487 training acc: 0.97768 testing acc: 0.9642\n",
      "epoch: 8 iter: 798 loss: 0.07675 training acc: 0.97758 testing acc: 0.9632\n",
      "epoch: 8 iter: 799 loss: 0.11956 training acc: 0.96136 testing acc: 0.9506\n",
      "epoch: 8 iter: 800 loss: 0.09036 training acc: 0.97304 testing acc: 0.9592\n",
      "epoch: 8 iter: 801 loss: 0.08610 training acc: 0.97436 testing acc: 0.9598\n",
      "epoch: 8 iter: 802 loss: 0.08307 training acc: 0.97576 testing acc: 0.9608\n",
      "epoch: 8 iter: 803 loss: 0.07844 training acc: 0.97734 testing acc: 0.9626\n",
      "epoch: 8 iter: 804 loss: 0.07271 training acc: 0.97858 testing acc: 0.967\n",
      "epoch: 8 iter: 805 loss: 0.07842 training acc: 0.97652 testing acc: 0.964\n",
      "epoch: 8 iter: 806 loss: 0.07590 training acc: 0.97756 testing acc: 0.965\n",
      "epoch: 8 iter: 807 loss: 0.08059 training acc: 0.97628 testing acc: 0.9634\n",
      "epoch: 8 iter: 808 loss: 0.07823 training acc: 0.97730 testing acc: 0.965\n",
      "epoch: 8 iter: 809 loss: 0.08335 training acc: 0.97494 testing acc: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 iter: 810 loss: 0.07377 training acc: 0.97866 testing acc: 0.9668\n",
      "epoch: 8 iter: 811 loss: 0.06944 training acc: 0.98018 testing acc: 0.9684\n",
      "epoch: 8 iter: 812 loss: 0.06971 training acc: 0.98006 testing acc: 0.9686\n",
      "epoch: 8 iter: 813 loss: 0.07087 training acc: 0.97962 testing acc: 0.9668\n",
      "epoch: 8 iter: 814 loss: 0.07150 training acc: 0.97958 testing acc: 0.969\n",
      "epoch: 8 iter: 815 loss: 0.07096 training acc: 0.98004 testing acc: 0.9678\n",
      "epoch: 8 iter: 816 loss: 0.07195 training acc: 0.97996 testing acc: 0.9694\n",
      "epoch: 8 iter: 817 loss: 0.07294 training acc: 0.97868 testing acc: 0.9694\n",
      "epoch: 8 iter: 818 loss: 0.07161 training acc: 0.97932 testing acc: 0.9698\n",
      "epoch: 8 iter: 819 loss: 0.07497 training acc: 0.97808 testing acc: 0.969\n",
      "epoch: 8 iter: 820 loss: 0.08071 training acc: 0.97624 testing acc: 0.9676\n",
      "epoch: 8 iter: 821 loss: 0.07284 training acc: 0.97852 testing acc: 0.9686\n",
      "epoch: 8 iter: 822 loss: 0.07540 training acc: 0.97782 testing acc: 0.9682\n",
      "epoch: 8 iter: 823 loss: 0.11105 training acc: 0.96526 testing acc: 0.9534\n",
      "epoch: 8 iter: 824 loss: 0.09670 training acc: 0.97020 testing acc: 0.9582\n",
      "epoch: 8 iter: 825 loss: 0.09322 training acc: 0.97094 testing acc: 0.96\n",
      "epoch: 8 iter: 826 loss: 0.09064 training acc: 0.97208 testing acc: 0.961\n",
      "epoch: 8 iter: 827 loss: 0.09234 training acc: 0.97206 testing acc: 0.9602\n",
      "epoch: 8 iter: 828 loss: 0.09028 training acc: 0.97288 testing acc: 0.9594\n",
      "epoch: 8 iter: 829 loss: 0.08274 training acc: 0.97504 testing acc: 0.9624\n",
      "epoch: 8 iter: 830 loss: 0.08568 training acc: 0.97334 testing acc: 0.9616\n",
      "epoch: 8 iter: 831 loss: 0.08094 training acc: 0.97498 testing acc: 0.964\n",
      "epoch: 8 iter: 832 loss: 0.08220 training acc: 0.97440 testing acc: 0.9604\n",
      "epoch: 8 iter: 833 loss: 0.08570 training acc: 0.97348 testing acc: 0.9598\n",
      "epoch: 8 iter: 834 loss: 0.08947 training acc: 0.97184 testing acc: 0.96\n",
      "epoch: 8 iter: 835 loss: 0.07738 training acc: 0.97616 testing acc: 0.9636\n",
      "epoch: 8 iter: 836 loss: 0.07561 training acc: 0.97674 testing acc: 0.9632\n",
      "epoch: 8 iter: 837 loss: 0.07691 training acc: 0.97644 testing acc: 0.9626\n",
      "epoch: 8 iter: 838 loss: 0.07703 training acc: 0.97602 testing acc: 0.9628\n",
      "epoch: 8 iter: 839 loss: 0.07561 training acc: 0.97652 testing acc: 0.9644\n",
      "epoch: 8 iter: 840 loss: 0.08360 training acc: 0.97384 testing acc: 0.9608\n",
      "epoch: 8 iter: 841 loss: 0.07902 training acc: 0.97580 testing acc: 0.9652\n",
      "epoch: 8 iter: 842 loss: 0.07767 training acc: 0.97608 testing acc: 0.9624\n",
      "epoch: 8 iter: 843 loss: 0.07872 training acc: 0.97572 testing acc: 0.9632\n",
      "epoch: 8 iter: 844 loss: 0.07873 training acc: 0.97576 testing acc: 0.9622\n",
      "epoch: 8 iter: 845 loss: 0.07362 training acc: 0.97764 testing acc: 0.9648\n",
      "epoch: 8 iter: 846 loss: 0.07766 training acc: 0.97668 testing acc: 0.9648\n",
      "epoch: 8 iter: 847 loss: 0.07261 training acc: 0.97836 testing acc: 0.9672\n",
      "epoch: 8 iter: 848 loss: 0.07301 training acc: 0.97838 testing acc: 0.9668\n",
      "epoch: 8 iter: 849 loss: 0.06783 training acc: 0.98034 testing acc: 0.9694\n",
      "epoch: 8 iter: 850 loss: 0.07835 training acc: 0.97686 testing acc: 0.9632\n",
      "epoch: 8 iter: 851 loss: 0.08021 training acc: 0.97624 testing acc: 0.9626\n",
      "epoch: 8 iter: 852 loss: 0.07650 training acc: 0.97718 testing acc: 0.9636\n",
      "epoch: 8 iter: 853 loss: 0.07370 training acc: 0.97828 testing acc: 0.9642\n",
      "epoch: 8 iter: 854 loss: 0.06926 training acc: 0.97968 testing acc: 0.9676\n",
      "epoch: 8 iter: 855 loss: 0.07115 training acc: 0.97892 testing acc: 0.9668\n",
      "epoch: 8 iter: 856 loss: 0.10164 training acc: 0.96842 testing acc: 0.9612\n",
      "epoch: 8 iter: 857 loss: 0.07254 training acc: 0.97846 testing acc: 0.9706\n",
      "epoch: 8 iter: 858 loss: 0.07075 training acc: 0.97988 testing acc: 0.9678\n",
      "epoch: 8 iter: 859 loss: 0.07103 training acc: 0.97992 testing acc: 0.9684\n",
      "epoch: 8 iter: 860 loss: 0.07222 training acc: 0.97924 testing acc: 0.9676\n",
      "epoch: 8 iter: 861 loss: 0.07195 training acc: 0.97926 testing acc: 0.9676\n",
      "epoch: 8 iter: 862 loss: 0.08789 training acc: 0.97414 testing acc: 0.9672\n",
      "epoch: 8 iter: 863 loss: 0.07972 training acc: 0.97692 testing acc: 0.968\n",
      "epoch: 8 iter: 864 loss: 0.07654 training acc: 0.97758 testing acc: 0.9676\n",
      "epoch: 8 iter: 865 loss: 0.07843 training acc: 0.97678 testing acc: 0.966\n",
      "epoch: 8 iter: 866 loss: 0.07781 training acc: 0.97736 testing acc: 0.9664\n",
      "epoch: 8 iter: 867 loss: 0.07465 training acc: 0.97862 testing acc: 0.9682\n",
      "epoch: 8 iter: 868 loss: 0.07102 training acc: 0.97976 testing acc: 0.969\n",
      "epoch: 8 iter: 869 loss: 0.06763 training acc: 0.98034 testing acc: 0.9692\n",
      "epoch: 8 iter: 870 loss: 0.06658 training acc: 0.98108 testing acc: 0.9698\n",
      "epoch: 8 iter: 871 loss: 0.06686 training acc: 0.98102 testing acc: 0.9696\n",
      "epoch: 8 iter: 872 loss: 0.06874 training acc: 0.98030 testing acc: 0.97\n",
      "epoch: 8 iter: 873 loss: 0.07424 training acc: 0.97840 testing acc: 0.9676\n",
      "epoch: 8 iter: 874 loss: 0.07254 training acc: 0.97924 testing acc: 0.9676\n",
      "epoch: 8 iter: 875 loss: 0.06838 training acc: 0.98012 testing acc: 0.968\n",
      "epoch: 8 iter: 876 loss: 0.06873 training acc: 0.98064 testing acc: 0.9698\n",
      "epoch: 8 iter: 877 loss: 0.06770 training acc: 0.98106 testing acc: 0.9702\n",
      "epoch: 8 iter: 878 loss: 0.06831 training acc: 0.98038 testing acc: 0.9698\n",
      "epoch: 8 iter: 879 loss: 0.06774 training acc: 0.98056 testing acc: 0.97\n",
      "epoch: 8 iter: 880 loss: 0.09017 training acc: 0.97328 testing acc: 0.962\n",
      "epoch: 8 iter: 881 loss: 0.08285 training acc: 0.97500 testing acc: 0.9648\n",
      "epoch: 8 iter: 882 loss: 0.08182 training acc: 0.97562 testing acc: 0.9656\n",
      "epoch: 8 iter: 883 loss: 0.07830 training acc: 0.97702 testing acc: 0.9666\n",
      "epoch: 8 iter: 884 loss: 0.07508 training acc: 0.97768 testing acc: 0.9656\n",
      "epoch: 8 iter: 885 loss: 0.07156 training acc: 0.97908 testing acc: 0.9678\n",
      "epoch: 8 iter: 886 loss: 0.06976 training acc: 0.97932 testing acc: 0.9662\n",
      "epoch: 8 iter: 887 loss: 0.07516 training acc: 0.97822 testing acc: 0.9646\n",
      "epoch: 8 iter: 888 loss: 0.07375 training acc: 0.97862 testing acc: 0.9656\n",
      "epoch: 8 iter: 889 loss: 0.07245 training acc: 0.97892 testing acc: 0.9668\n",
      "epoch: 8 iter: 890 loss: 0.07023 training acc: 0.97898 testing acc: 0.9658\n",
      "epoch: 8 iter: 891 loss: 0.07209 training acc: 0.97814 testing acc: 0.9654\n",
      "epoch: 8 iter: 892 loss: 0.07107 training acc: 0.97854 testing acc: 0.9662\n",
      "epoch: 8 iter: 893 loss: 0.07300 training acc: 0.97772 testing acc: 0.9648\n",
      "epoch: 8 iter: 894 loss: 0.07591 training acc: 0.97688 testing acc: 0.9642\n",
      "epoch: 8 iter: 895 loss: 0.07627 training acc: 0.97664 testing acc: 0.9642\n",
      "epoch: 8 iter: 896 loss: 0.06999 training acc: 0.97932 testing acc: 0.9658\n",
      "epoch: 8 iter: 897 loss: 0.07241 training acc: 0.97832 testing acc: 0.9658\n",
      "epoch: 8 iter: 898 loss: 0.06875 training acc: 0.97958 testing acc: 0.968\n",
      "epoch: 8 iter: 899 loss: 0.06786 training acc: 0.98022 testing acc: 0.969\n",
      "epoch: 8 iter: 900 loss: 0.06779 training acc: 0.98012 testing acc: 0.9694\n",
      "epoch: 8 iter: 901 loss: 0.06847 training acc: 0.98026 testing acc: 0.968\n",
      "epoch: 8 iter: 902 loss: 0.07128 training acc: 0.97908 testing acc: 0.968\n",
      "epoch: 8 iter: 903 loss: 0.07260 training acc: 0.97880 testing acc: 0.9664\n",
      "epoch: 8 iter: 904 loss: 0.06935 training acc: 0.97994 testing acc: 0.969\n",
      "epoch: 8 iter: 905 loss: 0.07167 training acc: 0.97884 testing acc: 0.968\n",
      "epoch: 8 iter: 906 loss: 0.07001 training acc: 0.97944 testing acc: 0.9684\n",
      "epoch: 8 iter: 907 loss: 0.07129 training acc: 0.97886 testing acc: 0.9686\n",
      "epoch: 8 iter: 908 loss: 0.08327 training acc: 0.97540 testing acc: 0.9666\n",
      "epoch: 8 iter: 909 loss: 0.08280 training acc: 0.97514 testing acc: 0.9658\n",
      "epoch: 8 iter: 910 loss: 0.08201 training acc: 0.97542 testing acc: 0.9662\n",
      "epoch: 8 iter: 911 loss: 0.08606 training acc: 0.97342 testing acc: 0.9644\n",
      "epoch: 8 iter: 912 loss: 0.08585 training acc: 0.97374 testing acc: 0.9638\n",
      "epoch: 8 iter: 913 loss: 0.07332 training acc: 0.97760 testing acc: 0.9684\n",
      "epoch: 8 iter: 914 loss: 0.07345 training acc: 0.97756 testing acc: 0.9674\n",
      "epoch: 8 iter: 915 loss: 0.07072 training acc: 0.97900 testing acc: 0.9682\n",
      "epoch: 8 iter: 916 loss: 0.06888 training acc: 0.97952 testing acc: 0.969\n",
      "epoch: 8 iter: 917 loss: 0.07472 training acc: 0.97834 testing acc: 0.9664\n",
      "epoch: 8 iter: 918 loss: 0.07030 training acc: 0.97896 testing acc: 0.9672\n",
      "epoch: 8 iter: 919 loss: 0.06843 training acc: 0.97952 testing acc: 0.9672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 iter: 920 loss: 0.07523 training acc: 0.97846 testing acc: 0.9658\n",
      "epoch: 8 iter: 921 loss: 0.06976 training acc: 0.97982 testing acc: 0.968\n",
      "epoch: 8 iter: 922 loss: 0.06888 training acc: 0.97988 testing acc: 0.9676\n",
      "epoch: 8 iter: 923 loss: 0.07305 training acc: 0.97856 testing acc: 0.9682\n",
      "epoch: 8 iter: 924 loss: 0.07367 training acc: 0.97832 testing acc: 0.967\n",
      "epoch: 8 iter: 925 loss: 0.08130 training acc: 0.97564 testing acc: 0.9656\n",
      "epoch: 8 iter: 926 loss: 0.08104 training acc: 0.97614 testing acc: 0.9654\n",
      "epoch: 8 iter: 927 loss: 0.07199 training acc: 0.97874 testing acc: 0.9682\n",
      "epoch: 8 iter: 928 loss: 0.07307 training acc: 0.97764 testing acc: 0.9658\n",
      "epoch: 8 iter: 929 loss: 0.07099 training acc: 0.97854 testing acc: 0.968\n",
      "epoch: 8 iter: 930 loss: 0.07639 training acc: 0.97692 testing acc: 0.964\n",
      "epoch: 8 iter: 931 loss: 0.07906 training acc: 0.97556 testing acc: 0.9612\n",
      "epoch: 8 iter: 932 loss: 0.07652 training acc: 0.97646 testing acc: 0.9626\n",
      "epoch: 8 iter: 933 loss: 0.07257 training acc: 0.97794 testing acc: 0.9656\n",
      "epoch: 8 iter: 934 loss: 0.07447 training acc: 0.97758 testing acc: 0.9634\n",
      "epoch: 8 iter: 935 loss: 0.07452 training acc: 0.97760 testing acc: 0.9644\n",
      "epoch: 8 iter: 936 loss: 0.07049 training acc: 0.97932 testing acc: 0.9668\n",
      "epoch: 8 iter: 937 loss: 0.07080 training acc: 0.97930 testing acc: 0.9666\n",
      "epoch: 8 iter: 938 loss: 0.06921 training acc: 0.97970 testing acc: 0.967\n",
      "epoch: 8 iter: 939 loss: 0.07379 training acc: 0.97824 testing acc: 0.9648\n",
      "epoch: 8 iter: 940 loss: 0.07145 training acc: 0.97890 testing acc: 0.9664\n",
      "epoch: 8 iter: 941 loss: 0.07493 training acc: 0.97726 testing acc: 0.9656\n",
      "epoch: 8 iter: 942 loss: 0.07488 training acc: 0.97768 testing acc: 0.965\n",
      "epoch: 8 iter: 943 loss: 0.07409 training acc: 0.97790 testing acc: 0.9654\n",
      "epoch: 8 iter: 944 loss: 0.07458 training acc: 0.97776 testing acc: 0.9652\n",
      "epoch: 8 iter: 945 loss: 0.06938 training acc: 0.97948 testing acc: 0.9666\n",
      "epoch: 8 iter: 946 loss: 0.06750 training acc: 0.98022 testing acc: 0.968\n",
      "epoch: 8 iter: 947 loss: 0.07008 training acc: 0.97964 testing acc: 0.9682\n",
      "epoch: 8 iter: 948 loss: 0.07069 training acc: 0.97964 testing acc: 0.9678\n",
      "epoch: 8 iter: 949 loss: 0.06953 training acc: 0.97986 testing acc: 0.9688\n",
      "epoch: 8 iter: 950 loss: 0.07036 training acc: 0.97914 testing acc: 0.9666\n",
      "epoch: 8 iter: 951 loss: 0.06634 training acc: 0.98048 testing acc: 0.9704\n",
      "epoch: 8 iter: 952 loss: 0.06714 training acc: 0.98048 testing acc: 0.9696\n",
      "epoch: 8 iter: 953 loss: 0.07972 training acc: 0.97564 testing acc: 0.9652\n",
      "epoch: 8 iter: 954 loss: 0.07534 training acc: 0.97730 testing acc: 0.9664\n",
      "epoch: 8 iter: 955 loss: 0.07963 training acc: 0.97606 testing acc: 0.964\n",
      "epoch: 8 iter: 956 loss: 0.07727 training acc: 0.97648 testing acc: 0.9636\n",
      "epoch: 8 iter: 957 loss: 0.06936 training acc: 0.97902 testing acc: 0.966\n",
      "epoch: 8 iter: 958 loss: 0.06831 training acc: 0.97992 testing acc: 0.9686\n",
      "epoch: 8 iter: 959 loss: 0.06493 training acc: 0.98128 testing acc: 0.9692\n",
      "epoch: 8 iter: 960 loss: 0.06636 training acc: 0.98042 testing acc: 0.969\n",
      "epoch: 8 iter: 961 loss: 0.06830 training acc: 0.97990 testing acc: 0.9688\n",
      "epoch: 8 iter: 962 loss: 0.08254 training acc: 0.97504 testing acc: 0.964\n",
      "epoch: 8 iter: 963 loss: 0.07064 training acc: 0.97914 testing acc: 0.9688\n",
      "epoch: 8 iter: 964 loss: 0.08004 training acc: 0.97576 testing acc: 0.9666\n",
      "epoch: 8 iter: 965 loss: 0.07172 training acc: 0.97862 testing acc: 0.9684\n",
      "epoch: 8 iter: 966 loss: 0.07364 training acc: 0.97742 testing acc: 0.9694\n",
      "epoch: 8 iter: 967 loss: 0.07040 training acc: 0.97926 testing acc: 0.9688\n",
      "epoch: 8 iter: 968 loss: 0.06671 training acc: 0.98094 testing acc: 0.9704\n",
      "epoch: 8 iter: 969 loss: 0.06686 training acc: 0.98028 testing acc: 0.9708\n",
      "epoch: 8 iter: 970 loss: 0.07068 training acc: 0.97954 testing acc: 0.9706\n",
      "epoch: 8 iter: 971 loss: 0.06799 training acc: 0.98072 testing acc: 0.9712\n",
      "epoch: 8 iter: 972 loss: 0.06865 training acc: 0.98042 testing acc: 0.97\n",
      "epoch: 8 iter: 973 loss: 0.06959 training acc: 0.98008 testing acc: 0.9692\n",
      "epoch: 8 iter: 974 loss: 0.06788 training acc: 0.98026 testing acc: 0.9702\n",
      "epoch: 8 iter: 975 loss: 0.06959 training acc: 0.98034 testing acc: 0.9694\n",
      "epoch: 8 iter: 976 loss: 0.07966 training acc: 0.97648 testing acc: 0.9658\n",
      "epoch: 8 iter: 977 loss: 0.08019 training acc: 0.97636 testing acc: 0.9654\n",
      "epoch: 8 iter: 978 loss: 0.08378 training acc: 0.97482 testing acc: 0.964\n",
      "epoch: 8 iter: 979 loss: 0.07738 training acc: 0.97726 testing acc: 0.9654\n",
      "epoch: 8 iter: 980 loss: 0.06898 training acc: 0.97982 testing acc: 0.9682\n",
      "epoch: 8 iter: 981 loss: 0.07896 training acc: 0.97688 testing acc: 0.968\n",
      "epoch: 8 iter: 982 loss: 0.06642 training acc: 0.98072 testing acc: 0.9706\n",
      "epoch: 8 iter: 983 loss: 0.06655 training acc: 0.98136 testing acc: 0.9708\n",
      "epoch: 8 iter: 984 loss: 0.07766 training acc: 0.97782 testing acc: 0.967\n",
      "epoch: 8 iter: 985 loss: 0.06680 training acc: 0.98132 testing acc: 0.972\n",
      "epoch: 8 iter: 986 loss: 0.06666 training acc: 0.98138 testing acc: 0.972\n",
      "epoch: 8 iter: 987 loss: 0.06606 training acc: 0.98110 testing acc: 0.971\n",
      "epoch: 8 iter: 988 loss: 0.06509 training acc: 0.98172 testing acc: 0.9712\n",
      "epoch: 8 iter: 989 loss: 0.06463 training acc: 0.98212 testing acc: 0.9718\n",
      "epoch: 8 iter: 990 loss: 0.06535 training acc: 0.98154 testing acc: 0.9712\n",
      "epoch: 8 iter: 991 loss: 0.06537 training acc: 0.98152 testing acc: 0.9712\n",
      "epoch: 8 iter: 992 loss: 0.06514 training acc: 0.98152 testing acc: 0.9718\n",
      "epoch: 8 iter: 993 loss: 0.06771 training acc: 0.98062 testing acc: 0.9692\n",
      "epoch: 8 iter: 994 loss: 0.06771 training acc: 0.98026 testing acc: 0.9702\n",
      "epoch: 8 iter: 995 loss: 0.07148 training acc: 0.97898 testing acc: 0.9702\n",
      "epoch: 8 iter: 996 loss: 0.06924 training acc: 0.97940 testing acc: 0.9686\n",
      "epoch: 8 iter: 997 loss: 0.06709 training acc: 0.98052 testing acc: 0.97\n",
      "epoch: 8 iter: 998 loss: 0.06655 training acc: 0.98058 testing acc: 0.9696\n",
      "epoch: 8 iter: 999 loss: 0.06772 training acc: 0.98016 testing acc: 0.9698\n",
      "epoch: 9 iter:   0 loss: 0.07312 training acc: 0.97802 testing acc: 0.9672\n",
      "epoch: 9 iter:   1 loss: 0.07528 training acc: 0.97722 testing acc: 0.9658\n",
      "epoch: 9 iter:   2 loss: 0.07462 training acc: 0.97784 testing acc: 0.9664\n",
      "epoch: 9 iter:   3 loss: 0.06973 training acc: 0.97934 testing acc: 0.9682\n",
      "epoch: 9 iter:   4 loss: 0.07014 training acc: 0.97878 testing acc: 0.9684\n",
      "epoch: 9 iter:   5 loss: 0.07566 training acc: 0.97650 testing acc: 0.9646\n",
      "epoch: 9 iter:   6 loss: 0.07491 training acc: 0.97758 testing acc: 0.9672\n",
      "epoch: 9 iter:   7 loss: 0.08354 training acc: 0.97514 testing acc: 0.9648\n",
      "epoch: 9 iter:   8 loss: 0.08680 training acc: 0.97382 testing acc: 0.9626\n",
      "epoch: 9 iter:   9 loss: 0.08765 training acc: 0.97328 testing acc: 0.961\n",
      "epoch: 9 iter:  10 loss: 0.07506 training acc: 0.97804 testing acc: 0.9662\n",
      "epoch: 9 iter:  11 loss: 0.06858 training acc: 0.97980 testing acc: 0.969\n",
      "epoch: 9 iter:  12 loss: 0.06960 training acc: 0.97930 testing acc: 0.9672\n",
      "epoch: 9 iter:  13 loss: 0.06690 training acc: 0.98066 testing acc: 0.9686\n",
      "epoch: 9 iter:  14 loss: 0.06610 training acc: 0.98108 testing acc: 0.9688\n",
      "epoch: 9 iter:  15 loss: 0.06611 training acc: 0.98118 testing acc: 0.9696\n",
      "epoch: 9 iter:  16 loss: 0.07010 training acc: 0.97974 testing acc: 0.9684\n",
      "epoch: 9 iter:  17 loss: 0.06872 training acc: 0.98006 testing acc: 0.9694\n",
      "epoch: 9 iter:  18 loss: 0.06824 training acc: 0.98124 testing acc: 0.97\n",
      "epoch: 9 iter:  19 loss: 0.06820 training acc: 0.98108 testing acc: 0.9702\n",
      "epoch: 9 iter:  20 loss: 0.06662 training acc: 0.98144 testing acc: 0.9708\n",
      "epoch: 9 iter:  21 loss: 0.07256 training acc: 0.97800 testing acc: 0.9676\n",
      "epoch: 9 iter:  22 loss: 0.06553 training acc: 0.98148 testing acc: 0.9708\n",
      "epoch: 9 iter:  23 loss: 0.06718 training acc: 0.98122 testing acc: 0.9712\n",
      "epoch: 9 iter:  24 loss: 0.06709 training acc: 0.98074 testing acc: 0.9718\n",
      "epoch: 9 iter:  25 loss: 0.06691 training acc: 0.98076 testing acc: 0.9712\n",
      "epoch: 9 iter:  26 loss: 0.06671 training acc: 0.98078 testing acc: 0.9716\n",
      "epoch: 9 iter:  27 loss: 0.06654 training acc: 0.98066 testing acc: 0.9718\n",
      "epoch: 9 iter:  28 loss: 0.07274 training acc: 0.97840 testing acc: 0.9678\n",
      "epoch: 9 iter:  29 loss: 0.06943 training acc: 0.97900 testing acc: 0.9686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 iter:  30 loss: 0.06529 training acc: 0.98086 testing acc: 0.9702\n",
      "epoch: 9 iter:  31 loss: 0.06530 training acc: 0.98128 testing acc: 0.9706\n",
      "epoch: 9 iter:  32 loss: 0.06853 training acc: 0.97984 testing acc: 0.9704\n",
      "epoch: 9 iter:  33 loss: 0.06936 training acc: 0.97968 testing acc: 0.9706\n",
      "epoch: 9 iter:  34 loss: 0.06824 training acc: 0.97994 testing acc: 0.9708\n",
      "epoch: 9 iter:  35 loss: 0.08022 training acc: 0.97558 testing acc: 0.9678\n",
      "epoch: 9 iter:  36 loss: 0.08394 training acc: 0.97458 testing acc: 0.9648\n",
      "epoch: 9 iter:  37 loss: 0.07152 training acc: 0.97970 testing acc: 0.9694\n",
      "epoch: 9 iter:  38 loss: 0.07014 training acc: 0.97984 testing acc: 0.9692\n",
      "epoch: 9 iter:  39 loss: 0.07074 training acc: 0.97962 testing acc: 0.969\n",
      "epoch: 9 iter:  40 loss: 0.06837 training acc: 0.98036 testing acc: 0.9696\n",
      "epoch: 9 iter:  41 loss: 0.06797 training acc: 0.98038 testing acc: 0.9698\n",
      "epoch: 9 iter:  42 loss: 0.07124 training acc: 0.97982 testing acc: 0.9696\n",
      "epoch: 9 iter:  43 loss: 0.06550 training acc: 0.98124 testing acc: 0.9722\n",
      "epoch: 9 iter:  44 loss: 0.06455 training acc: 0.98170 testing acc: 0.9722\n",
      "epoch: 9 iter:  45 loss: 0.06522 training acc: 0.98152 testing acc: 0.9732\n",
      "epoch: 9 iter:  46 loss: 0.07705 training acc: 0.97760 testing acc: 0.9694\n",
      "epoch: 9 iter:  47 loss: 0.06917 training acc: 0.97990 testing acc: 0.9704\n",
      "epoch: 9 iter:  48 loss: 0.07316 training acc: 0.97848 testing acc: 0.9684\n",
      "epoch: 9 iter:  49 loss: 0.07346 training acc: 0.97800 testing acc: 0.9676\n",
      "epoch: 9 iter:  50 loss: 0.07782 training acc: 0.97612 testing acc: 0.9676\n",
      "epoch: 9 iter:  51 loss: 0.07472 training acc: 0.97760 testing acc: 0.967\n",
      "epoch: 9 iter:  52 loss: 0.06501 training acc: 0.98086 testing acc: 0.971\n",
      "epoch: 9 iter:  53 loss: 0.07655 training acc: 0.97668 testing acc: 0.9646\n",
      "epoch: 9 iter:  54 loss: 0.07800 training acc: 0.97670 testing acc: 0.9658\n",
      "epoch: 9 iter:  55 loss: 0.07606 training acc: 0.97716 testing acc: 0.9656\n",
      "epoch: 9 iter:  56 loss: 0.07249 training acc: 0.97832 testing acc: 0.9664\n",
      "epoch: 9 iter:  57 loss: 0.08601 training acc: 0.97336 testing acc: 0.9622\n",
      "epoch: 9 iter:  58 loss: 0.08344 training acc: 0.97450 testing acc: 0.9644\n",
      "epoch: 9 iter:  59 loss: 0.08072 training acc: 0.97560 testing acc: 0.9658\n",
      "epoch: 9 iter:  60 loss: 0.07478 training acc: 0.97672 testing acc: 0.9658\n",
      "epoch: 9 iter:  61 loss: 0.07733 training acc: 0.97584 testing acc: 0.9672\n",
      "epoch: 9 iter:  62 loss: 0.07124 training acc: 0.97934 testing acc: 0.9682\n",
      "epoch: 9 iter:  63 loss: 0.06888 training acc: 0.97960 testing acc: 0.9678\n",
      "epoch: 9 iter:  64 loss: 0.06860 training acc: 0.97964 testing acc: 0.9686\n",
      "epoch: 9 iter:  65 loss: 0.06922 training acc: 0.97960 testing acc: 0.9682\n",
      "epoch: 9 iter:  66 loss: 0.06648 training acc: 0.98088 testing acc: 0.9694\n",
      "epoch: 9 iter:  67 loss: 0.06602 training acc: 0.98108 testing acc: 0.9696\n",
      "epoch: 9 iter:  68 loss: 0.06897 training acc: 0.97966 testing acc: 0.9674\n",
      "epoch: 9 iter:  69 loss: 0.07036 training acc: 0.97866 testing acc: 0.9664\n",
      "epoch: 9 iter:  70 loss: 0.07611 training acc: 0.97718 testing acc: 0.9656\n",
      "epoch: 9 iter:  71 loss: 0.07161 training acc: 0.97886 testing acc: 0.9668\n",
      "epoch: 9 iter:  72 loss: 0.06789 training acc: 0.98002 testing acc: 0.969\n",
      "epoch: 9 iter:  73 loss: 0.06578 training acc: 0.98092 testing acc: 0.9704\n",
      "epoch: 9 iter:  74 loss: 0.06477 training acc: 0.98114 testing acc: 0.9708\n",
      "epoch: 9 iter:  75 loss: 0.06566 training acc: 0.98074 testing acc: 0.97\n",
      "epoch: 9 iter:  76 loss: 0.06433 training acc: 0.98156 testing acc: 0.9698\n",
      "epoch: 9 iter:  77 loss: 0.06613 training acc: 0.98050 testing acc: 0.9692\n",
      "epoch: 9 iter:  78 loss: 0.06529 training acc: 0.98100 testing acc: 0.9694\n",
      "epoch: 9 iter:  79 loss: 0.06738 training acc: 0.98038 testing acc: 0.9684\n",
      "epoch: 9 iter:  80 loss: 0.06442 training acc: 0.98130 testing acc: 0.9698\n",
      "epoch: 9 iter:  81 loss: 0.08050 training acc: 0.97522 testing acc: 0.9646\n",
      "epoch: 9 iter:  82 loss: 0.07194 training acc: 0.97902 testing acc: 0.9668\n",
      "epoch: 9 iter:  83 loss: 0.06984 training acc: 0.97962 testing acc: 0.9678\n",
      "epoch: 9 iter:  84 loss: 0.07325 training acc: 0.97814 testing acc: 0.9672\n",
      "epoch: 9 iter:  85 loss: 0.07659 training acc: 0.97696 testing acc: 0.9672\n",
      "epoch: 9 iter:  86 loss: 0.08053 training acc: 0.97536 testing acc: 0.966\n",
      "epoch: 9 iter:  87 loss: 0.07933 training acc: 0.97584 testing acc: 0.9634\n",
      "epoch: 9 iter:  88 loss: 0.06700 training acc: 0.98078 testing acc: 0.969\n",
      "epoch: 9 iter:  89 loss: 0.11995 training acc: 0.96210 testing acc: 0.9514\n",
      "epoch: 9 iter:  90 loss: 0.07506 training acc: 0.97798 testing acc: 0.9664\n",
      "epoch: 9 iter:  91 loss: 0.07330 training acc: 0.97864 testing acc: 0.9678\n",
      "epoch: 9 iter:  92 loss: 0.06930 training acc: 0.97974 testing acc: 0.9706\n",
      "epoch: 9 iter:  93 loss: 0.06821 training acc: 0.98046 testing acc: 0.9716\n",
      "epoch: 9 iter:  94 loss: 0.06537 training acc: 0.98096 testing acc: 0.9692\n",
      "epoch: 9 iter:  95 loss: 0.06863 training acc: 0.98014 testing acc: 0.9656\n",
      "epoch: 9 iter:  96 loss: 0.06862 training acc: 0.98034 testing acc: 0.9662\n",
      "epoch: 9 iter:  97 loss: 0.07005 training acc: 0.97972 testing acc: 0.966\n",
      "epoch: 9 iter:  98 loss: 0.07007 training acc: 0.97978 testing acc: 0.9664\n",
      "epoch: 9 iter:  99 loss: 0.06873 training acc: 0.97964 testing acc: 0.9672\n",
      "epoch: 9 iter: 100 loss: 0.06755 training acc: 0.98006 testing acc: 0.968\n",
      "epoch: 9 iter: 101 loss: 0.06775 training acc: 0.97986 testing acc: 0.968\n",
      "epoch: 9 iter: 102 loss: 0.06887 training acc: 0.97932 testing acc: 0.9666\n",
      "epoch: 9 iter: 103 loss: 0.07261 training acc: 0.97836 testing acc: 0.9656\n",
      "epoch: 9 iter: 104 loss: 0.07123 training acc: 0.97902 testing acc: 0.966\n",
      "epoch: 9 iter: 105 loss: 0.07064 training acc: 0.97920 testing acc: 0.966\n",
      "epoch: 9 iter: 106 loss: 0.07050 training acc: 0.97950 testing acc: 0.9666\n",
      "epoch: 9 iter: 107 loss: 0.07107 training acc: 0.97878 testing acc: 0.9682\n",
      "epoch: 9 iter: 108 loss: 0.07341 training acc: 0.97778 testing acc: 0.9658\n",
      "epoch: 9 iter: 109 loss: 0.07193 training acc: 0.97818 testing acc: 0.9652\n",
      "epoch: 9 iter: 110 loss: 0.06558 training acc: 0.98062 testing acc: 0.9684\n",
      "epoch: 9 iter: 111 loss: 0.06520 training acc: 0.98100 testing acc: 0.9672\n",
      "epoch: 9 iter: 112 loss: 0.06317 training acc: 0.98154 testing acc: 0.9672\n",
      "epoch: 9 iter: 113 loss: 0.06453 training acc: 0.98132 testing acc: 0.97\n",
      "epoch: 9 iter: 114 loss: 0.06461 training acc: 0.98110 testing acc: 0.9702\n",
      "epoch: 9 iter: 115 loss: 0.06570 training acc: 0.98056 testing acc: 0.9678\n",
      "epoch: 9 iter: 116 loss: 0.06626 training acc: 0.98034 testing acc: 0.9688\n",
      "epoch: 9 iter: 117 loss: 0.07793 training acc: 0.97630 testing acc: 0.9626\n",
      "epoch: 9 iter: 118 loss: 0.06692 training acc: 0.97990 testing acc: 0.9688\n",
      "epoch: 9 iter: 119 loss: 0.06534 training acc: 0.98098 testing acc: 0.9692\n",
      "epoch: 9 iter: 120 loss: 0.06685 training acc: 0.98024 testing acc: 0.969\n",
      "epoch: 9 iter: 121 loss: 0.06962 training acc: 0.97956 testing acc: 0.9658\n",
      "epoch: 9 iter: 122 loss: 0.06810 training acc: 0.98004 testing acc: 0.965\n",
      "epoch: 9 iter: 123 loss: 0.06578 training acc: 0.98096 testing acc: 0.9682\n",
      "epoch: 9 iter: 124 loss: 0.06499 training acc: 0.98108 testing acc: 0.9678\n",
      "epoch: 9 iter: 125 loss: 0.06759 training acc: 0.97998 testing acc: 0.9658\n",
      "epoch: 9 iter: 126 loss: 0.06708 training acc: 0.97984 testing acc: 0.9666\n",
      "epoch: 9 iter: 127 loss: 0.07142 training acc: 0.97900 testing acc: 0.9664\n",
      "epoch: 9 iter: 128 loss: 0.06269 training acc: 0.98178 testing acc: 0.9694\n",
      "epoch: 9 iter: 129 loss: 0.07178 training acc: 0.97874 testing acc: 0.9658\n",
      "epoch: 9 iter: 130 loss: 0.06492 training acc: 0.98156 testing acc: 0.9712\n",
      "epoch: 9 iter: 131 loss: 0.06621 training acc: 0.98090 testing acc: 0.9698\n",
      "epoch: 9 iter: 132 loss: 0.06916 training acc: 0.97920 testing acc: 0.969\n",
      "epoch: 9 iter: 133 loss: 0.07186 training acc: 0.97798 testing acc: 0.9672\n",
      "epoch: 9 iter: 134 loss: 0.07086 training acc: 0.97822 testing acc: 0.9654\n",
      "epoch: 9 iter: 135 loss: 0.06318 training acc: 0.98186 testing acc: 0.9688\n",
      "epoch: 9 iter: 136 loss: 0.06362 training acc: 0.98168 testing acc: 0.968\n",
      "epoch: 9 iter: 137 loss: 0.06300 training acc: 0.98182 testing acc: 0.969\n",
      "epoch: 9 iter: 138 loss: 0.06310 training acc: 0.98198 testing acc: 0.9684\n",
      "epoch: 9 iter: 139 loss: 0.06462 training acc: 0.98144 testing acc: 0.9684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 iter: 140 loss: 0.06412 training acc: 0.98156 testing acc: 0.9686\n",
      "epoch: 9 iter: 141 loss: 0.06370 training acc: 0.98190 testing acc: 0.971\n",
      "epoch: 9 iter: 142 loss: 0.07024 training acc: 0.98036 testing acc: 0.9662\n",
      "epoch: 9 iter: 143 loss: 0.06571 training acc: 0.98154 testing acc: 0.9696\n",
      "epoch: 9 iter: 144 loss: 0.07696 training acc: 0.97774 testing acc: 0.9658\n",
      "epoch: 9 iter: 145 loss: 0.06738 training acc: 0.98084 testing acc: 0.9686\n",
      "epoch: 9 iter: 146 loss: 0.06349 training acc: 0.98190 testing acc: 0.9682\n",
      "epoch: 9 iter: 147 loss: 0.06364 training acc: 0.98184 testing acc: 0.9678\n",
      "epoch: 9 iter: 148 loss: 0.06604 training acc: 0.98082 testing acc: 0.9674\n",
      "epoch: 9 iter: 149 loss: 0.06488 training acc: 0.98124 testing acc: 0.9676\n",
      "epoch: 9 iter: 150 loss: 0.06616 training acc: 0.98116 testing acc: 0.9672\n",
      "epoch: 9 iter: 151 loss: 0.06489 training acc: 0.98152 testing acc: 0.9674\n",
      "epoch: 9 iter: 152 loss: 0.06450 training acc: 0.98126 testing acc: 0.9694\n",
      "epoch: 9 iter: 153 loss: 0.06526 training acc: 0.98116 testing acc: 0.9684\n",
      "epoch: 9 iter: 154 loss: 0.06866 training acc: 0.97958 testing acc: 0.969\n",
      "epoch: 9 iter: 155 loss: 0.07414 training acc: 0.97752 testing acc: 0.9656\n",
      "epoch: 9 iter: 156 loss: 0.07830 training acc: 0.97674 testing acc: 0.9638\n",
      "epoch: 9 iter: 157 loss: 0.06601 training acc: 0.98130 testing acc: 0.9694\n",
      "epoch: 9 iter: 158 loss: 0.06626 training acc: 0.98110 testing acc: 0.9684\n",
      "epoch: 9 iter: 159 loss: 0.06550 training acc: 0.98122 testing acc: 0.9684\n",
      "epoch: 9 iter: 160 loss: 0.06438 training acc: 0.98182 testing acc: 0.969\n",
      "epoch: 9 iter: 161 loss: 0.06391 training acc: 0.98196 testing acc: 0.9696\n",
      "epoch: 9 iter: 162 loss: 0.06315 training acc: 0.98204 testing acc: 0.969\n",
      "epoch: 9 iter: 163 loss: 0.06338 training acc: 0.98204 testing acc: 0.969\n",
      "epoch: 9 iter: 164 loss: 0.07110 training acc: 0.97868 testing acc: 0.9678\n",
      "epoch: 9 iter: 165 loss: 0.07156 training acc: 0.97826 testing acc: 0.9678\n",
      "epoch: 9 iter: 166 loss: 0.07414 training acc: 0.97716 testing acc: 0.9654\n",
      "epoch: 9 iter: 167 loss: 0.07382 training acc: 0.97708 testing acc: 0.9646\n",
      "epoch: 9 iter: 168 loss: 0.07509 training acc: 0.97824 testing acc: 0.965\n",
      "epoch: 9 iter: 169 loss: 0.07839 training acc: 0.97686 testing acc: 0.9658\n",
      "epoch: 9 iter: 170 loss: 0.07626 training acc: 0.97778 testing acc: 0.9658\n",
      "epoch: 9 iter: 171 loss: 0.07294 training acc: 0.97852 testing acc: 0.965\n",
      "epoch: 9 iter: 172 loss: 0.07225 training acc: 0.97852 testing acc: 0.9658\n",
      "epoch: 9 iter: 173 loss: 0.06848 training acc: 0.97998 testing acc: 0.967\n",
      "epoch: 9 iter: 174 loss: 0.06686 training acc: 0.98026 testing acc: 0.9686\n",
      "epoch: 9 iter: 175 loss: 0.06642 training acc: 0.98032 testing acc: 0.9688\n",
      "epoch: 9 iter: 176 loss: 0.06874 training acc: 0.98014 testing acc: 0.968\n",
      "epoch: 9 iter: 177 loss: 0.06924 training acc: 0.97990 testing acc: 0.9674\n",
      "epoch: 9 iter: 178 loss: 0.06854 training acc: 0.98014 testing acc: 0.9682\n",
      "epoch: 9 iter: 179 loss: 0.06975 training acc: 0.97936 testing acc: 0.9646\n",
      "epoch: 9 iter: 180 loss: 0.06597 training acc: 0.98098 testing acc: 0.9678\n",
      "epoch: 9 iter: 181 loss: 0.07076 training acc: 0.97944 testing acc: 0.9696\n",
      "epoch: 9 iter: 182 loss: 0.07119 training acc: 0.97922 testing acc: 0.968\n",
      "epoch: 9 iter: 183 loss: 0.07227 training acc: 0.97940 testing acc: 0.966\n",
      "epoch: 9 iter: 184 loss: 0.07388 training acc: 0.97828 testing acc: 0.9666\n",
      "epoch: 9 iter: 185 loss: 0.07203 training acc: 0.97908 testing acc: 0.9656\n",
      "epoch: 9 iter: 186 loss: 0.07174 training acc: 0.97940 testing acc: 0.9666\n",
      "epoch: 9 iter: 187 loss: 0.07032 training acc: 0.97964 testing acc: 0.9668\n",
      "epoch: 9 iter: 188 loss: 0.06718 training acc: 0.98046 testing acc: 0.9672\n",
      "epoch: 9 iter: 189 loss: 0.06660 training acc: 0.98062 testing acc: 0.9672\n",
      "epoch: 9 iter: 190 loss: 0.06560 training acc: 0.98130 testing acc: 0.9672\n",
      "epoch: 9 iter: 191 loss: 0.07180 training acc: 0.97872 testing acc: 0.9672\n",
      "epoch: 9 iter: 192 loss: 0.07350 training acc: 0.97802 testing acc: 0.9654\n",
      "epoch: 9 iter: 193 loss: 0.07209 training acc: 0.97872 testing acc: 0.9652\n",
      "epoch: 9 iter: 194 loss: 0.06438 training acc: 0.98138 testing acc: 0.9674\n",
      "epoch: 9 iter: 195 loss: 0.06353 training acc: 0.98206 testing acc: 0.968\n",
      "epoch: 9 iter: 196 loss: 0.06801 training acc: 0.98054 testing acc: 0.9668\n",
      "epoch: 9 iter: 197 loss: 0.06676 training acc: 0.98090 testing acc: 0.9676\n",
      "epoch: 9 iter: 198 loss: 0.06766 training acc: 0.98024 testing acc: 0.9684\n",
      "epoch: 9 iter: 199 loss: 0.07725 training acc: 0.97628 testing acc: 0.9664\n",
      "epoch: 9 iter: 200 loss: 0.08152 training acc: 0.97418 testing acc: 0.9646\n",
      "epoch: 9 iter: 201 loss: 0.07062 training acc: 0.97922 testing acc: 0.9688\n",
      "epoch: 9 iter: 202 loss: 0.07041 training acc: 0.97932 testing acc: 0.9678\n",
      "epoch: 9 iter: 203 loss: 0.06753 training acc: 0.98062 testing acc: 0.9672\n",
      "epoch: 9 iter: 204 loss: 0.06731 training acc: 0.98064 testing acc: 0.9676\n",
      "epoch: 9 iter: 205 loss: 0.06709 training acc: 0.98058 testing acc: 0.9674\n",
      "epoch: 9 iter: 206 loss: 0.07704 training acc: 0.97680 testing acc: 0.9638\n",
      "epoch: 9 iter: 207 loss: 0.07856 training acc: 0.97616 testing acc: 0.961\n",
      "epoch: 9 iter: 208 loss: 0.07840 training acc: 0.97640 testing acc: 0.963\n",
      "epoch: 9 iter: 209 loss: 0.08480 training acc: 0.97386 testing acc: 0.959\n",
      "epoch: 9 iter: 210 loss: 0.07960 training acc: 0.97618 testing acc: 0.9616\n",
      "epoch: 9 iter: 211 loss: 0.06665 training acc: 0.98136 testing acc: 0.968\n",
      "epoch: 9 iter: 212 loss: 0.06669 training acc: 0.98116 testing acc: 0.9682\n",
      "epoch: 9 iter: 213 loss: 0.07564 training acc: 0.97812 testing acc: 0.9654\n",
      "epoch: 9 iter: 214 loss: 0.07468 training acc: 0.97762 testing acc: 0.9644\n",
      "epoch: 9 iter: 215 loss: 0.07139 training acc: 0.97874 testing acc: 0.9664\n",
      "epoch: 9 iter: 216 loss: 0.07076 training acc: 0.97952 testing acc: 0.9658\n",
      "epoch: 9 iter: 217 loss: 0.06825 training acc: 0.98024 testing acc: 0.9666\n",
      "epoch: 9 iter: 218 loss: 0.06489 training acc: 0.98178 testing acc: 0.9706\n",
      "epoch: 9 iter: 219 loss: 0.06470 training acc: 0.98188 testing acc: 0.9708\n",
      "epoch: 9 iter: 220 loss: 0.06499 training acc: 0.98192 testing acc: 0.9716\n",
      "epoch: 9 iter: 221 loss: 0.06642 training acc: 0.98126 testing acc: 0.9708\n",
      "epoch: 9 iter: 222 loss: 0.07525 training acc: 0.97750 testing acc: 0.968\n",
      "epoch: 9 iter: 223 loss: 0.06864 training acc: 0.97988 testing acc: 0.9676\n",
      "epoch: 9 iter: 224 loss: 0.06957 training acc: 0.97974 testing acc: 0.9676\n",
      "epoch: 9 iter: 225 loss: 0.07590 training acc: 0.97700 testing acc: 0.9666\n",
      "epoch: 9 iter: 226 loss: 0.07642 training acc: 0.97672 testing acc: 0.9658\n",
      "epoch: 9 iter: 227 loss: 0.07413 training acc: 0.97754 testing acc: 0.9672\n",
      "epoch: 9 iter: 228 loss: 0.07055 training acc: 0.97906 testing acc: 0.9676\n",
      "epoch: 9 iter: 229 loss: 0.06873 training acc: 0.97994 testing acc: 0.9672\n",
      "epoch: 9 iter: 230 loss: 0.06525 training acc: 0.98100 testing acc: 0.9692\n",
      "epoch: 9 iter: 231 loss: 0.08531 training acc: 0.97374 testing acc: 0.964\n",
      "epoch: 9 iter: 232 loss: 0.08753 training acc: 0.97282 testing acc: 0.9626\n",
      "epoch: 9 iter: 233 loss: 0.08322 training acc: 0.97472 testing acc: 0.964\n",
      "epoch: 9 iter: 234 loss: 0.08113 training acc: 0.97530 testing acc: 0.9638\n",
      "epoch: 9 iter: 235 loss: 0.07610 training acc: 0.97742 testing acc: 0.9668\n",
      "epoch: 9 iter: 236 loss: 0.06775 training acc: 0.98054 testing acc: 0.9706\n",
      "epoch: 9 iter: 237 loss: 0.06688 training acc: 0.98094 testing acc: 0.9706\n",
      "epoch: 9 iter: 238 loss: 0.06510 training acc: 0.98102 testing acc: 0.9692\n",
      "epoch: 9 iter: 239 loss: 0.06644 training acc: 0.98044 testing acc: 0.969\n",
      "epoch: 9 iter: 240 loss: 0.06527 training acc: 0.98072 testing acc: 0.9698\n",
      "epoch: 9 iter: 241 loss: 0.06627 training acc: 0.98028 testing acc: 0.9698\n",
      "epoch: 9 iter: 242 loss: 0.07112 training acc: 0.97966 testing acc: 0.9684\n",
      "epoch: 9 iter: 243 loss: 0.06765 training acc: 0.98088 testing acc: 0.97\n",
      "epoch: 9 iter: 244 loss: 0.06597 training acc: 0.98104 testing acc: 0.971\n",
      "epoch: 9 iter: 245 loss: 0.07545 training acc: 0.97804 testing acc: 0.9664\n",
      "epoch: 9 iter: 246 loss: 0.06658 training acc: 0.98112 testing acc: 0.969\n",
      "epoch: 9 iter: 247 loss: 0.06398 training acc: 0.98224 testing acc: 0.969\n",
      "epoch: 9 iter: 248 loss: 0.06457 training acc: 0.98200 testing acc: 0.97\n",
      "epoch: 9 iter: 249 loss: 0.06460 training acc: 0.98196 testing acc: 0.9698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 iter: 250 loss: 0.06308 training acc: 0.98230 testing acc: 0.9696\n",
      "epoch: 9 iter: 251 loss: 0.06469 training acc: 0.98156 testing acc: 0.968\n",
      "epoch: 9 iter: 252 loss: 0.06286 training acc: 0.98226 testing acc: 0.9678\n",
      "epoch: 9 iter: 253 loss: 0.06196 training acc: 0.98252 testing acc: 0.969\n",
      "epoch: 9 iter: 254 loss: 0.06077 training acc: 0.98304 testing acc: 0.9694\n",
      "epoch: 9 iter: 255 loss: 0.06700 training acc: 0.98060 testing acc: 0.9668\n",
      "epoch: 9 iter: 256 loss: 0.06403 training acc: 0.98144 testing acc: 0.9676\n",
      "epoch: 9 iter: 257 loss: 0.06338 training acc: 0.98176 testing acc: 0.9686\n",
      "epoch: 9 iter: 258 loss: 0.06464 training acc: 0.98184 testing acc: 0.9678\n",
      "epoch: 9 iter: 259 loss: 0.06329 training acc: 0.98210 testing acc: 0.968\n",
      "epoch: 9 iter: 260 loss: 0.06345 training acc: 0.98206 testing acc: 0.9682\n",
      "epoch: 9 iter: 261 loss: 0.06684 training acc: 0.98060 testing acc: 0.9664\n",
      "epoch: 9 iter: 262 loss: 0.06435 training acc: 0.98198 testing acc: 0.9678\n",
      "epoch: 9 iter: 263 loss: 0.06408 training acc: 0.98194 testing acc: 0.9688\n",
      "epoch: 9 iter: 264 loss: 0.06374 training acc: 0.98182 testing acc: 0.9678\n",
      "epoch: 9 iter: 265 loss: 0.06294 training acc: 0.98270 testing acc: 0.9682\n",
      "epoch: 9 iter: 266 loss: 0.06799 training acc: 0.98042 testing acc: 0.9664\n",
      "epoch: 9 iter: 267 loss: 0.06922 training acc: 0.97960 testing acc: 0.9662\n",
      "epoch: 9 iter: 268 loss: 0.07281 training acc: 0.97882 testing acc: 0.967\n",
      "epoch: 9 iter: 269 loss: 0.07592 training acc: 0.97788 testing acc: 0.9654\n",
      "epoch: 9 iter: 270 loss: 0.07748 training acc: 0.97730 testing acc: 0.9674\n",
      "epoch: 9 iter: 271 loss: 0.07869 training acc: 0.97650 testing acc: 0.9632\n",
      "epoch: 9 iter: 272 loss: 0.07744 training acc: 0.97734 testing acc: 0.9656\n",
      "epoch: 9 iter: 273 loss: 0.07259 training acc: 0.97898 testing acc: 0.9668\n",
      "epoch: 9 iter: 274 loss: 0.07856 training acc: 0.97696 testing acc: 0.9638\n",
      "epoch: 9 iter: 275 loss: 0.07119 training acc: 0.97934 testing acc: 0.9668\n",
      "epoch: 9 iter: 276 loss: 0.07081 training acc: 0.97922 testing acc: 0.9678\n",
      "epoch: 9 iter: 277 loss: 0.07495 training acc: 0.97842 testing acc: 0.9642\n",
      "epoch: 9 iter: 278 loss: 0.07322 training acc: 0.97942 testing acc: 0.965\n",
      "epoch: 9 iter: 279 loss: 0.07425 training acc: 0.97854 testing acc: 0.9644\n",
      "epoch: 9 iter: 280 loss: 0.06822 training acc: 0.98074 testing acc: 0.969\n",
      "epoch: 9 iter: 281 loss: 0.06628 training acc: 0.98148 testing acc: 0.969\n",
      "epoch: 9 iter: 282 loss: 0.06640 training acc: 0.98132 testing acc: 0.969\n",
      "epoch: 9 iter: 283 loss: 0.06802 training acc: 0.98036 testing acc: 0.968\n",
      "epoch: 9 iter: 284 loss: 0.07783 training acc: 0.97618 testing acc: 0.9644\n",
      "epoch: 9 iter: 285 loss: 0.08266 training acc: 0.97430 testing acc: 0.9648\n",
      "epoch: 9 iter: 286 loss: 0.08182 training acc: 0.97434 testing acc: 0.9638\n",
      "epoch: 9 iter: 287 loss: 0.08096 training acc: 0.97498 testing acc: 0.964\n",
      "epoch: 9 iter: 288 loss: 0.07617 training acc: 0.97696 testing acc: 0.9642\n",
      "epoch: 9 iter: 289 loss: 0.07454 training acc: 0.97732 testing acc: 0.9652\n",
      "epoch: 9 iter: 290 loss: 0.06745 training acc: 0.98064 testing acc: 0.9686\n",
      "epoch: 9 iter: 291 loss: 0.07104 training acc: 0.97844 testing acc: 0.9662\n",
      "epoch: 9 iter: 292 loss: 0.06804 training acc: 0.97984 testing acc: 0.967\n",
      "epoch: 9 iter: 293 loss: 0.06775 training acc: 0.98008 testing acc: 0.9678\n",
      "epoch: 9 iter: 294 loss: 0.06726 training acc: 0.98054 testing acc: 0.968\n",
      "epoch: 9 iter: 295 loss: 0.06886 training acc: 0.98018 testing acc: 0.968\n",
      "epoch: 9 iter: 296 loss: 0.06851 training acc: 0.97964 testing acc: 0.9686\n",
      "epoch: 9 iter: 297 loss: 0.06579 training acc: 0.98096 testing acc: 0.9684\n",
      "epoch: 9 iter: 298 loss: 0.06741 training acc: 0.98082 testing acc: 0.9672\n",
      "epoch: 9 iter: 299 loss: 0.06463 training acc: 0.98148 testing acc: 0.9694\n",
      "epoch: 9 iter: 300 loss: 0.06350 training acc: 0.98206 testing acc: 0.9696\n",
      "epoch: 9 iter: 301 loss: 0.06582 training acc: 0.98080 testing acc: 0.969\n",
      "epoch: 9 iter: 302 loss: 0.06377 training acc: 0.98170 testing acc: 0.9694\n",
      "epoch: 9 iter: 303 loss: 0.06390 training acc: 0.98164 testing acc: 0.9682\n",
      "epoch: 9 iter: 304 loss: 0.06248 training acc: 0.98194 testing acc: 0.9688\n",
      "epoch: 9 iter: 305 loss: 0.07524 training acc: 0.97750 testing acc: 0.9658\n",
      "epoch: 9 iter: 306 loss: 0.07077 training acc: 0.97890 testing acc: 0.9682\n",
      "epoch: 9 iter: 307 loss: 0.06732 training acc: 0.97998 testing acc: 0.969\n",
      "epoch: 9 iter: 308 loss: 0.07832 training acc: 0.97650 testing acc: 0.9644\n",
      "epoch: 9 iter: 309 loss: 0.07922 training acc: 0.97640 testing acc: 0.9644\n",
      "epoch: 9 iter: 310 loss: 0.09859 training acc: 0.97062 testing acc: 0.9588\n",
      "epoch: 9 iter: 311 loss: 0.10443 training acc: 0.96878 testing acc: 0.957\n",
      "epoch: 9 iter: 312 loss: 0.06644 training acc: 0.98046 testing acc: 0.9702\n",
      "epoch: 9 iter: 313 loss: 0.07009 training acc: 0.97892 testing acc: 0.9694\n",
      "epoch: 9 iter: 314 loss: 0.07088 training acc: 0.97908 testing acc: 0.968\n",
      "epoch: 9 iter: 315 loss: 0.06781 training acc: 0.98020 testing acc: 0.9686\n",
      "epoch: 9 iter: 316 loss: 0.06679 training acc: 0.98076 testing acc: 0.9686\n",
      "epoch: 9 iter: 317 loss: 0.06601 training acc: 0.98082 testing acc: 0.9692\n",
      "epoch: 9 iter: 318 loss: 0.07219 training acc: 0.97842 testing acc: 0.9654\n",
      "epoch: 9 iter: 319 loss: 0.07290 training acc: 0.97826 testing acc: 0.966\n",
      "epoch: 9 iter: 320 loss: 0.06871 training acc: 0.98016 testing acc: 0.9684\n",
      "epoch: 9 iter: 321 loss: 0.06470 training acc: 0.98134 testing acc: 0.9686\n",
      "epoch: 9 iter: 322 loss: 0.06435 training acc: 0.98154 testing acc: 0.9694\n",
      "epoch: 9 iter: 323 loss: 0.06584 training acc: 0.98038 testing acc: 0.9682\n",
      "epoch: 9 iter: 324 loss: 0.06458 training acc: 0.98096 testing acc: 0.9682\n",
      "epoch: 9 iter: 325 loss: 0.06445 training acc: 0.98096 testing acc: 0.9684\n",
      "epoch: 9 iter: 326 loss: 0.06352 training acc: 0.98154 testing acc: 0.9688\n",
      "epoch: 9 iter: 327 loss: 0.06379 training acc: 0.98140 testing acc: 0.9692\n",
      "epoch: 9 iter: 328 loss: 0.06748 training acc: 0.97996 testing acc: 0.9674\n",
      "epoch: 9 iter: 329 loss: 0.06895 training acc: 0.97878 testing acc: 0.9698\n",
      "epoch: 9 iter: 330 loss: 0.06751 training acc: 0.97958 testing acc: 0.9704\n",
      "epoch: 9 iter: 331 loss: 0.07207 training acc: 0.97790 testing acc: 0.9646\n",
      "epoch: 9 iter: 332 loss: 0.07034 training acc: 0.97884 testing acc: 0.9656\n",
      "epoch: 9 iter: 333 loss: 0.06914 training acc: 0.97884 testing acc: 0.9664\n",
      "epoch: 9 iter: 334 loss: 0.06779 training acc: 0.98016 testing acc: 0.9688\n",
      "epoch: 9 iter: 335 loss: 0.06421 training acc: 0.98206 testing acc: 0.9692\n",
      "epoch: 9 iter: 336 loss: 0.06347 training acc: 0.98214 testing acc: 0.97\n",
      "epoch: 9 iter: 337 loss: 0.06444 training acc: 0.98200 testing acc: 0.9688\n",
      "epoch: 9 iter: 338 loss: 0.06450 training acc: 0.98164 testing acc: 0.969\n",
      "epoch: 9 iter: 339 loss: 0.06857 training acc: 0.98006 testing acc: 0.97\n",
      "epoch: 9 iter: 340 loss: 0.06544 training acc: 0.98140 testing acc: 0.97\n",
      "epoch: 9 iter: 341 loss: 0.06964 training acc: 0.98004 testing acc: 0.9678\n",
      "epoch: 9 iter: 342 loss: 0.07388 training acc: 0.97864 testing acc: 0.967\n",
      "epoch: 9 iter: 343 loss: 0.06669 training acc: 0.98126 testing acc: 0.9684\n",
      "epoch: 9 iter: 344 loss: 0.06951 training acc: 0.98016 testing acc: 0.967\n",
      "epoch: 9 iter: 345 loss: 0.06730 training acc: 0.98018 testing acc: 0.9676\n",
      "epoch: 9 iter: 346 loss: 0.07109 training acc: 0.97886 testing acc: 0.9666\n",
      "epoch: 9 iter: 347 loss: 0.07120 training acc: 0.97838 testing acc: 0.9674\n",
      "epoch: 9 iter: 348 loss: 0.07898 training acc: 0.97592 testing acc: 0.9642\n",
      "epoch: 9 iter: 349 loss: 0.09357 training acc: 0.97148 testing acc: 0.9602\n",
      "epoch: 9 iter: 350 loss: 0.08077 training acc: 0.97562 testing acc: 0.9646\n",
      "epoch: 9 iter: 351 loss: 0.08447 training acc: 0.97480 testing acc: 0.964\n",
      "epoch: 9 iter: 352 loss: 0.07730 training acc: 0.97702 testing acc: 0.9664\n",
      "epoch: 9 iter: 353 loss: 0.08950 training acc: 0.97344 testing acc: 0.9608\n",
      "epoch: 9 iter: 354 loss: 0.08499 training acc: 0.97442 testing acc: 0.965\n",
      "epoch: 9 iter: 355 loss: 0.08340 training acc: 0.97496 testing acc: 0.965\n",
      "epoch: 9 iter: 356 loss: 0.08154 training acc: 0.97580 testing acc: 0.9654\n",
      "epoch: 9 iter: 357 loss: 0.07734 training acc: 0.97754 testing acc: 0.9676\n",
      "epoch: 9 iter: 358 loss: 0.07068 training acc: 0.97940 testing acc: 0.9694\n",
      "epoch: 9 iter: 359 loss: 0.06856 training acc: 0.98040 testing acc: 0.9708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 iter: 360 loss: 0.06611 training acc: 0.98118 testing acc: 0.9708\n",
      "epoch: 9 iter: 361 loss: 0.06499 training acc: 0.98170 testing acc: 0.9704\n",
      "epoch: 9 iter: 362 loss: 0.06816 training acc: 0.98082 testing acc: 0.9702\n",
      "epoch: 9 iter: 363 loss: 0.06666 training acc: 0.98138 testing acc: 0.9706\n",
      "epoch: 9 iter: 364 loss: 0.06404 training acc: 0.98216 testing acc: 0.9706\n",
      "epoch: 9 iter: 365 loss: 0.07009 training acc: 0.97998 testing acc: 0.9696\n",
      "epoch: 9 iter: 366 loss: 0.07049 training acc: 0.98004 testing acc: 0.9694\n",
      "epoch: 9 iter: 367 loss: 0.07009 training acc: 0.98016 testing acc: 0.9698\n",
      "epoch: 9 iter: 368 loss: 0.07088 training acc: 0.98014 testing acc: 0.9698\n",
      "epoch: 9 iter: 369 loss: 0.06794 training acc: 0.98070 testing acc: 0.9704\n",
      "epoch: 9 iter: 370 loss: 0.07634 training acc: 0.97856 testing acc: 0.9684\n",
      "epoch: 9 iter: 371 loss: 0.09293 training acc: 0.97074 testing acc: 0.961\n",
      "epoch: 9 iter: 372 loss: 0.08064 training acc: 0.97612 testing acc: 0.9656\n",
      "epoch: 9 iter: 373 loss: 0.07654 training acc: 0.97718 testing acc: 0.9666\n",
      "epoch: 9 iter: 374 loss: 0.07418 training acc: 0.97814 testing acc: 0.9668\n",
      "epoch: 9 iter: 375 loss: 0.07313 training acc: 0.97832 testing acc: 0.9656\n",
      "epoch: 9 iter: 376 loss: 0.07148 training acc: 0.97918 testing acc: 0.9664\n",
      "epoch: 9 iter: 377 loss: 0.07106 training acc: 0.97898 testing acc: 0.9668\n",
      "epoch: 9 iter: 378 loss: 0.07011 training acc: 0.97960 testing acc: 0.9664\n",
      "epoch: 9 iter: 379 loss: 0.06745 training acc: 0.98052 testing acc: 0.9676\n",
      "epoch: 9 iter: 380 loss: 0.06837 training acc: 0.97996 testing acc: 0.966\n",
      "epoch: 9 iter: 381 loss: 0.06645 training acc: 0.98034 testing acc: 0.9696\n",
      "epoch: 9 iter: 382 loss: 0.06615 training acc: 0.98088 testing acc: 0.9684\n",
      "epoch: 9 iter: 383 loss: 0.07222 training acc: 0.97900 testing acc: 0.9662\n",
      "epoch: 9 iter: 384 loss: 0.06895 training acc: 0.97970 testing acc: 0.9692\n",
      "epoch: 9 iter: 385 loss: 0.06974 training acc: 0.97980 testing acc: 0.9678\n",
      "epoch: 9 iter: 386 loss: 0.06527 training acc: 0.98062 testing acc: 0.9702\n",
      "epoch: 9 iter: 387 loss: 0.06474 training acc: 0.98128 testing acc: 0.9714\n",
      "epoch: 9 iter: 388 loss: 0.06481 training acc: 0.98120 testing acc: 0.9712\n",
      "epoch: 9 iter: 389 loss: 0.06460 training acc: 0.98104 testing acc: 0.971\n",
      "epoch: 9 iter: 390 loss: 0.06471 training acc: 0.98152 testing acc: 0.9712\n",
      "epoch: 9 iter: 391 loss: 0.06485 training acc: 0.98170 testing acc: 0.9686\n",
      "epoch: 9 iter: 392 loss: 0.06792 training acc: 0.98004 testing acc: 0.9702\n",
      "epoch: 9 iter: 393 loss: 0.06596 training acc: 0.98080 testing acc: 0.97\n",
      "epoch: 9 iter: 394 loss: 0.07148 training acc: 0.97890 testing acc: 0.9682\n",
      "epoch: 9 iter: 395 loss: 0.06770 training acc: 0.98010 testing acc: 0.9688\n",
      "epoch: 9 iter: 396 loss: 0.06882 training acc: 0.98046 testing acc: 0.9688\n",
      "epoch: 9 iter: 397 loss: 0.06901 training acc: 0.98024 testing acc: 0.9694\n",
      "epoch: 9 iter: 398 loss: 0.06402 training acc: 0.98176 testing acc: 0.9716\n",
      "epoch: 9 iter: 399 loss: 0.07887 training acc: 0.97582 testing acc: 0.9614\n",
      "epoch: 9 iter: 400 loss: 0.07713 training acc: 0.97652 testing acc: 0.9618\n",
      "epoch: 9 iter: 401 loss: 0.06462 training acc: 0.98112 testing acc: 0.9674\n",
      "epoch: 9 iter: 402 loss: 0.06302 training acc: 0.98170 testing acc: 0.97\n",
      "epoch: 9 iter: 403 loss: 0.06313 training acc: 0.98166 testing acc: 0.9698\n",
      "epoch: 9 iter: 404 loss: 0.06231 training acc: 0.98210 testing acc: 0.9696\n",
      "epoch: 9 iter: 405 loss: 0.06368 training acc: 0.98166 testing acc: 0.9694\n",
      "epoch: 9 iter: 406 loss: 0.06421 training acc: 0.98142 testing acc: 0.9692\n",
      "epoch: 9 iter: 407 loss: 0.07640 training acc: 0.97598 testing acc: 0.9652\n",
      "epoch: 9 iter: 408 loss: 0.08083 training acc: 0.97460 testing acc: 0.9646\n",
      "epoch: 9 iter: 409 loss: 0.07935 training acc: 0.97484 testing acc: 0.9644\n",
      "epoch: 9 iter: 410 loss: 0.07542 training acc: 0.97678 testing acc: 0.966\n",
      "epoch: 9 iter: 411 loss: 0.07894 training acc: 0.97588 testing acc: 0.964\n",
      "epoch: 9 iter: 412 loss: 0.06240 training acc: 0.98178 testing acc: 0.9708\n",
      "epoch: 9 iter: 413 loss: 0.07289 training acc: 0.97814 testing acc: 0.9652\n",
      "epoch: 9 iter: 414 loss: 0.07732 training acc: 0.97620 testing acc: 0.9664\n",
      "epoch: 9 iter: 415 loss: 0.07024 training acc: 0.97904 testing acc: 0.97\n",
      "epoch: 9 iter: 416 loss: 0.06515 training acc: 0.98066 testing acc: 0.9712\n",
      "epoch: 9 iter: 417 loss: 0.06597 training acc: 0.98036 testing acc: 0.9696\n",
      "epoch: 9 iter: 418 loss: 0.06794 training acc: 0.98020 testing acc: 0.9692\n",
      "epoch: 9 iter: 419 loss: 0.06848 training acc: 0.98016 testing acc: 0.9686\n",
      "epoch: 9 iter: 420 loss: 0.06472 training acc: 0.98146 testing acc: 0.968\n",
      "epoch: 9 iter: 421 loss: 0.06460 training acc: 0.98180 testing acc: 0.9688\n",
      "epoch: 9 iter: 422 loss: 0.06626 training acc: 0.98072 testing acc: 0.9688\n",
      "epoch: 9 iter: 423 loss: 0.06728 training acc: 0.98022 testing acc: 0.967\n",
      "epoch: 9 iter: 424 loss: 0.07068 training acc: 0.97950 testing acc: 0.9676\n",
      "epoch: 9 iter: 425 loss: 0.06742 training acc: 0.98100 testing acc: 0.9684\n",
      "epoch: 9 iter: 426 loss: 0.06475 training acc: 0.98124 testing acc: 0.969\n",
      "epoch: 9 iter: 427 loss: 0.06318 training acc: 0.98178 testing acc: 0.9696\n",
      "epoch: 9 iter: 428 loss: 0.06570 training acc: 0.98092 testing acc: 0.9696\n",
      "epoch: 9 iter: 429 loss: 0.06598 training acc: 0.98074 testing acc: 0.9692\n",
      "epoch: 9 iter: 430 loss: 0.06291 training acc: 0.98190 testing acc: 0.9688\n",
      "epoch: 9 iter: 431 loss: 0.06208 training acc: 0.98210 testing acc: 0.9702\n",
      "epoch: 9 iter: 432 loss: 0.06770 training acc: 0.98040 testing acc: 0.9682\n",
      "epoch: 9 iter: 433 loss: 0.06529 training acc: 0.98078 testing acc: 0.9668\n",
      "epoch: 9 iter: 434 loss: 0.06415 training acc: 0.98106 testing acc: 0.9686\n",
      "epoch: 9 iter: 435 loss: 0.06329 training acc: 0.98154 testing acc: 0.9698\n",
      "epoch: 9 iter: 436 loss: 0.06059 training acc: 0.98262 testing acc: 0.9702\n",
      "epoch: 9 iter: 437 loss: 0.06096 training acc: 0.98262 testing acc: 0.9704\n",
      "epoch: 9 iter: 438 loss: 0.06450 training acc: 0.98090 testing acc: 0.9684\n",
      "epoch: 9 iter: 439 loss: 0.06206 training acc: 0.98226 testing acc: 0.9706\n",
      "epoch: 9 iter: 440 loss: 0.06046 training acc: 0.98310 testing acc: 0.971\n",
      "epoch: 9 iter: 441 loss: 0.05990 training acc: 0.98316 testing acc: 0.9716\n",
      "epoch: 9 iter: 442 loss: 0.05915 training acc: 0.98346 testing acc: 0.9726\n",
      "epoch: 9 iter: 443 loss: 0.05957 training acc: 0.98308 testing acc: 0.9724\n",
      "epoch: 9 iter: 444 loss: 0.06377 training acc: 0.98228 testing acc: 0.9716\n",
      "epoch: 9 iter: 445 loss: 0.06808 training acc: 0.98006 testing acc: 0.968\n",
      "epoch: 9 iter: 446 loss: 0.06336 training acc: 0.98180 testing acc: 0.9686\n",
      "epoch: 9 iter: 447 loss: 0.06922 training acc: 0.97892 testing acc: 0.9656\n",
      "epoch: 9 iter: 448 loss: 0.06836 training acc: 0.97942 testing acc: 0.965\n",
      "epoch: 9 iter: 449 loss: 0.06362 training acc: 0.98138 testing acc: 0.9686\n",
      "epoch: 9 iter: 450 loss: 0.06018 training acc: 0.98374 testing acc: 0.971\n",
      "epoch: 9 iter: 451 loss: 0.06108 training acc: 0.98352 testing acc: 0.9714\n",
      "epoch: 9 iter: 452 loss: 0.06136 training acc: 0.98298 testing acc: 0.9714\n",
      "epoch: 9 iter: 453 loss: 0.06045 training acc: 0.98332 testing acc: 0.9708\n",
      "epoch: 9 iter: 454 loss: 0.06128 training acc: 0.98306 testing acc: 0.9712\n",
      "epoch: 9 iter: 455 loss: 0.06871 training acc: 0.97998 testing acc: 0.9664\n",
      "epoch: 9 iter: 456 loss: 0.06861 training acc: 0.98004 testing acc: 0.9664\n",
      "epoch: 9 iter: 457 loss: 0.06390 training acc: 0.98166 testing acc: 0.9672\n",
      "epoch: 9 iter: 458 loss: 0.06106 training acc: 0.98306 testing acc: 0.9696\n",
      "epoch: 9 iter: 459 loss: 0.06178 training acc: 0.98214 testing acc: 0.9686\n",
      "epoch: 9 iter: 460 loss: 0.06507 training acc: 0.98102 testing acc: 0.9682\n",
      "epoch: 9 iter: 461 loss: 0.06485 training acc: 0.98134 testing acc: 0.9676\n",
      "epoch: 9 iter: 462 loss: 0.06454 training acc: 0.98154 testing acc: 0.9672\n",
      "epoch: 9 iter: 463 loss: 0.06678 training acc: 0.98014 testing acc: 0.9678\n",
      "epoch: 9 iter: 464 loss: 0.06163 training acc: 0.98268 testing acc: 0.9702\n",
      "epoch: 9 iter: 465 loss: 0.06052 training acc: 0.98304 testing acc: 0.9706\n",
      "epoch: 9 iter: 466 loss: 0.06072 training acc: 0.98288 testing acc: 0.9704\n",
      "epoch: 9 iter: 467 loss: 0.06012 training acc: 0.98284 testing acc: 0.9706\n",
      "epoch: 9 iter: 468 loss: 0.06566 training acc: 0.98116 testing acc: 0.9668\n",
      "epoch: 9 iter: 469 loss: 0.06348 training acc: 0.98196 testing acc: 0.9686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 iter: 470 loss: 0.06447 training acc: 0.98098 testing acc: 0.9684\n",
      "epoch: 9 iter: 471 loss: 0.06855 training acc: 0.98010 testing acc: 0.9668\n",
      "epoch: 9 iter: 472 loss: 0.06458 training acc: 0.98142 testing acc: 0.9686\n",
      "epoch: 9 iter: 473 loss: 0.06486 training acc: 0.98128 testing acc: 0.9682\n",
      "epoch: 9 iter: 474 loss: 0.07056 training acc: 0.97938 testing acc: 0.9662\n",
      "epoch: 9 iter: 475 loss: 0.06751 training acc: 0.98046 testing acc: 0.9684\n",
      "epoch: 9 iter: 476 loss: 0.07135 training acc: 0.97888 testing acc: 0.9664\n",
      "epoch: 9 iter: 477 loss: 0.07861 training acc: 0.97586 testing acc: 0.961\n",
      "epoch: 9 iter: 478 loss: 0.06751 training acc: 0.98018 testing acc: 0.9674\n",
      "epoch: 9 iter: 479 loss: 0.07255 training acc: 0.97848 testing acc: 0.9654\n",
      "epoch: 9 iter: 480 loss: 0.06978 training acc: 0.97910 testing acc: 0.9656\n",
      "epoch: 9 iter: 481 loss: 0.06258 training acc: 0.98156 testing acc: 0.969\n",
      "epoch: 9 iter: 482 loss: 0.06092 training acc: 0.98224 testing acc: 0.9708\n",
      "epoch: 9 iter: 483 loss: 0.06136 training acc: 0.98204 testing acc: 0.9702\n",
      "epoch: 9 iter: 484 loss: 0.05974 training acc: 0.98286 testing acc: 0.9704\n",
      "epoch: 9 iter: 485 loss: 0.05951 training acc: 0.98310 testing acc: 0.9694\n",
      "epoch: 9 iter: 486 loss: 0.05973 training acc: 0.98322 testing acc: 0.9702\n",
      "epoch: 9 iter: 487 loss: 0.05965 training acc: 0.98310 testing acc: 0.9706\n",
      "epoch: 9 iter: 488 loss: 0.05981 training acc: 0.98318 testing acc: 0.9706\n",
      "epoch: 9 iter: 489 loss: 0.06418 training acc: 0.98168 testing acc: 0.9676\n",
      "epoch: 9 iter: 490 loss: 0.06313 training acc: 0.98192 testing acc: 0.9684\n",
      "epoch: 9 iter: 491 loss: 0.06508 training acc: 0.98092 testing acc: 0.968\n",
      "epoch: 9 iter: 492 loss: 0.06486 training acc: 0.98094 testing acc: 0.968\n",
      "epoch: 9 iter: 493 loss: 0.06628 training acc: 0.98036 testing acc: 0.967\n",
      "epoch: 9 iter: 494 loss: 0.06140 training acc: 0.98190 testing acc: 0.9692\n",
      "epoch: 9 iter: 495 loss: 0.07436 training acc: 0.97724 testing acc: 0.9646\n",
      "epoch: 9 iter: 496 loss: 0.07325 training acc: 0.97792 testing acc: 0.9648\n",
      "epoch: 9 iter: 497 loss: 0.06757 training acc: 0.98004 testing acc: 0.9656\n",
      "epoch: 9 iter: 498 loss: 0.06935 training acc: 0.97936 testing acc: 0.9664\n",
      "epoch: 9 iter: 499 loss: 0.06391 training acc: 0.98184 testing acc: 0.9684\n",
      "epoch: 9 iter: 500 loss: 0.06950 training acc: 0.97916 testing acc: 0.967\n",
      "epoch: 9 iter: 501 loss: 0.06507 training acc: 0.98064 testing acc: 0.9676\n",
      "epoch: 9 iter: 502 loss: 0.06144 training acc: 0.98236 testing acc: 0.9702\n",
      "epoch: 9 iter: 503 loss: 0.06342 training acc: 0.98094 testing acc: 0.9686\n",
      "epoch: 9 iter: 504 loss: 0.06568 training acc: 0.98042 testing acc: 0.966\n",
      "epoch: 9 iter: 505 loss: 0.06438 training acc: 0.98086 testing acc: 0.9668\n",
      "epoch: 9 iter: 506 loss: 0.06526 training acc: 0.98048 testing acc: 0.9676\n",
      "epoch: 9 iter: 507 loss: 0.06328 training acc: 0.98126 testing acc: 0.9678\n",
      "epoch: 9 iter: 508 loss: 0.06747 training acc: 0.98064 testing acc: 0.9688\n",
      "epoch: 9 iter: 509 loss: 0.06544 training acc: 0.98078 testing acc: 0.9708\n",
      "epoch: 9 iter: 510 loss: 0.09041 training acc: 0.97312 testing acc: 0.9616\n",
      "epoch: 9 iter: 511 loss: 0.08210 training acc: 0.97606 testing acc: 0.9648\n",
      "epoch: 9 iter: 512 loss: 0.06859 training acc: 0.98008 testing acc: 0.9672\n",
      "epoch: 9 iter: 513 loss: 0.06880 training acc: 0.97964 testing acc: 0.9676\n",
      "epoch: 9 iter: 514 loss: 0.06813 training acc: 0.98044 testing acc: 0.97\n",
      "epoch: 9 iter: 515 loss: 0.06321 training acc: 0.98146 testing acc: 0.9698\n",
      "epoch: 9 iter: 516 loss: 0.06040 training acc: 0.98262 testing acc: 0.9706\n",
      "epoch: 9 iter: 517 loss: 0.06072 training acc: 0.98230 testing acc: 0.9714\n",
      "epoch: 9 iter: 518 loss: 0.06171 training acc: 0.98222 testing acc: 0.9714\n",
      "epoch: 9 iter: 519 loss: 0.06247 training acc: 0.98202 testing acc: 0.9696\n",
      "epoch: 9 iter: 520 loss: 0.06536 training acc: 0.98036 testing acc: 0.967\n",
      "epoch: 9 iter: 521 loss: 0.06530 training acc: 0.98046 testing acc: 0.9668\n",
      "epoch: 9 iter: 522 loss: 0.05898 training acc: 0.98264 testing acc: 0.971\n",
      "epoch: 9 iter: 523 loss: 0.05938 training acc: 0.98268 testing acc: 0.9708\n",
      "epoch: 9 iter: 524 loss: 0.05903 training acc: 0.98264 testing acc: 0.971\n",
      "epoch: 9 iter: 525 loss: 0.06482 training acc: 0.98034 testing acc: 0.967\n",
      "epoch: 9 iter: 526 loss: 0.06419 training acc: 0.98110 testing acc: 0.971\n",
      "epoch: 9 iter: 527 loss: 0.06389 training acc: 0.98130 testing acc: 0.9712\n",
      "epoch: 9 iter: 528 loss: 0.06373 training acc: 0.98146 testing acc: 0.966\n",
      "epoch: 9 iter: 529 loss: 0.06355 training acc: 0.98118 testing acc: 0.9654\n",
      "epoch: 9 iter: 530 loss: 0.06619 training acc: 0.97972 testing acc: 0.9642\n",
      "epoch: 9 iter: 531 loss: 0.07007 training acc: 0.97838 testing acc: 0.9634\n",
      "epoch: 9 iter: 532 loss: 0.06211 training acc: 0.98164 testing acc: 0.969\n",
      "epoch: 9 iter: 533 loss: 0.06458 training acc: 0.98098 testing acc: 0.9686\n",
      "epoch: 9 iter: 534 loss: 0.06494 training acc: 0.98088 testing acc: 0.9682\n",
      "epoch: 9 iter: 535 loss: 0.06288 training acc: 0.98146 testing acc: 0.9694\n",
      "epoch: 9 iter: 536 loss: 0.05864 training acc: 0.98302 testing acc: 0.9694\n",
      "epoch: 9 iter: 537 loss: 0.05870 training acc: 0.98266 testing acc: 0.9696\n",
      "epoch: 9 iter: 538 loss: 0.05835 training acc: 0.98318 testing acc: 0.971\n",
      "epoch: 9 iter: 539 loss: 0.05918 training acc: 0.98306 testing acc: 0.9708\n",
      "epoch: 9 iter: 540 loss: 0.05815 training acc: 0.98306 testing acc: 0.9694\n",
      "epoch: 9 iter: 541 loss: 0.05779 training acc: 0.98334 testing acc: 0.97\n",
      "epoch: 9 iter: 542 loss: 0.05888 training acc: 0.98240 testing acc: 0.9686\n",
      "epoch: 9 iter: 543 loss: 0.05890 training acc: 0.98280 testing acc: 0.9678\n",
      "epoch: 9 iter: 544 loss: 0.05712 training acc: 0.98336 testing acc: 0.9716\n",
      "epoch: 9 iter: 545 loss: 0.07354 training acc: 0.97832 testing acc: 0.9624\n",
      "epoch: 9 iter: 546 loss: 0.06913 training acc: 0.97944 testing acc: 0.9656\n",
      "epoch: 9 iter: 547 loss: 0.05840 training acc: 0.98304 testing acc: 0.9702\n",
      "epoch: 9 iter: 548 loss: 0.05808 training acc: 0.98310 testing acc: 0.9716\n",
      "epoch: 9 iter: 549 loss: 0.07321 training acc: 0.97722 testing acc: 0.9644\n",
      "epoch: 9 iter: 550 loss: 0.06819 training acc: 0.97942 testing acc: 0.9678\n",
      "epoch: 9 iter: 551 loss: 0.07085 training acc: 0.97802 testing acc: 0.9652\n",
      "epoch: 9 iter: 552 loss: 0.07324 training acc: 0.97706 testing acc: 0.9638\n",
      "epoch: 9 iter: 553 loss: 0.07136 training acc: 0.97788 testing acc: 0.9642\n",
      "epoch: 9 iter: 554 loss: 0.06648 training acc: 0.98028 testing acc: 0.9684\n",
      "epoch: 9 iter: 555 loss: 0.07056 training acc: 0.97864 testing acc: 0.9666\n",
      "epoch: 9 iter: 556 loss: 0.06803 training acc: 0.97922 testing acc: 0.9672\n",
      "epoch: 9 iter: 557 loss: 0.06555 training acc: 0.98062 testing acc: 0.968\n",
      "epoch: 9 iter: 558 loss: 0.06546 training acc: 0.98066 testing acc: 0.9682\n",
      "epoch: 9 iter: 559 loss: 0.06511 training acc: 0.98088 testing acc: 0.9684\n",
      "epoch: 9 iter: 560 loss: 0.06304 training acc: 0.98188 testing acc: 0.97\n",
      "epoch: 9 iter: 561 loss: 0.07313 training acc: 0.97806 testing acc: 0.9666\n",
      "epoch: 9 iter: 562 loss: 0.07008 training acc: 0.97938 testing acc: 0.9672\n",
      "epoch: 9 iter: 563 loss: 0.06361 training acc: 0.98196 testing acc: 0.9714\n",
      "epoch: 9 iter: 564 loss: 0.06371 training acc: 0.98224 testing acc: 0.969\n",
      "epoch: 9 iter: 565 loss: 0.06728 training acc: 0.98086 testing acc: 0.9688\n",
      "epoch: 9 iter: 566 loss: 0.07476 training acc: 0.97766 testing acc: 0.966\n",
      "epoch: 9 iter: 567 loss: 0.07004 training acc: 0.97904 testing acc: 0.9668\n",
      "epoch: 9 iter: 568 loss: 0.06708 training acc: 0.98038 testing acc: 0.9672\n",
      "epoch: 9 iter: 569 loss: 0.06984 training acc: 0.97898 testing acc: 0.966\n",
      "epoch: 9 iter: 570 loss: 0.06830 training acc: 0.97972 testing acc: 0.967\n",
      "epoch: 9 iter: 571 loss: 0.06300 training acc: 0.98216 testing acc: 0.9706\n",
      "epoch: 9 iter: 572 loss: 0.06191 training acc: 0.98276 testing acc: 0.9704\n",
      "epoch: 9 iter: 573 loss: 0.06312 training acc: 0.98222 testing acc: 0.9694\n",
      "epoch: 9 iter: 574 loss: 0.06335 training acc: 0.98218 testing acc: 0.9686\n",
      "epoch: 9 iter: 575 loss: 0.06958 training acc: 0.97898 testing acc: 0.9654\n",
      "epoch: 9 iter: 576 loss: 0.06702 training acc: 0.97990 testing acc: 0.9684\n",
      "epoch: 9 iter: 577 loss: 0.06901 training acc: 0.97898 testing acc: 0.9656\n",
      "epoch: 9 iter: 578 loss: 0.07515 training acc: 0.97702 testing acc: 0.9644\n",
      "epoch: 9 iter: 579 loss: 0.06693 training acc: 0.98052 testing acc: 0.9674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 iter: 580 loss: 0.06653 training acc: 0.97996 testing acc: 0.968\n",
      "epoch: 9 iter: 581 loss: 0.06448 training acc: 0.98138 testing acc: 0.9686\n",
      "epoch: 9 iter: 582 loss: 0.06189 training acc: 0.98180 testing acc: 0.9696\n",
      "epoch: 9 iter: 583 loss: 0.06081 training acc: 0.98222 testing acc: 0.969\n",
      "epoch: 9 iter: 584 loss: 0.05948 training acc: 0.98288 testing acc: 0.9688\n",
      "epoch: 9 iter: 585 loss: 0.06872 training acc: 0.98006 testing acc: 0.966\n",
      "epoch: 9 iter: 586 loss: 0.06158 training acc: 0.98246 testing acc: 0.9694\n",
      "epoch: 9 iter: 587 loss: 0.06278 training acc: 0.98192 testing acc: 0.9688\n",
      "epoch: 9 iter: 588 loss: 0.06248 training acc: 0.98196 testing acc: 0.9684\n",
      "epoch: 9 iter: 589 loss: 0.06185 training acc: 0.98248 testing acc: 0.971\n",
      "epoch: 9 iter: 590 loss: 0.06429 training acc: 0.98136 testing acc: 0.9682\n",
      "epoch: 9 iter: 591 loss: 0.06509 training acc: 0.98156 testing acc: 0.9702\n",
      "epoch: 9 iter: 592 loss: 0.06787 training acc: 0.97970 testing acc: 0.9684\n",
      "epoch: 9 iter: 593 loss: 0.06674 training acc: 0.97988 testing acc: 0.9688\n",
      "epoch: 9 iter: 594 loss: 0.06029 training acc: 0.98308 testing acc: 0.9702\n",
      "epoch: 9 iter: 595 loss: 0.06029 training acc: 0.98326 testing acc: 0.9708\n",
      "epoch: 9 iter: 596 loss: 0.06511 training acc: 0.98136 testing acc: 0.9706\n",
      "epoch: 9 iter: 597 loss: 0.06602 training acc: 0.98032 testing acc: 0.9674\n",
      "epoch: 9 iter: 598 loss: 0.06106 training acc: 0.98238 testing acc: 0.9706\n",
      "epoch: 9 iter: 599 loss: 0.06046 training acc: 0.98276 testing acc: 0.9706\n",
      "epoch: 9 iter: 600 loss: 0.06059 training acc: 0.98242 testing acc: 0.9696\n",
      "epoch: 9 iter: 601 loss: 0.06690 training acc: 0.98036 testing acc: 0.9676\n",
      "epoch: 9 iter: 602 loss: 0.06302 training acc: 0.98152 testing acc: 0.9704\n",
      "epoch: 9 iter: 603 loss: 0.06705 training acc: 0.97996 testing acc: 0.967\n",
      "epoch: 9 iter: 604 loss: 0.06235 training acc: 0.98210 testing acc: 0.9696\n",
      "epoch: 9 iter: 605 loss: 0.06323 training acc: 0.98150 testing acc: 0.9698\n",
      "epoch: 9 iter: 606 loss: 0.06230 training acc: 0.98192 testing acc: 0.9706\n",
      "epoch: 9 iter: 607 loss: 0.06592 training acc: 0.98060 testing acc: 0.9708\n",
      "epoch: 9 iter: 608 loss: 0.06879 training acc: 0.97946 testing acc: 0.9694\n",
      "epoch: 9 iter: 609 loss: 0.06765 training acc: 0.98024 testing acc: 0.9662\n",
      "epoch: 9 iter: 610 loss: 0.06653 training acc: 0.98050 testing acc: 0.9674\n",
      "epoch: 9 iter: 611 loss: 0.06349 training acc: 0.98168 testing acc: 0.9684\n",
      "epoch: 9 iter: 612 loss: 0.06410 training acc: 0.98144 testing acc: 0.9688\n",
      "epoch: 9 iter: 613 loss: 0.07205 training acc: 0.97850 testing acc: 0.9658\n",
      "epoch: 9 iter: 614 loss: 0.07098 training acc: 0.97926 testing acc: 0.9654\n",
      "epoch: 9 iter: 615 loss: 0.06962 training acc: 0.97980 testing acc: 0.965\n",
      "epoch: 9 iter: 616 loss: 0.06778 training acc: 0.98018 testing acc: 0.9682\n",
      "epoch: 9 iter: 617 loss: 0.08632 training acc: 0.97398 testing acc: 0.9604\n",
      "epoch: 9 iter: 618 loss: 0.08318 training acc: 0.97538 testing acc: 0.961\n",
      "epoch: 9 iter: 619 loss: 0.07860 training acc: 0.97684 testing acc: 0.9628\n",
      "epoch: 9 iter: 620 loss: 0.06240 training acc: 0.98236 testing acc: 0.972\n",
      "epoch: 9 iter: 621 loss: 0.06384 training acc: 0.98190 testing acc: 0.9702\n",
      "epoch: 9 iter: 622 loss: 0.06344 training acc: 0.98192 testing acc: 0.9702\n",
      "epoch: 9 iter: 623 loss: 0.06440 training acc: 0.98212 testing acc: 0.9722\n",
      "epoch: 9 iter: 624 loss: 0.06887 training acc: 0.97956 testing acc: 0.9698\n",
      "epoch: 9 iter: 625 loss: 0.06652 training acc: 0.98000 testing acc: 0.9704\n",
      "epoch: 9 iter: 626 loss: 0.07797 training acc: 0.97588 testing acc: 0.9656\n",
      "epoch: 9 iter: 627 loss: 0.07812 training acc: 0.97586 testing acc: 0.9656\n",
      "epoch: 9 iter: 628 loss: 0.07639 training acc: 0.97680 testing acc: 0.9662\n",
      "epoch: 9 iter: 629 loss: 0.06769 training acc: 0.98026 testing acc: 0.969\n",
      "epoch: 9 iter: 630 loss: 0.08508 training acc: 0.97386 testing acc: 0.9652\n",
      "epoch: 9 iter: 631 loss: 0.07885 training acc: 0.97638 testing acc: 0.9672\n",
      "epoch: 9 iter: 632 loss: 0.07474 training acc: 0.97786 testing acc: 0.9696\n",
      "epoch: 9 iter: 633 loss: 0.06314 training acc: 0.98136 testing acc: 0.9716\n",
      "epoch: 9 iter: 634 loss: 0.06346 training acc: 0.98122 testing acc: 0.9712\n",
      "epoch: 9 iter: 635 loss: 0.06059 training acc: 0.98246 testing acc: 0.9722\n",
      "epoch: 9 iter: 636 loss: 0.06127 training acc: 0.98200 testing acc: 0.9692\n",
      "epoch: 9 iter: 637 loss: 0.06187 training acc: 0.98186 testing acc: 0.969\n",
      "epoch: 9 iter: 638 loss: 0.06226 training acc: 0.98196 testing acc: 0.9694\n",
      "epoch: 9 iter: 639 loss: 0.06171 training acc: 0.98194 testing acc: 0.9702\n",
      "epoch: 9 iter: 640 loss: 0.07090 training acc: 0.97762 testing acc: 0.9644\n",
      "epoch: 9 iter: 641 loss: 0.06367 training acc: 0.98094 testing acc: 0.9694\n",
      "epoch: 9 iter: 642 loss: 0.06367 training acc: 0.98076 testing acc: 0.9686\n",
      "epoch: 9 iter: 643 loss: 0.06016 training acc: 0.98236 testing acc: 0.969\n",
      "epoch: 9 iter: 644 loss: 0.05865 training acc: 0.98306 testing acc: 0.97\n",
      "epoch: 9 iter: 645 loss: 0.06148 training acc: 0.98186 testing acc: 0.9686\n",
      "epoch: 9 iter: 646 loss: 0.05952 training acc: 0.98274 testing acc: 0.9688\n",
      "epoch: 9 iter: 647 loss: 0.05995 training acc: 0.98230 testing acc: 0.9694\n",
      "epoch: 9 iter: 648 loss: 0.05982 training acc: 0.98260 testing acc: 0.9688\n",
      "epoch: 9 iter: 649 loss: 0.06045 training acc: 0.98224 testing acc: 0.9686\n",
      "epoch: 9 iter: 650 loss: 0.05939 training acc: 0.98264 testing acc: 0.969\n",
      "epoch: 9 iter: 651 loss: 0.05976 training acc: 0.98256 testing acc: 0.9718\n",
      "epoch: 9 iter: 652 loss: 0.06025 training acc: 0.98236 testing acc: 0.9716\n",
      "epoch: 9 iter: 653 loss: 0.05950 training acc: 0.98266 testing acc: 0.9722\n",
      "epoch: 9 iter: 654 loss: 0.06017 training acc: 0.98238 testing acc: 0.9722\n",
      "epoch: 9 iter: 655 loss: 0.05838 training acc: 0.98328 testing acc: 0.9712\n",
      "epoch: 9 iter: 656 loss: 0.05999 training acc: 0.98236 testing acc: 0.9686\n",
      "epoch: 9 iter: 657 loss: 0.06211 training acc: 0.98220 testing acc: 0.97\n",
      "epoch: 9 iter: 658 loss: 0.06106 training acc: 0.98272 testing acc: 0.9702\n",
      "epoch: 9 iter: 659 loss: 0.06075 training acc: 0.98266 testing acc: 0.97\n",
      "epoch: 9 iter: 660 loss: 0.06138 training acc: 0.98284 testing acc: 0.9712\n",
      "epoch: 9 iter: 661 loss: 0.06625 training acc: 0.98064 testing acc: 0.9682\n",
      "epoch: 9 iter: 662 loss: 0.05839 training acc: 0.98378 testing acc: 0.9716\n",
      "epoch: 9 iter: 663 loss: 0.06128 training acc: 0.98274 testing acc: 0.971\n",
      "epoch: 9 iter: 664 loss: 0.06188 training acc: 0.98268 testing acc: 0.97\n",
      "epoch: 9 iter: 665 loss: 0.06148 training acc: 0.98254 testing acc: 0.9712\n",
      "epoch: 9 iter: 666 loss: 0.06139 training acc: 0.98272 testing acc: 0.9708\n",
      "epoch: 9 iter: 667 loss: 0.05968 training acc: 0.98356 testing acc: 0.9712\n",
      "epoch: 9 iter: 668 loss: 0.05847 training acc: 0.98402 testing acc: 0.9712\n",
      "epoch: 9 iter: 669 loss: 0.06190 training acc: 0.98246 testing acc: 0.9698\n",
      "epoch: 9 iter: 670 loss: 0.06096 training acc: 0.98288 testing acc: 0.9702\n",
      "epoch: 9 iter: 671 loss: 0.05864 training acc: 0.98366 testing acc: 0.9694\n",
      "epoch: 9 iter: 672 loss: 0.05898 training acc: 0.98316 testing acc: 0.97\n",
      "epoch: 9 iter: 673 loss: 0.06781 training acc: 0.98016 testing acc: 0.9662\n",
      "epoch: 9 iter: 674 loss: 0.06512 training acc: 0.98108 testing acc: 0.9674\n",
      "epoch: 9 iter: 675 loss: 0.06406 training acc: 0.98148 testing acc: 0.968\n",
      "epoch: 9 iter: 676 loss: 0.06029 training acc: 0.98280 testing acc: 0.968\n",
      "epoch: 9 iter: 677 loss: 0.05967 training acc: 0.98336 testing acc: 0.9688\n",
      "epoch: 9 iter: 678 loss: 0.05693 training acc: 0.98482 testing acc: 0.9704\n",
      "epoch: 9 iter: 679 loss: 0.05671 training acc: 0.98444 testing acc: 0.9698\n",
      "epoch: 9 iter: 680 loss: 0.05696 training acc: 0.98440 testing acc: 0.9704\n",
      "epoch: 9 iter: 681 loss: 0.05665 training acc: 0.98454 testing acc: 0.9702\n",
      "epoch: 9 iter: 682 loss: 0.05719 training acc: 0.98430 testing acc: 0.9704\n",
      "epoch: 9 iter: 683 loss: 0.05754 training acc: 0.98418 testing acc: 0.9702\n",
      "epoch: 9 iter: 684 loss: 0.05803 training acc: 0.98378 testing acc: 0.9686\n",
      "epoch: 9 iter: 685 loss: 0.05828 training acc: 0.98388 testing acc: 0.968\n",
      "epoch: 9 iter: 686 loss: 0.06991 training acc: 0.97962 testing acc: 0.9658\n",
      "epoch: 9 iter: 687 loss: 0.06489 training acc: 0.98142 testing acc: 0.9686\n",
      "epoch: 9 iter: 688 loss: 0.06622 training acc: 0.98084 testing acc: 0.9686\n",
      "epoch: 9 iter: 689 loss: 0.06597 training acc: 0.98072 testing acc: 0.9674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 iter: 690 loss: 0.06449 training acc: 0.98104 testing acc: 0.9682\n",
      "epoch: 9 iter: 691 loss: 0.06473 training acc: 0.98098 testing acc: 0.9674\n",
      "epoch: 9 iter: 692 loss: 0.06290 training acc: 0.98154 testing acc: 0.9668\n",
      "epoch: 9 iter: 693 loss: 0.06318 training acc: 0.98154 testing acc: 0.9674\n",
      "epoch: 9 iter: 694 loss: 0.07867 training acc: 0.97544 testing acc: 0.963\n",
      "epoch: 9 iter: 695 loss: 0.08147 training acc: 0.97468 testing acc: 0.9614\n",
      "epoch: 9 iter: 696 loss: 0.06991 training acc: 0.97882 testing acc: 0.9654\n",
      "epoch: 9 iter: 697 loss: 0.07246 training acc: 0.97820 testing acc: 0.9656\n",
      "epoch: 9 iter: 698 loss: 0.07391 training acc: 0.97766 testing acc: 0.9656\n",
      "epoch: 9 iter: 699 loss: 0.08293 training acc: 0.97380 testing acc: 0.959\n",
      "epoch: 9 iter: 700 loss: 0.06796 training acc: 0.97950 testing acc: 0.9654\n",
      "epoch: 9 iter: 701 loss: 0.06757 training acc: 0.97956 testing acc: 0.9644\n",
      "epoch: 9 iter: 702 loss: 0.06314 training acc: 0.98186 testing acc: 0.9674\n",
      "epoch: 9 iter: 703 loss: 0.06139 training acc: 0.98270 testing acc: 0.9696\n",
      "epoch: 9 iter: 704 loss: 0.06217 training acc: 0.98204 testing acc: 0.971\n",
      "epoch: 9 iter: 705 loss: 0.06291 training acc: 0.98200 testing acc: 0.9708\n",
      "epoch: 9 iter: 706 loss: 0.07332 training acc: 0.97856 testing acc: 0.967\n",
      "epoch: 9 iter: 707 loss: 0.07283 training acc: 0.97850 testing acc: 0.968\n",
      "epoch: 9 iter: 708 loss: 0.06359 training acc: 0.98166 testing acc: 0.969\n",
      "epoch: 9 iter: 709 loss: 0.06244 training acc: 0.98214 testing acc: 0.9696\n",
      "epoch: 9 iter: 710 loss: 0.06972 training acc: 0.97930 testing acc: 0.967\n",
      "epoch: 9 iter: 711 loss: 0.06629 training acc: 0.98060 testing acc: 0.967\n",
      "epoch: 9 iter: 712 loss: 0.06603 training acc: 0.98094 testing acc: 0.967\n",
      "epoch: 9 iter: 713 loss: 0.06854 training acc: 0.98036 testing acc: 0.9666\n",
      "epoch: 9 iter: 714 loss: 0.06795 training acc: 0.98042 testing acc: 0.9668\n",
      "epoch: 9 iter: 715 loss: 0.06533 training acc: 0.98130 testing acc: 0.9676\n",
      "epoch: 9 iter: 716 loss: 0.06288 training acc: 0.98198 testing acc: 0.9672\n",
      "epoch: 9 iter: 717 loss: 0.06138 training acc: 0.98256 testing acc: 0.967\n",
      "epoch: 9 iter: 718 loss: 0.05834 training acc: 0.98402 testing acc: 0.9688\n",
      "epoch: 9 iter: 719 loss: 0.05808 training acc: 0.98372 testing acc: 0.9688\n",
      "epoch: 9 iter: 720 loss: 0.06297 training acc: 0.98176 testing acc: 0.9682\n",
      "epoch: 9 iter: 721 loss: 0.06468 training acc: 0.98074 testing acc: 0.965\n",
      "epoch: 9 iter: 722 loss: 0.06280 training acc: 0.98126 testing acc: 0.966\n",
      "epoch: 9 iter: 723 loss: 0.06152 training acc: 0.98216 testing acc: 0.9674\n",
      "epoch: 9 iter: 724 loss: 0.06178 training acc: 0.98188 testing acc: 0.968\n",
      "epoch: 9 iter: 725 loss: 0.06261 training acc: 0.98178 testing acc: 0.9686\n",
      "epoch: 9 iter: 726 loss: 0.06375 training acc: 0.98092 testing acc: 0.9682\n",
      "epoch: 9 iter: 727 loss: 0.06197 training acc: 0.98148 testing acc: 0.9674\n",
      "epoch: 9 iter: 728 loss: 0.06005 training acc: 0.98258 testing acc: 0.9702\n",
      "epoch: 9 iter: 729 loss: 0.06737 training acc: 0.97972 testing acc: 0.9668\n",
      "epoch: 9 iter: 730 loss: 0.07082 training acc: 0.97834 testing acc: 0.9654\n",
      "epoch: 9 iter: 731 loss: 0.07226 training acc: 0.97756 testing acc: 0.963\n",
      "epoch: 9 iter: 732 loss: 0.06320 training acc: 0.98168 testing acc: 0.9648\n",
      "epoch: 9 iter: 733 loss: 0.05799 training acc: 0.98346 testing acc: 0.9684\n",
      "epoch: 9 iter: 734 loss: 0.05798 training acc: 0.98346 testing acc: 0.9688\n",
      "epoch: 9 iter: 735 loss: 0.05776 training acc: 0.98342 testing acc: 0.969\n",
      "epoch: 9 iter: 736 loss: 0.05910 training acc: 0.98288 testing acc: 0.9698\n",
      "epoch: 9 iter: 737 loss: 0.06282 training acc: 0.98154 testing acc: 0.9678\n",
      "epoch: 9 iter: 738 loss: 0.06192 training acc: 0.98200 testing acc: 0.968\n",
      "epoch: 9 iter: 739 loss: 0.06023 training acc: 0.98274 testing acc: 0.9682\n",
      "epoch: 9 iter: 740 loss: 0.06012 training acc: 0.98272 testing acc: 0.9666\n",
      "epoch: 9 iter: 741 loss: 0.06232 training acc: 0.98226 testing acc: 0.9646\n",
      "epoch: 9 iter: 742 loss: 0.06470 training acc: 0.98176 testing acc: 0.9664\n",
      "epoch: 9 iter: 743 loss: 0.06574 training acc: 0.98064 testing acc: 0.9658\n",
      "epoch: 9 iter: 744 loss: 0.06454 training acc: 0.98106 testing acc: 0.9664\n",
      "epoch: 9 iter: 745 loss: 0.06307 training acc: 0.98196 testing acc: 0.9646\n",
      "epoch: 9 iter: 746 loss: 0.06690 training acc: 0.98080 testing acc: 0.9638\n",
      "epoch: 9 iter: 747 loss: 0.06332 training acc: 0.98210 testing acc: 0.9666\n",
      "epoch: 9 iter: 748 loss: 0.06448 training acc: 0.98112 testing acc: 0.966\n",
      "epoch: 9 iter: 749 loss: 0.06337 training acc: 0.98142 testing acc: 0.9666\n",
      "epoch: 9 iter: 750 loss: 0.07957 training acc: 0.97502 testing acc: 0.9636\n",
      "epoch: 9 iter: 751 loss: 0.08000 training acc: 0.97492 testing acc: 0.9634\n",
      "epoch: 9 iter: 752 loss: 0.07402 training acc: 0.97762 testing acc: 0.9622\n",
      "epoch: 9 iter: 753 loss: 0.07758 training acc: 0.97554 testing acc: 0.9612\n",
      "epoch: 9 iter: 754 loss: 0.06863 training acc: 0.97966 testing acc: 0.9644\n",
      "epoch: 9 iter: 755 loss: 0.06397 training acc: 0.98132 testing acc: 0.966\n",
      "epoch: 9 iter: 756 loss: 0.06488 training acc: 0.98084 testing acc: 0.9662\n",
      "epoch: 9 iter: 757 loss: 0.06373 training acc: 0.98180 testing acc: 0.9668\n",
      "epoch: 9 iter: 758 loss: 0.06977 training acc: 0.97848 testing acc: 0.9658\n",
      "epoch: 9 iter: 759 loss: 0.06677 training acc: 0.97994 testing acc: 0.9648\n",
      "epoch: 9 iter: 760 loss: 0.06455 training acc: 0.98112 testing acc: 0.967\n",
      "epoch: 9 iter: 761 loss: 0.06333 training acc: 0.98162 testing acc: 0.9678\n",
      "epoch: 9 iter: 762 loss: 0.06453 training acc: 0.98118 testing acc: 0.9684\n",
      "epoch: 9 iter: 763 loss: 0.07033 training acc: 0.97902 testing acc: 0.967\n",
      "epoch: 9 iter: 764 loss: 0.06449 training acc: 0.98120 testing acc: 0.9674\n",
      "epoch: 9 iter: 765 loss: 0.06929 training acc: 0.97976 testing acc: 0.9672\n",
      "epoch: 9 iter: 766 loss: 0.06344 training acc: 0.98190 testing acc: 0.9704\n",
      "epoch: 9 iter: 767 loss: 0.06132 training acc: 0.98262 testing acc: 0.97\n",
      "epoch: 9 iter: 768 loss: 0.06213 training acc: 0.98236 testing acc: 0.9706\n",
      "epoch: 9 iter: 769 loss: 0.06277 training acc: 0.98174 testing acc: 0.9696\n",
      "epoch: 9 iter: 770 loss: 0.06287 training acc: 0.98174 testing acc: 0.9684\n",
      "epoch: 9 iter: 771 loss: 0.06309 training acc: 0.98184 testing acc: 0.97\n",
      "epoch: 9 iter: 772 loss: 0.06250 training acc: 0.98196 testing acc: 0.97\n",
      "epoch: 9 iter: 773 loss: 0.06446 training acc: 0.98180 testing acc: 0.9678\n",
      "epoch: 9 iter: 774 loss: 0.06551 training acc: 0.98136 testing acc: 0.9678\n",
      "epoch: 9 iter: 775 loss: 0.06494 training acc: 0.98140 testing acc: 0.9674\n",
      "epoch: 9 iter: 776 loss: 0.06454 training acc: 0.98186 testing acc: 0.967\n",
      "epoch: 9 iter: 777 loss: 0.06252 training acc: 0.98200 testing acc: 0.9682\n",
      "epoch: 9 iter: 778 loss: 0.06337 training acc: 0.98156 testing acc: 0.9686\n",
      "epoch: 9 iter: 779 loss: 0.06444 training acc: 0.98136 testing acc: 0.9672\n",
      "epoch: 9 iter: 780 loss: 0.06321 training acc: 0.98200 testing acc: 0.9682\n",
      "epoch: 9 iter: 781 loss: 0.06285 training acc: 0.98232 testing acc: 0.9682\n",
      "epoch: 9 iter: 782 loss: 0.06462 training acc: 0.98142 testing acc: 0.9672\n",
      "epoch: 9 iter: 783 loss: 0.06364 training acc: 0.98186 testing acc: 0.9682\n",
      "epoch: 9 iter: 784 loss: 0.06469 training acc: 0.98164 testing acc: 0.9674\n",
      "epoch: 9 iter: 785 loss: 0.06459 training acc: 0.98134 testing acc: 0.9676\n",
      "epoch: 9 iter: 786 loss: 0.06574 training acc: 0.98108 testing acc: 0.9678\n",
      "epoch: 9 iter: 787 loss: 0.07656 training acc: 0.97704 testing acc: 0.9664\n",
      "epoch: 9 iter: 788 loss: 0.06977 training acc: 0.97952 testing acc: 0.9678\n",
      "epoch: 9 iter: 789 loss: 0.06729 training acc: 0.98036 testing acc: 0.9682\n",
      "epoch: 9 iter: 790 loss: 0.07614 training acc: 0.97742 testing acc: 0.966\n",
      "epoch: 9 iter: 791 loss: 0.06958 training acc: 0.97956 testing acc: 0.967\n",
      "epoch: 9 iter: 792 loss: 0.06093 training acc: 0.98260 testing acc: 0.9672\n",
      "epoch: 9 iter: 793 loss: 0.06034 training acc: 0.98272 testing acc: 0.967\n",
      "epoch: 9 iter: 794 loss: 0.06263 training acc: 0.98172 testing acc: 0.9658\n",
      "epoch: 9 iter: 795 loss: 0.06581 training acc: 0.98022 testing acc: 0.9664\n",
      "epoch: 9 iter: 796 loss: 0.06251 training acc: 0.98140 testing acc: 0.9664\n",
      "epoch: 9 iter: 797 loss: 0.06218 training acc: 0.98218 testing acc: 0.968\n",
      "epoch: 9 iter: 798 loss: 0.05854 training acc: 0.98336 testing acc: 0.9692\n",
      "epoch: 9 iter: 799 loss: 0.06450 training acc: 0.98108 testing acc: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 iter: 800 loss: 0.06099 training acc: 0.98242 testing acc: 0.967\n",
      "epoch: 9 iter: 801 loss: 0.06078 training acc: 0.98252 testing acc: 0.9674\n",
      "epoch: 9 iter: 802 loss: 0.06622 training acc: 0.98022 testing acc: 0.9652\n",
      "epoch: 9 iter: 803 loss: 0.06794 training acc: 0.97982 testing acc: 0.9668\n",
      "epoch: 9 iter: 804 loss: 0.06479 training acc: 0.98142 testing acc: 0.9652\n",
      "epoch: 9 iter: 805 loss: 0.06237 training acc: 0.98202 testing acc: 0.9688\n",
      "epoch: 9 iter: 806 loss: 0.06504 training acc: 0.98130 testing acc: 0.9692\n",
      "epoch: 9 iter: 807 loss: 0.06603 training acc: 0.98070 testing acc: 0.9658\n",
      "epoch: 9 iter: 808 loss: 0.06992 training acc: 0.97936 testing acc: 0.9634\n",
      "epoch: 9 iter: 809 loss: 0.06847 training acc: 0.97976 testing acc: 0.9634\n",
      "epoch: 9 iter: 810 loss: 0.06486 training acc: 0.98130 testing acc: 0.9664\n",
      "epoch: 9 iter: 811 loss: 0.06874 training acc: 0.97996 testing acc: 0.964\n",
      "epoch: 9 iter: 812 loss: 0.06807 training acc: 0.98054 testing acc: 0.967\n",
      "epoch: 9 iter: 813 loss: 0.06844 training acc: 0.98042 testing acc: 0.9664\n",
      "epoch: 9 iter: 814 loss: 0.06890 training acc: 0.98024 testing acc: 0.9664\n",
      "epoch: 9 iter: 815 loss: 0.06641 training acc: 0.98108 testing acc: 0.9666\n",
      "epoch: 9 iter: 816 loss: 0.07897 training acc: 0.97642 testing acc: 0.964\n",
      "epoch: 9 iter: 817 loss: 0.07971 training acc: 0.97612 testing acc: 0.964\n",
      "epoch: 9 iter: 818 loss: 0.07351 training acc: 0.97836 testing acc: 0.9678\n",
      "epoch: 9 iter: 819 loss: 0.06223 training acc: 0.98208 testing acc: 0.9698\n",
      "epoch: 9 iter: 820 loss: 0.06161 training acc: 0.98218 testing acc: 0.9702\n",
      "epoch: 9 iter: 821 loss: 0.05984 training acc: 0.98256 testing acc: 0.9692\n",
      "epoch: 9 iter: 822 loss: 0.05952 training acc: 0.98254 testing acc: 0.9696\n",
      "epoch: 9 iter: 823 loss: 0.05892 training acc: 0.98288 testing acc: 0.9704\n",
      "epoch: 9 iter: 824 loss: 0.06132 training acc: 0.98208 testing acc: 0.9686\n",
      "epoch: 9 iter: 825 loss: 0.06098 training acc: 0.98224 testing acc: 0.9686\n",
      "epoch: 9 iter: 826 loss: 0.05926 training acc: 0.98258 testing acc: 0.9686\n",
      "epoch: 9 iter: 827 loss: 0.05673 training acc: 0.98396 testing acc: 0.9712\n",
      "epoch: 9 iter: 828 loss: 0.05577 training acc: 0.98418 testing acc: 0.97\n",
      "epoch: 9 iter: 829 loss: 0.06070 training acc: 0.98294 testing acc: 0.9686\n",
      "epoch: 9 iter: 830 loss: 0.07039 training acc: 0.97922 testing acc: 0.964\n",
      "epoch: 9 iter: 831 loss: 0.06527 training acc: 0.98100 testing acc: 0.964\n",
      "epoch: 9 iter: 832 loss: 0.06011 training acc: 0.98290 testing acc: 0.9676\n",
      "epoch: 9 iter: 833 loss: 0.05979 training acc: 0.98300 testing acc: 0.9662\n",
      "epoch: 9 iter: 834 loss: 0.06507 training acc: 0.98068 testing acc: 0.9662\n",
      "epoch: 9 iter: 835 loss: 0.06215 training acc: 0.98224 testing acc: 0.969\n",
      "epoch: 9 iter: 836 loss: 0.06212 training acc: 0.98216 testing acc: 0.9682\n",
      "epoch: 9 iter: 837 loss: 0.06183 training acc: 0.98282 testing acc: 0.97\n",
      "epoch: 9 iter: 838 loss: 0.06035 training acc: 0.98304 testing acc: 0.9694\n",
      "epoch: 9 iter: 839 loss: 0.06211 training acc: 0.98240 testing acc: 0.9688\n",
      "epoch: 9 iter: 840 loss: 0.06123 training acc: 0.98266 testing acc: 0.9674\n",
      "epoch: 9 iter: 841 loss: 0.06001 training acc: 0.98292 testing acc: 0.9686\n",
      "epoch: 9 iter: 842 loss: 0.05856 training acc: 0.98346 testing acc: 0.9692\n",
      "epoch: 9 iter: 843 loss: 0.05927 training acc: 0.98362 testing acc: 0.97\n",
      "epoch: 9 iter: 844 loss: 0.05938 training acc: 0.98312 testing acc: 0.9702\n",
      "epoch: 9 iter: 845 loss: 0.07946 training acc: 0.97678 testing acc: 0.9636\n",
      "epoch: 9 iter: 846 loss: 0.07078 training acc: 0.97886 testing acc: 0.9654\n",
      "epoch: 9 iter: 847 loss: 0.06901 training acc: 0.97942 testing acc: 0.9654\n",
      "epoch: 9 iter: 848 loss: 0.07107 training acc: 0.97880 testing acc: 0.9666\n",
      "epoch: 9 iter: 849 loss: 0.06749 training acc: 0.98010 testing acc: 0.9688\n",
      "epoch: 9 iter: 850 loss: 0.06527 training acc: 0.98080 testing acc: 0.969\n",
      "epoch: 9 iter: 851 loss: 0.05905 training acc: 0.98334 testing acc: 0.9714\n",
      "epoch: 9 iter: 852 loss: 0.05891 training acc: 0.98326 testing acc: 0.9724\n",
      "epoch: 9 iter: 853 loss: 0.06016 training acc: 0.98250 testing acc: 0.9712\n",
      "epoch: 9 iter: 854 loss: 0.06324 training acc: 0.98124 testing acc: 0.969\n",
      "epoch: 9 iter: 855 loss: 0.06197 training acc: 0.98160 testing acc: 0.9694\n",
      "epoch: 9 iter: 856 loss: 0.06065 training acc: 0.98216 testing acc: 0.9694\n",
      "epoch: 9 iter: 857 loss: 0.05764 training acc: 0.98338 testing acc: 0.971\n",
      "epoch: 9 iter: 858 loss: 0.05704 training acc: 0.98362 testing acc: 0.9726\n",
      "epoch: 9 iter: 859 loss: 0.06344 training acc: 0.98150 testing acc: 0.967\n",
      "epoch: 9 iter: 860 loss: 0.05856 training acc: 0.98320 testing acc: 0.9696\n",
      "epoch: 9 iter: 861 loss: 0.05736 training acc: 0.98354 testing acc: 0.9698\n",
      "epoch: 9 iter: 862 loss: 0.05699 training acc: 0.98368 testing acc: 0.9694\n",
      "epoch: 9 iter: 863 loss: 0.05452 training acc: 0.98458 testing acc: 0.9702\n",
      "epoch: 9 iter: 864 loss: 0.05613 training acc: 0.98376 testing acc: 0.97\n",
      "epoch: 9 iter: 865 loss: 0.05575 training acc: 0.98384 testing acc: 0.9702\n",
      "epoch: 9 iter: 866 loss: 0.05756 training acc: 0.98306 testing acc: 0.9708\n",
      "epoch: 9 iter: 867 loss: 0.05610 training acc: 0.98374 testing acc: 0.9712\n",
      "epoch: 9 iter: 868 loss: 0.05581 training acc: 0.98392 testing acc: 0.972\n",
      "epoch: 9 iter: 869 loss: 0.05876 training acc: 0.98284 testing acc: 0.9694\n",
      "epoch: 9 iter: 870 loss: 0.05828 training acc: 0.98306 testing acc: 0.9696\n",
      "epoch: 9 iter: 871 loss: 0.06559 training acc: 0.98074 testing acc: 0.9684\n",
      "epoch: 9 iter: 872 loss: 0.06782 training acc: 0.97952 testing acc: 0.9666\n",
      "epoch: 9 iter: 873 loss: 0.07841 training acc: 0.97578 testing acc: 0.96\n",
      "epoch: 9 iter: 874 loss: 0.07274 training acc: 0.97760 testing acc: 0.9638\n",
      "epoch: 9 iter: 875 loss: 0.07096 training acc: 0.97844 testing acc: 0.966\n",
      "epoch: 9 iter: 876 loss: 0.06829 training acc: 0.97934 testing acc: 0.9688\n",
      "epoch: 9 iter: 877 loss: 0.06798 training acc: 0.97972 testing acc: 0.9692\n",
      "epoch: 9 iter: 878 loss: 0.06830 training acc: 0.97916 testing acc: 0.9696\n",
      "epoch: 9 iter: 879 loss: 0.06648 training acc: 0.98076 testing acc: 0.9692\n",
      "epoch: 9 iter: 880 loss: 0.07129 training acc: 0.97886 testing acc: 0.9674\n",
      "epoch: 9 iter: 881 loss: 0.06377 training acc: 0.98164 testing acc: 0.97\n",
      "epoch: 9 iter: 882 loss: 0.06529 training acc: 0.98100 testing acc: 0.969\n",
      "epoch: 9 iter: 883 loss: 0.06611 training acc: 0.98044 testing acc: 0.9694\n",
      "epoch: 9 iter: 884 loss: 0.06463 training acc: 0.98098 testing acc: 0.9702\n",
      "epoch: 9 iter: 885 loss: 0.06189 training acc: 0.98180 testing acc: 0.969\n",
      "epoch: 9 iter: 886 loss: 0.06180 training acc: 0.98188 testing acc: 0.9692\n",
      "epoch: 9 iter: 887 loss: 0.06009 training acc: 0.98210 testing acc: 0.9706\n",
      "epoch: 9 iter: 888 loss: 0.06318 training acc: 0.98106 testing acc: 0.9694\n",
      "epoch: 9 iter: 889 loss: 0.06420 training acc: 0.98124 testing acc: 0.969\n",
      "epoch: 9 iter: 890 loss: 0.06174 training acc: 0.98174 testing acc: 0.971\n",
      "epoch: 9 iter: 891 loss: 0.06119 training acc: 0.98192 testing acc: 0.9708\n",
      "epoch: 9 iter: 892 loss: 0.06283 training acc: 0.98208 testing acc: 0.9684\n",
      "epoch: 9 iter: 893 loss: 0.07203 training acc: 0.97896 testing acc: 0.9664\n",
      "epoch: 9 iter: 894 loss: 0.06399 training acc: 0.98132 testing acc: 0.9672\n",
      "epoch: 9 iter: 895 loss: 0.06457 training acc: 0.98070 testing acc: 0.9666\n",
      "epoch: 9 iter: 896 loss: 0.07219 training acc: 0.97846 testing acc: 0.9642\n",
      "epoch: 9 iter: 897 loss: 0.07511 training acc: 0.97836 testing acc: 0.9622\n",
      "epoch: 9 iter: 898 loss: 0.07713 training acc: 0.97800 testing acc: 0.9616\n",
      "epoch: 9 iter: 899 loss: 0.07035 training acc: 0.97992 testing acc: 0.9642\n",
      "epoch: 9 iter: 900 loss: 0.06915 training acc: 0.98020 testing acc: 0.965\n",
      "epoch: 9 iter: 901 loss: 0.06752 training acc: 0.98050 testing acc: 0.9668\n",
      "epoch: 9 iter: 902 loss: 0.06631 training acc: 0.98104 testing acc: 0.967\n",
      "epoch: 9 iter: 903 loss: 0.06524 training acc: 0.98156 testing acc: 0.968\n",
      "epoch: 9 iter: 904 loss: 0.06457 training acc: 0.98176 testing acc: 0.968\n",
      "epoch: 9 iter: 905 loss: 0.06246 training acc: 0.98184 testing acc: 0.9676\n",
      "epoch: 9 iter: 906 loss: 0.06215 training acc: 0.98206 testing acc: 0.968\n",
      "epoch: 9 iter: 907 loss: 0.06193 training acc: 0.98220 testing acc: 0.9684\n",
      "epoch: 9 iter: 908 loss: 0.06146 training acc: 0.98238 testing acc: 0.9676\n",
      "epoch: 9 iter: 909 loss: 0.06595 training acc: 0.98078 testing acc: 0.9672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 iter: 910 loss: 0.06552 training acc: 0.98094 testing acc: 0.9674\n",
      "epoch: 9 iter: 911 loss: 0.06276 training acc: 0.98188 testing acc: 0.9684\n",
      "epoch: 9 iter: 912 loss: 0.06232 training acc: 0.98168 testing acc: 0.9688\n",
      "epoch: 9 iter: 913 loss: 0.05832 training acc: 0.98334 testing acc: 0.9718\n",
      "epoch: 9 iter: 914 loss: 0.05712 training acc: 0.98374 testing acc: 0.9722\n",
      "epoch: 9 iter: 915 loss: 0.05859 training acc: 0.98294 testing acc: 0.9712\n",
      "epoch: 9 iter: 916 loss: 0.05990 training acc: 0.98230 testing acc: 0.9708\n",
      "epoch: 9 iter: 917 loss: 0.06230 training acc: 0.98114 testing acc: 0.971\n",
      "epoch: 9 iter: 918 loss: 0.05904 training acc: 0.98264 testing acc: 0.9724\n",
      "epoch: 9 iter: 919 loss: 0.06230 training acc: 0.98134 testing acc: 0.9714\n",
      "epoch: 9 iter: 920 loss: 0.05945 training acc: 0.98288 testing acc: 0.9708\n",
      "epoch: 9 iter: 921 loss: 0.05954 training acc: 0.98262 testing acc: 0.972\n",
      "epoch: 9 iter: 922 loss: 0.06145 training acc: 0.98146 testing acc: 0.9696\n",
      "epoch: 9 iter: 923 loss: 0.05937 training acc: 0.98364 testing acc: 0.9684\n",
      "epoch: 9 iter: 924 loss: 0.05834 training acc: 0.98392 testing acc: 0.9702\n",
      "epoch: 9 iter: 925 loss: 0.05786 training acc: 0.98376 testing acc: 0.9702\n",
      "epoch: 9 iter: 926 loss: 0.06623 training acc: 0.98116 testing acc: 0.9668\n",
      "epoch: 9 iter: 927 loss: 0.06854 training acc: 0.97988 testing acc: 0.9662\n",
      "epoch: 9 iter: 928 loss: 0.07422 training acc: 0.97772 testing acc: 0.9634\n",
      "epoch: 9 iter: 929 loss: 0.06531 training acc: 0.98096 testing acc: 0.9676\n",
      "epoch: 9 iter: 930 loss: 0.06211 training acc: 0.98232 testing acc: 0.9702\n",
      "epoch: 9 iter: 931 loss: 0.08240 training acc: 0.97510 testing acc: 0.9648\n",
      "epoch: 9 iter: 932 loss: 0.06298 training acc: 0.98168 testing acc: 0.9698\n",
      "epoch: 9 iter: 933 loss: 0.06261 training acc: 0.98208 testing acc: 0.9674\n",
      "epoch: 9 iter: 934 loss: 0.06376 training acc: 0.98172 testing acc: 0.9696\n",
      "epoch: 9 iter: 935 loss: 0.06101 training acc: 0.98280 testing acc: 0.9696\n",
      "epoch: 9 iter: 936 loss: 0.05845 training acc: 0.98346 testing acc: 0.9706\n",
      "epoch: 9 iter: 937 loss: 0.06363 training acc: 0.98144 testing acc: 0.9678\n",
      "epoch: 9 iter: 938 loss: 0.05794 training acc: 0.98328 testing acc: 0.9706\n",
      "epoch: 9 iter: 939 loss: 0.05669 training acc: 0.98366 testing acc: 0.9698\n",
      "epoch: 9 iter: 940 loss: 0.05678 training acc: 0.98362 testing acc: 0.9692\n",
      "epoch: 9 iter: 941 loss: 0.05640 training acc: 0.98378 testing acc: 0.9692\n",
      "epoch: 9 iter: 942 loss: 0.05875 training acc: 0.98260 testing acc: 0.9682\n",
      "epoch: 9 iter: 943 loss: 0.05975 training acc: 0.98246 testing acc: 0.9682\n",
      "epoch: 9 iter: 944 loss: 0.06429 training acc: 0.98038 testing acc: 0.9662\n",
      "epoch: 9 iter: 945 loss: 0.05883 training acc: 0.98248 testing acc: 0.9686\n",
      "epoch: 9 iter: 946 loss: 0.05998 training acc: 0.98244 testing acc: 0.9688\n",
      "epoch: 9 iter: 947 loss: 0.06153 training acc: 0.98222 testing acc: 0.967\n",
      "epoch: 9 iter: 948 loss: 0.05827 training acc: 0.98332 testing acc: 0.97\n",
      "epoch: 9 iter: 949 loss: 0.05566 training acc: 0.98422 testing acc: 0.971\n",
      "epoch: 9 iter: 950 loss: 0.05558 training acc: 0.98446 testing acc: 0.9704\n",
      "epoch: 9 iter: 951 loss: 0.05703 training acc: 0.98408 testing acc: 0.9706\n",
      "epoch: 9 iter: 952 loss: 0.05699 training acc: 0.98392 testing acc: 0.9708\n",
      "epoch: 9 iter: 953 loss: 0.05771 training acc: 0.98404 testing acc: 0.9698\n",
      "epoch: 9 iter: 954 loss: 0.05714 training acc: 0.98420 testing acc: 0.9704\n",
      "epoch: 9 iter: 955 loss: 0.05609 training acc: 0.98456 testing acc: 0.9694\n",
      "epoch: 9 iter: 956 loss: 0.06044 training acc: 0.98228 testing acc: 0.9674\n",
      "epoch: 9 iter: 957 loss: 0.05720 training acc: 0.98322 testing acc: 0.9688\n",
      "epoch: 9 iter: 958 loss: 0.05790 training acc: 0.98294 testing acc: 0.9698\n",
      "epoch: 9 iter: 959 loss: 0.06023 training acc: 0.98200 testing acc: 0.9684\n",
      "epoch: 9 iter: 960 loss: 0.07018 training acc: 0.97864 testing acc: 0.9646\n",
      "epoch: 9 iter: 961 loss: 0.06199 training acc: 0.98128 testing acc: 0.9688\n",
      "epoch: 9 iter: 962 loss: 0.06175 training acc: 0.98134 testing acc: 0.9698\n",
      "epoch: 9 iter: 963 loss: 0.05813 training acc: 0.98300 testing acc: 0.9712\n",
      "epoch: 9 iter: 964 loss: 0.05740 training acc: 0.98338 testing acc: 0.9706\n",
      "epoch: 9 iter: 965 loss: 0.05869 training acc: 0.98288 testing acc: 0.9702\n",
      "epoch: 9 iter: 966 loss: 0.05605 training acc: 0.98376 testing acc: 0.9704\n",
      "epoch: 9 iter: 967 loss: 0.05751 training acc: 0.98340 testing acc: 0.9708\n",
      "epoch: 9 iter: 968 loss: 0.05923 training acc: 0.98274 testing acc: 0.9692\n",
      "epoch: 9 iter: 969 loss: 0.05505 training acc: 0.98440 testing acc: 0.9704\n",
      "epoch: 9 iter: 970 loss: 0.05797 training acc: 0.98326 testing acc: 0.9718\n",
      "epoch: 9 iter: 971 loss: 0.05793 training acc: 0.98308 testing acc: 0.9714\n",
      "epoch: 9 iter: 972 loss: 0.06095 training acc: 0.98238 testing acc: 0.9694\n",
      "epoch: 9 iter: 973 loss: 0.06149 training acc: 0.98216 testing acc: 0.9704\n",
      "epoch: 9 iter: 974 loss: 0.05753 training acc: 0.98320 testing acc: 0.9726\n",
      "epoch: 9 iter: 975 loss: 0.06232 training acc: 0.98192 testing acc: 0.9706\n",
      "epoch: 9 iter: 976 loss: 0.06241 training acc: 0.98240 testing acc: 0.9698\n",
      "epoch: 9 iter: 977 loss: 0.06130 training acc: 0.98258 testing acc: 0.9692\n",
      "epoch: 9 iter: 978 loss: 0.06187 training acc: 0.98198 testing acc: 0.9714\n",
      "epoch: 9 iter: 979 loss: 0.06076 training acc: 0.98166 testing acc: 0.9728\n",
      "epoch: 9 iter: 980 loss: 0.06050 training acc: 0.98164 testing acc: 0.9714\n",
      "epoch: 9 iter: 981 loss: 0.05965 training acc: 0.98258 testing acc: 0.9728\n",
      "epoch: 9 iter: 982 loss: 0.06111 training acc: 0.98244 testing acc: 0.972\n",
      "epoch: 9 iter: 983 loss: 0.05943 training acc: 0.98272 testing acc: 0.9722\n",
      "epoch: 9 iter: 984 loss: 0.06759 training acc: 0.97926 testing acc: 0.9668\n",
      "epoch: 9 iter: 985 loss: 0.07575 training acc: 0.97658 testing acc: 0.967\n",
      "epoch: 9 iter: 986 loss: 0.06846 training acc: 0.97914 testing acc: 0.9688\n",
      "epoch: 9 iter: 987 loss: 0.07318 training acc: 0.97710 testing acc: 0.9652\n",
      "epoch: 9 iter: 988 loss: 0.07265 training acc: 0.97716 testing acc: 0.9648\n",
      "epoch: 9 iter: 989 loss: 0.06893 training acc: 0.97878 testing acc: 0.9668\n",
      "epoch: 9 iter: 990 loss: 0.06037 training acc: 0.98214 testing acc: 0.9704\n",
      "epoch: 9 iter: 991 loss: 0.06149 training acc: 0.98172 testing acc: 0.968\n",
      "epoch: 9 iter: 992 loss: 0.06493 training acc: 0.98064 testing acc: 0.9682\n",
      "epoch: 9 iter: 993 loss: 0.06079 training acc: 0.98204 testing acc: 0.9696\n",
      "epoch: 9 iter: 994 loss: 0.07319 training acc: 0.97780 testing acc: 0.9654\n",
      "epoch: 9 iter: 995 loss: 0.08036 training acc: 0.97548 testing acc: 0.9614\n",
      "epoch: 9 iter: 996 loss: 0.07965 training acc: 0.97560 testing acc: 0.9594\n",
      "epoch: 9 iter: 997 loss: 0.05919 training acc: 0.98260 testing acc: 0.9678\n",
      "epoch: 9 iter: 998 loss: 0.05836 training acc: 0.98278 testing acc: 0.9692\n",
      "epoch: 9 iter: 999 loss: 0.06232 training acc: 0.98158 testing acc: 0.9674\n",
      "epoch: 10 iter:   0 loss: 0.06785 training acc: 0.97938 testing acc: 0.9662\n",
      "epoch: 10 iter:   1 loss: 0.05909 training acc: 0.98258 testing acc: 0.9668\n",
      "epoch: 10 iter:   2 loss: 0.05856 training acc: 0.98280 testing acc: 0.968\n",
      "epoch: 10 iter:   3 loss: 0.05649 training acc: 0.98348 testing acc: 0.9688\n",
      "epoch: 10 iter:   4 loss: 0.05593 training acc: 0.98384 testing acc: 0.969\n",
      "epoch: 10 iter:   5 loss: 0.06074 training acc: 0.98194 testing acc: 0.9698\n",
      "epoch: 10 iter:   6 loss: 0.06539 training acc: 0.97992 testing acc: 0.9688\n",
      "epoch: 10 iter:   7 loss: 0.06197 training acc: 0.98120 testing acc: 0.9684\n",
      "epoch: 10 iter:   8 loss: 0.06034 training acc: 0.98166 testing acc: 0.9684\n",
      "epoch: 10 iter:   9 loss: 0.05995 training acc: 0.98202 testing acc: 0.969\n",
      "epoch: 10 iter:  10 loss: 0.06145 training acc: 0.98160 testing acc: 0.9684\n",
      "epoch: 10 iter:  11 loss: 0.05802 training acc: 0.98312 testing acc: 0.9692\n",
      "epoch: 10 iter:  12 loss: 0.06792 training acc: 0.97910 testing acc: 0.9644\n",
      "epoch: 10 iter:  13 loss: 0.05829 training acc: 0.98252 testing acc: 0.9688\n",
      "epoch: 10 iter:  14 loss: 0.05857 training acc: 0.98234 testing acc: 0.9688\n",
      "epoch: 10 iter:  15 loss: 0.07093 training acc: 0.97850 testing acc: 0.9642\n",
      "epoch: 10 iter:  16 loss: 0.06083 training acc: 0.98200 testing acc: 0.9684\n",
      "epoch: 10 iter:  17 loss: 0.06011 training acc: 0.98198 testing acc: 0.9694\n",
      "epoch: 10 iter:  18 loss: 0.05755 training acc: 0.98254 testing acc: 0.9712\n",
      "epoch: 10 iter:  19 loss: 0.05986 training acc: 0.98228 testing acc: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 iter:  20 loss: 0.05922 training acc: 0.98254 testing acc: 0.9702\n",
      "epoch: 10 iter:  21 loss: 0.06902 training acc: 0.97902 testing acc: 0.966\n",
      "epoch: 10 iter:  22 loss: 0.06237 training acc: 0.98114 testing acc: 0.9682\n",
      "epoch: 10 iter:  23 loss: 0.05763 training acc: 0.98272 testing acc: 0.9694\n",
      "epoch: 10 iter:  24 loss: 0.05776 training acc: 0.98310 testing acc: 0.9718\n",
      "epoch: 10 iter:  25 loss: 0.05720 training acc: 0.98340 testing acc: 0.9716\n",
      "epoch: 10 iter:  26 loss: 0.05726 training acc: 0.98292 testing acc: 0.971\n",
      "epoch: 10 iter:  27 loss: 0.05405 training acc: 0.98442 testing acc: 0.9714\n",
      "epoch: 10 iter:  28 loss: 0.05453 training acc: 0.98404 testing acc: 0.9712\n",
      "epoch: 10 iter:  29 loss: 0.05539 training acc: 0.98378 testing acc: 0.9722\n",
      "epoch: 10 iter:  30 loss: 0.05552 training acc: 0.98376 testing acc: 0.9724\n",
      "epoch: 10 iter:  31 loss: 0.05673 training acc: 0.98360 testing acc: 0.9722\n",
      "epoch: 10 iter:  32 loss: 0.05806 training acc: 0.98310 testing acc: 0.9708\n",
      "epoch: 10 iter:  33 loss: 0.05791 training acc: 0.98346 testing acc: 0.9692\n",
      "epoch: 10 iter:  34 loss: 0.05909 training acc: 0.98234 testing acc: 0.9694\n",
      "epoch: 10 iter:  35 loss: 0.06262 training acc: 0.98118 testing acc: 0.9688\n",
      "epoch: 10 iter:  36 loss: 0.06119 training acc: 0.98150 testing acc: 0.9686\n",
      "epoch: 10 iter:  37 loss: 0.05838 training acc: 0.98238 testing acc: 0.9694\n",
      "epoch: 10 iter:  38 loss: 0.06581 training acc: 0.98066 testing acc: 0.97\n",
      "epoch: 10 iter:  39 loss: 0.06041 training acc: 0.98274 testing acc: 0.9714\n",
      "epoch: 10 iter:  40 loss: 0.05986 training acc: 0.98250 testing acc: 0.9698\n",
      "epoch: 10 iter:  41 loss: 0.05913 training acc: 0.98288 testing acc: 0.9702\n",
      "epoch: 10 iter:  42 loss: 0.07048 training acc: 0.97868 testing acc: 0.9676\n",
      "epoch: 10 iter:  43 loss: 0.07838 training acc: 0.97612 testing acc: 0.965\n",
      "epoch: 10 iter:  44 loss: 0.07733 training acc: 0.97620 testing acc: 0.966\n",
      "epoch: 10 iter:  45 loss: 0.06533 training acc: 0.98124 testing acc: 0.97\n",
      "epoch: 10 iter:  46 loss: 0.06293 training acc: 0.98190 testing acc: 0.9698\n",
      "epoch: 10 iter:  47 loss: 0.06139 training acc: 0.98244 testing acc: 0.97\n",
      "epoch: 10 iter:  48 loss: 0.07099 training acc: 0.97894 testing acc: 0.9646\n",
      "epoch: 10 iter:  49 loss: 0.06173 training acc: 0.98174 testing acc: 0.9682\n",
      "epoch: 10 iter:  50 loss: 0.06083 training acc: 0.98196 testing acc: 0.9682\n",
      "epoch: 10 iter:  51 loss: 0.05750 training acc: 0.98348 testing acc: 0.9718\n",
      "epoch: 10 iter:  52 loss: 0.05641 training acc: 0.98414 testing acc: 0.9712\n",
      "epoch: 10 iter:  53 loss: 0.06865 training acc: 0.97940 testing acc: 0.9646\n",
      "epoch: 10 iter:  54 loss: 0.05732 training acc: 0.98348 testing acc: 0.9708\n",
      "epoch: 10 iter:  55 loss: 0.05780 training acc: 0.98322 testing acc: 0.9716\n",
      "epoch: 10 iter:  56 loss: 0.05507 training acc: 0.98420 testing acc: 0.9732\n",
      "epoch: 10 iter:  57 loss: 0.05615 training acc: 0.98364 testing acc: 0.9724\n",
      "epoch: 10 iter:  58 loss: 0.05547 training acc: 0.98390 testing acc: 0.9726\n",
      "epoch: 10 iter:  59 loss: 0.05386 training acc: 0.98458 testing acc: 0.9738\n",
      "epoch: 10 iter:  60 loss: 0.05431 training acc: 0.98436 testing acc: 0.9734\n",
      "epoch: 10 iter:  61 loss: 0.05578 training acc: 0.98408 testing acc: 0.9722\n",
      "epoch: 10 iter:  62 loss: 0.06054 training acc: 0.98214 testing acc: 0.9718\n",
      "epoch: 10 iter:  63 loss: 0.05966 training acc: 0.98278 testing acc: 0.972\n",
      "epoch: 10 iter:  64 loss: 0.05831 training acc: 0.98284 testing acc: 0.9722\n",
      "epoch: 10 iter:  65 loss: 0.05699 training acc: 0.98350 testing acc: 0.972\n",
      "epoch: 10 iter:  66 loss: 0.05992 training acc: 0.98270 testing acc: 0.9702\n",
      "epoch: 10 iter:  67 loss: 0.06004 training acc: 0.98254 testing acc: 0.9706\n",
      "epoch: 10 iter:  68 loss: 0.06265 training acc: 0.98124 testing acc: 0.969\n",
      "epoch: 10 iter:  69 loss: 0.06078 training acc: 0.98206 testing acc: 0.9694\n",
      "epoch: 10 iter:  70 loss: 0.06319 training acc: 0.98052 testing acc: 0.9688\n",
      "epoch: 10 iter:  71 loss: 0.06191 training acc: 0.98120 testing acc: 0.9692\n",
      "epoch: 10 iter:  72 loss: 0.06477 training acc: 0.98044 testing acc: 0.9694\n",
      "epoch: 10 iter:  73 loss: 0.05992 training acc: 0.98202 testing acc: 0.9714\n",
      "epoch: 10 iter:  74 loss: 0.05670 training acc: 0.98360 testing acc: 0.9722\n",
      "epoch: 10 iter:  75 loss: 0.05751 training acc: 0.98348 testing acc: 0.9714\n",
      "epoch: 10 iter:  76 loss: 0.05799 training acc: 0.98336 testing acc: 0.9714\n",
      "epoch: 10 iter:  77 loss: 0.05830 training acc: 0.98342 testing acc: 0.971\n",
      "epoch: 10 iter:  78 loss: 0.05701 training acc: 0.98310 testing acc: 0.9708\n",
      "epoch: 10 iter:  79 loss: 0.05680 training acc: 0.98338 testing acc: 0.9702\n",
      "epoch: 10 iter:  80 loss: 0.05714 training acc: 0.98326 testing acc: 0.9704\n",
      "epoch: 10 iter:  81 loss: 0.05594 training acc: 0.98370 testing acc: 0.9712\n",
      "epoch: 10 iter:  82 loss: 0.06309 training acc: 0.98118 testing acc: 0.9676\n",
      "epoch: 10 iter:  83 loss: 0.06737 training acc: 0.97966 testing acc: 0.9666\n",
      "epoch: 10 iter:  84 loss: 0.06328 training acc: 0.98126 testing acc: 0.9678\n",
      "epoch: 10 iter:  85 loss: 0.06141 training acc: 0.98212 testing acc: 0.9686\n",
      "epoch: 10 iter:  86 loss: 0.05732 training acc: 0.98300 testing acc: 0.9702\n",
      "epoch: 10 iter:  87 loss: 0.06463 training acc: 0.98034 testing acc: 0.9672\n",
      "epoch: 10 iter:  88 loss: 0.06024 training acc: 0.98148 testing acc: 0.9686\n",
      "epoch: 10 iter:  89 loss: 0.06417 training acc: 0.98062 testing acc: 0.967\n",
      "epoch: 10 iter:  90 loss: 0.06349 training acc: 0.98096 testing acc: 0.968\n",
      "epoch: 10 iter:  91 loss: 0.05829 training acc: 0.98260 testing acc: 0.9706\n",
      "epoch: 10 iter:  92 loss: 0.05666 training acc: 0.98308 testing acc: 0.9692\n",
      "epoch: 10 iter:  93 loss: 0.07842 training acc: 0.97552 testing acc: 0.9604\n",
      "epoch: 10 iter:  94 loss: 0.06617 training acc: 0.97980 testing acc: 0.965\n",
      "epoch: 10 iter:  95 loss: 0.06592 training acc: 0.97930 testing acc: 0.9654\n",
      "epoch: 10 iter:  96 loss: 0.06479 training acc: 0.98038 testing acc: 0.9682\n",
      "epoch: 10 iter:  97 loss: 0.06297 training acc: 0.98090 testing acc: 0.97\n",
      "epoch: 10 iter:  98 loss: 0.06391 training acc: 0.98026 testing acc: 0.9692\n",
      "epoch: 10 iter:  99 loss: 0.06166 training acc: 0.98158 testing acc: 0.9694\n",
      "epoch: 10 iter: 100 loss: 0.06062 training acc: 0.98122 testing acc: 0.97\n",
      "epoch: 10 iter: 101 loss: 0.05906 training acc: 0.98230 testing acc: 0.9704\n",
      "epoch: 10 iter: 102 loss: 0.05897 training acc: 0.98254 testing acc: 0.9706\n",
      "epoch: 10 iter: 103 loss: 0.05977 training acc: 0.98178 testing acc: 0.9706\n",
      "epoch: 10 iter: 104 loss: 0.05637 training acc: 0.98370 testing acc: 0.9722\n",
      "epoch: 10 iter: 105 loss: 0.05557 training acc: 0.98398 testing acc: 0.9726\n",
      "epoch: 10 iter: 106 loss: 0.06456 training acc: 0.98100 testing acc: 0.9672\n",
      "epoch: 10 iter: 107 loss: 0.06242 training acc: 0.98166 testing acc: 0.9688\n",
      "epoch: 10 iter: 108 loss: 0.06295 training acc: 0.98134 testing acc: 0.9706\n",
      "epoch: 10 iter: 109 loss: 0.06285 training acc: 0.98128 testing acc: 0.9706\n",
      "epoch: 10 iter: 110 loss: 0.05827 training acc: 0.98336 testing acc: 0.9712\n",
      "epoch: 10 iter: 111 loss: 0.05747 training acc: 0.98366 testing acc: 0.9714\n",
      "epoch: 10 iter: 112 loss: 0.05727 training acc: 0.98376 testing acc: 0.9714\n",
      "epoch: 10 iter: 113 loss: 0.05659 training acc: 0.98402 testing acc: 0.9716\n",
      "epoch: 10 iter: 114 loss: 0.05727 training acc: 0.98352 testing acc: 0.9712\n",
      "epoch: 10 iter: 115 loss: 0.05495 training acc: 0.98464 testing acc: 0.9724\n",
      "epoch: 10 iter: 116 loss: 0.05750 training acc: 0.98324 testing acc: 0.972\n",
      "epoch: 10 iter: 117 loss: 0.05867 training acc: 0.98302 testing acc: 0.9726\n",
      "epoch: 10 iter: 118 loss: 0.05904 training acc: 0.98290 testing acc: 0.971\n",
      "epoch: 10 iter: 119 loss: 0.05627 training acc: 0.98382 testing acc: 0.9718\n",
      "epoch: 10 iter: 120 loss: 0.06105 training acc: 0.98236 testing acc: 0.97\n",
      "epoch: 10 iter: 121 loss: 0.05972 training acc: 0.98280 testing acc: 0.9712\n",
      "epoch: 10 iter: 122 loss: 0.06175 training acc: 0.98226 testing acc: 0.9716\n",
      "epoch: 10 iter: 123 loss: 0.06421 training acc: 0.98150 testing acc: 0.9708\n",
      "epoch: 10 iter: 124 loss: 0.06222 training acc: 0.98228 testing acc: 0.9714\n",
      "epoch: 10 iter: 125 loss: 0.06118 training acc: 0.98276 testing acc: 0.9724\n",
      "epoch: 10 iter: 126 loss: 0.06085 training acc: 0.98290 testing acc: 0.9724\n",
      "epoch: 10 iter: 127 loss: 0.06015 training acc: 0.98316 testing acc: 0.9718\n",
      "epoch: 10 iter: 128 loss: 0.05860 training acc: 0.98378 testing acc: 0.9732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 iter: 129 loss: 0.06487 training acc: 0.98096 testing acc: 0.971\n",
      "epoch: 10 iter: 130 loss: 0.05478 training acc: 0.98438 testing acc: 0.9724\n",
      "epoch: 10 iter: 131 loss: 0.05700 training acc: 0.98362 testing acc: 0.9714\n",
      "epoch: 10 iter: 132 loss: 0.05543 training acc: 0.98446 testing acc: 0.9726\n",
      "epoch: 10 iter: 133 loss: 0.05510 training acc: 0.98436 testing acc: 0.9726\n",
      "epoch: 10 iter: 134 loss: 0.05400 training acc: 0.98510 testing acc: 0.973\n",
      "epoch: 10 iter: 135 loss: 0.05517 training acc: 0.98484 testing acc: 0.9716\n",
      "epoch: 10 iter: 136 loss: 0.05382 training acc: 0.98528 testing acc: 0.9708\n",
      "epoch: 10 iter: 137 loss: 0.05734 training acc: 0.98408 testing acc: 0.9718\n",
      "epoch: 10 iter: 138 loss: 0.05841 training acc: 0.98348 testing acc: 0.9704\n",
      "epoch: 10 iter: 139 loss: 0.05912 training acc: 0.98338 testing acc: 0.9708\n",
      "epoch: 10 iter: 140 loss: 0.05656 training acc: 0.98412 testing acc: 0.9698\n",
      "epoch: 10 iter: 141 loss: 0.06077 training acc: 0.98288 testing acc: 0.9716\n",
      "epoch: 10 iter: 142 loss: 0.05700 training acc: 0.98444 testing acc: 0.9718\n",
      "epoch: 10 iter: 143 loss: 0.05704 training acc: 0.98330 testing acc: 0.97\n",
      "epoch: 10 iter: 144 loss: 0.05682 training acc: 0.98318 testing acc: 0.9704\n",
      "epoch: 10 iter: 145 loss: 0.05668 training acc: 0.98328 testing acc: 0.9708\n",
      "epoch: 10 iter: 146 loss: 0.05680 training acc: 0.98312 testing acc: 0.9704\n",
      "epoch: 10 iter: 147 loss: 0.05626 training acc: 0.98330 testing acc: 0.9702\n",
      "epoch: 10 iter: 148 loss: 0.05559 training acc: 0.98376 testing acc: 0.971\n",
      "epoch: 10 iter: 149 loss: 0.05521 training acc: 0.98388 testing acc: 0.9708\n",
      "epoch: 10 iter: 150 loss: 0.05446 training acc: 0.98434 testing acc: 0.9718\n",
      "epoch: 10 iter: 151 loss: 0.05375 training acc: 0.98464 testing acc: 0.9734\n",
      "epoch: 10 iter: 152 loss: 0.05242 training acc: 0.98510 testing acc: 0.9732\n",
      "epoch: 10 iter: 153 loss: 0.05273 training acc: 0.98510 testing acc: 0.9736\n",
      "epoch: 10 iter: 154 loss: 0.05666 training acc: 0.98362 testing acc: 0.9708\n",
      "epoch: 10 iter: 155 loss: 0.06991 training acc: 0.97820 testing acc: 0.9644\n",
      "epoch: 10 iter: 156 loss: 0.06481 training acc: 0.98040 testing acc: 0.9668\n",
      "epoch: 10 iter: 157 loss: 0.06306 training acc: 0.98108 testing acc: 0.9668\n",
      "epoch: 10 iter: 158 loss: 0.05685 training acc: 0.98354 testing acc: 0.9694\n",
      "epoch: 10 iter: 159 loss: 0.05114 training acc: 0.98600 testing acc: 0.9724\n",
      "epoch: 10 iter: 160 loss: 0.05343 training acc: 0.98512 testing acc: 0.9726\n",
      "epoch: 10 iter: 161 loss: 0.05394 training acc: 0.98462 testing acc: 0.9716\n",
      "epoch: 10 iter: 162 loss: 0.05404 training acc: 0.98424 testing acc: 0.9724\n",
      "epoch: 10 iter: 163 loss: 0.05431 training acc: 0.98460 testing acc: 0.9718\n",
      "epoch: 10 iter: 164 loss: 0.06021 training acc: 0.98226 testing acc: 0.9708\n",
      "epoch: 10 iter: 165 loss: 0.06517 training acc: 0.98010 testing acc: 0.967\n",
      "epoch: 10 iter: 166 loss: 0.06282 training acc: 0.98108 testing acc: 0.9682\n",
      "epoch: 10 iter: 167 loss: 0.05955 training acc: 0.98190 testing acc: 0.9688\n",
      "epoch: 10 iter: 168 loss: 0.06251 training acc: 0.98138 testing acc: 0.968\n",
      "epoch: 10 iter: 169 loss: 0.05710 training acc: 0.98334 testing acc: 0.9692\n",
      "epoch: 10 iter: 170 loss: 0.05872 training acc: 0.98256 testing acc: 0.9682\n",
      "epoch: 10 iter: 171 loss: 0.05874 training acc: 0.98234 testing acc: 0.968\n",
      "epoch: 10 iter: 172 loss: 0.06005 training acc: 0.98226 testing acc: 0.9696\n",
      "epoch: 10 iter: 173 loss: 0.05848 training acc: 0.98246 testing acc: 0.9684\n",
      "epoch: 10 iter: 174 loss: 0.05542 training acc: 0.98386 testing acc: 0.97\n",
      "epoch: 10 iter: 175 loss: 0.05380 training acc: 0.98440 testing acc: 0.9708\n",
      "epoch: 10 iter: 176 loss: 0.05404 training acc: 0.98446 testing acc: 0.9714\n",
      "epoch: 10 iter: 177 loss: 0.05328 training acc: 0.98500 testing acc: 0.972\n",
      "epoch: 10 iter: 178 loss: 0.05348 training acc: 0.98504 testing acc: 0.9712\n",
      "epoch: 10 iter: 179 loss: 0.05326 training acc: 0.98506 testing acc: 0.9706\n",
      "epoch: 10 iter: 180 loss: 0.05210 training acc: 0.98552 testing acc: 0.9726\n",
      "epoch: 10 iter: 181 loss: 0.05254 training acc: 0.98528 testing acc: 0.9726\n",
      "epoch: 10 iter: 182 loss: 0.05282 training acc: 0.98520 testing acc: 0.9726\n",
      "epoch: 10 iter: 183 loss: 0.05427 training acc: 0.98462 testing acc: 0.9714\n",
      "epoch: 10 iter: 184 loss: 0.05443 training acc: 0.98490 testing acc: 0.9728\n",
      "epoch: 10 iter: 185 loss: 0.05420 training acc: 0.98504 testing acc: 0.972\n",
      "epoch: 10 iter: 186 loss: 0.07870 training acc: 0.97652 testing acc: 0.9636\n",
      "epoch: 10 iter: 187 loss: 0.07011 training acc: 0.97966 testing acc: 0.9658\n",
      "epoch: 10 iter: 188 loss: 0.06519 training acc: 0.98092 testing acc: 0.9682\n",
      "epoch: 10 iter: 189 loss: 0.06098 training acc: 0.98236 testing acc: 0.9692\n",
      "epoch: 10 iter: 190 loss: 0.06219 training acc: 0.98200 testing acc: 0.9688\n",
      "epoch: 10 iter: 191 loss: 0.06176 training acc: 0.98192 testing acc: 0.97\n",
      "epoch: 10 iter: 192 loss: 0.06146 training acc: 0.98212 testing acc: 0.9708\n",
      "epoch: 10 iter: 193 loss: 0.06033 training acc: 0.98238 testing acc: 0.9714\n",
      "epoch: 10 iter: 194 loss: 0.07757 training acc: 0.97590 testing acc: 0.9634\n",
      "epoch: 10 iter: 195 loss: 0.06463 training acc: 0.98050 testing acc: 0.9674\n",
      "epoch: 10 iter: 196 loss: 0.06164 training acc: 0.98144 testing acc: 0.9684\n",
      "epoch: 10 iter: 197 loss: 0.06105 training acc: 0.98176 testing acc: 0.9686\n",
      "epoch: 10 iter: 198 loss: 0.05847 training acc: 0.98316 testing acc: 0.9702\n",
      "epoch: 10 iter: 199 loss: 0.06017 training acc: 0.98182 testing acc: 0.9698\n",
      "epoch: 10 iter: 200 loss: 0.05824 training acc: 0.98262 testing acc: 0.9696\n",
      "epoch: 10 iter: 201 loss: 0.05773 training acc: 0.98310 testing acc: 0.969\n",
      "epoch: 10 iter: 202 loss: 0.05832 training acc: 0.98280 testing acc: 0.9674\n",
      "epoch: 10 iter: 203 loss: 0.05822 training acc: 0.98260 testing acc: 0.9678\n",
      "epoch: 10 iter: 204 loss: 0.05803 training acc: 0.98234 testing acc: 0.9676\n",
      "epoch: 10 iter: 205 loss: 0.06005 training acc: 0.98174 testing acc: 0.966\n",
      "epoch: 10 iter: 206 loss: 0.05592 training acc: 0.98384 testing acc: 0.9704\n",
      "epoch: 10 iter: 207 loss: 0.05556 training acc: 0.98394 testing acc: 0.9706\n",
      "epoch: 10 iter: 208 loss: 0.05568 training acc: 0.98396 testing acc: 0.9702\n",
      "epoch: 10 iter: 209 loss: 0.05643 training acc: 0.98352 testing acc: 0.9704\n",
      "epoch: 10 iter: 210 loss: 0.05665 training acc: 0.98366 testing acc: 0.9702\n",
      "epoch: 10 iter: 211 loss: 0.05783 training acc: 0.98388 testing acc: 0.971\n",
      "epoch: 10 iter: 212 loss: 0.05666 training acc: 0.98424 testing acc: 0.9712\n",
      "epoch: 10 iter: 213 loss: 0.05653 training acc: 0.98444 testing acc: 0.9726\n",
      "epoch: 10 iter: 214 loss: 0.07559 training acc: 0.97694 testing acc: 0.9674\n",
      "epoch: 10 iter: 215 loss: 0.07562 training acc: 0.97696 testing acc: 0.9664\n",
      "epoch: 10 iter: 216 loss: 0.06580 training acc: 0.98030 testing acc: 0.968\n",
      "epoch: 10 iter: 217 loss: 0.06023 training acc: 0.98256 testing acc: 0.9682\n",
      "epoch: 10 iter: 218 loss: 0.05994 training acc: 0.98268 testing acc: 0.9682\n",
      "epoch: 10 iter: 219 loss: 0.05968 training acc: 0.98278 testing acc: 0.969\n",
      "epoch: 10 iter: 220 loss: 0.06040 training acc: 0.98222 testing acc: 0.9688\n",
      "epoch: 10 iter: 221 loss: 0.06383 training acc: 0.98046 testing acc: 0.9688\n",
      "epoch: 10 iter: 222 loss: 0.05786 training acc: 0.98280 testing acc: 0.9692\n",
      "epoch: 10 iter: 223 loss: 0.05836 training acc: 0.98260 testing acc: 0.9692\n",
      "epoch: 10 iter: 224 loss: 0.05454 training acc: 0.98432 testing acc: 0.9696\n",
      "epoch: 10 iter: 225 loss: 0.05346 training acc: 0.98458 testing acc: 0.9706\n",
      "epoch: 10 iter: 226 loss: 0.05398 training acc: 0.98468 testing acc: 0.9708\n",
      "epoch: 10 iter: 227 loss: 0.06120 training acc: 0.98190 testing acc: 0.9688\n",
      "epoch: 10 iter: 228 loss: 0.06162 training acc: 0.98150 testing acc: 0.9688\n",
      "epoch: 10 iter: 229 loss: 0.06817 training acc: 0.97894 testing acc: 0.9634\n",
      "epoch: 10 iter: 230 loss: 0.05719 training acc: 0.98332 testing acc: 0.9692\n",
      "epoch: 10 iter: 231 loss: 0.05440 training acc: 0.98404 testing acc: 0.9704\n",
      "epoch: 10 iter: 232 loss: 0.06179 training acc: 0.98138 testing acc: 0.9658\n",
      "epoch: 10 iter: 233 loss: 0.05410 training acc: 0.98454 testing acc: 0.9712\n",
      "epoch: 10 iter: 234 loss: 0.05269 training acc: 0.98524 testing acc: 0.9726\n",
      "epoch: 10 iter: 235 loss: 0.05990 training acc: 0.98284 testing acc: 0.9684\n",
      "epoch: 10 iter: 236 loss: 0.05848 training acc: 0.98290 testing acc: 0.9684\n",
      "epoch: 10 iter: 237 loss: 0.05793 training acc: 0.98320 testing acc: 0.9694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 iter: 238 loss: 0.05912 training acc: 0.98264 testing acc: 0.9698\n",
      "epoch: 10 iter: 239 loss: 0.06145 training acc: 0.98298 testing acc: 0.9714\n",
      "epoch: 10 iter: 240 loss: 0.05865 training acc: 0.98418 testing acc: 0.9718\n",
      "epoch: 10 iter: 241 loss: 0.05598 training acc: 0.98496 testing acc: 0.9732\n",
      "epoch: 10 iter: 242 loss: 0.05606 training acc: 0.98506 testing acc: 0.9728\n",
      "epoch: 10 iter: 243 loss: 0.05233 training acc: 0.98636 testing acc: 0.9726\n",
      "epoch: 10 iter: 244 loss: 0.05253 training acc: 0.98610 testing acc: 0.972\n",
      "epoch: 10 iter: 245 loss: 0.05293 training acc: 0.98594 testing acc: 0.973\n",
      "epoch: 10 iter: 246 loss: 0.05355 training acc: 0.98580 testing acc: 0.9728\n",
      "epoch: 10 iter: 247 loss: 0.05335 training acc: 0.98606 testing acc: 0.973\n",
      "epoch: 10 iter: 248 loss: 0.05661 training acc: 0.98438 testing acc: 0.972\n",
      "epoch: 10 iter: 249 loss: 0.05455 training acc: 0.98492 testing acc: 0.9704\n",
      "epoch: 10 iter: 250 loss: 0.05348 training acc: 0.98554 testing acc: 0.9706\n",
      "epoch: 10 iter: 251 loss: 0.05272 training acc: 0.98574 testing acc: 0.9698\n",
      "epoch: 10 iter: 252 loss: 0.05560 training acc: 0.98516 testing acc: 0.9704\n",
      "epoch: 10 iter: 253 loss: 0.05411 training acc: 0.98562 testing acc: 0.9704\n",
      "epoch: 10 iter: 254 loss: 0.05210 training acc: 0.98624 testing acc: 0.9712\n",
      "epoch: 10 iter: 255 loss: 0.05254 training acc: 0.98590 testing acc: 0.9726\n",
      "epoch: 10 iter: 256 loss: 0.05145 training acc: 0.98616 testing acc: 0.9716\n",
      "epoch: 10 iter: 257 loss: 0.05396 training acc: 0.98476 testing acc: 0.97\n",
      "epoch: 10 iter: 258 loss: 0.05779 training acc: 0.98338 testing acc: 0.968\n",
      "epoch: 10 iter: 259 loss: 0.05936 training acc: 0.98298 testing acc: 0.9692\n",
      "epoch: 10 iter: 260 loss: 0.05158 training acc: 0.98596 testing acc: 0.9726\n",
      "epoch: 10 iter: 261 loss: 0.05164 training acc: 0.98590 testing acc: 0.9718\n",
      "epoch: 10 iter: 262 loss: 0.05178 training acc: 0.98594 testing acc: 0.9732\n",
      "epoch: 10 iter: 263 loss: 0.05109 training acc: 0.98628 testing acc: 0.9738\n",
      "epoch: 10 iter: 264 loss: 0.07535 training acc: 0.97776 testing acc: 0.9622\n",
      "epoch: 10 iter: 265 loss: 0.06022 training acc: 0.98294 testing acc: 0.9694\n",
      "epoch: 10 iter: 266 loss: 0.06404 training acc: 0.98174 testing acc: 0.9676\n",
      "epoch: 10 iter: 267 loss: 0.06375 training acc: 0.98196 testing acc: 0.9676\n",
      "epoch: 10 iter: 268 loss: 0.06329 training acc: 0.98206 testing acc: 0.968\n",
      "epoch: 10 iter: 269 loss: 0.06117 training acc: 0.98222 testing acc: 0.9704\n",
      "epoch: 10 iter: 270 loss: 0.06559 training acc: 0.98032 testing acc: 0.9686\n",
      "epoch: 10 iter: 271 loss: 0.06112 training acc: 0.98230 testing acc: 0.9686\n",
      "epoch: 10 iter: 272 loss: 0.05932 training acc: 0.98246 testing acc: 0.9676\n",
      "epoch: 10 iter: 273 loss: 0.05557 training acc: 0.98412 testing acc: 0.9702\n",
      "epoch: 10 iter: 274 loss: 0.05951 training acc: 0.98300 testing acc: 0.9682\n",
      "epoch: 10 iter: 275 loss: 0.06921 training acc: 0.97896 testing acc: 0.9634\n",
      "epoch: 10 iter: 276 loss: 0.06945 training acc: 0.97866 testing acc: 0.9638\n",
      "epoch: 10 iter: 277 loss: 0.07015 training acc: 0.97894 testing acc: 0.9634\n",
      "epoch: 10 iter: 278 loss: 0.06526 training acc: 0.98024 testing acc: 0.9662\n",
      "epoch: 10 iter: 279 loss: 0.06545 training acc: 0.98054 testing acc: 0.9656\n",
      "epoch: 10 iter: 280 loss: 0.06253 training acc: 0.98166 testing acc: 0.9672\n",
      "epoch: 10 iter: 281 loss: 0.06415 training acc: 0.98074 testing acc: 0.9656\n",
      "epoch: 10 iter: 282 loss: 0.06099 training acc: 0.98208 testing acc: 0.9688\n",
      "epoch: 10 iter: 283 loss: 0.05997 training acc: 0.98242 testing acc: 0.9706\n",
      "epoch: 10 iter: 284 loss: 0.05806 training acc: 0.98340 testing acc: 0.97\n",
      "epoch: 10 iter: 285 loss: 0.05774 training acc: 0.98338 testing acc: 0.9694\n",
      "epoch: 10 iter: 286 loss: 0.05699 training acc: 0.98332 testing acc: 0.9696\n",
      "epoch: 10 iter: 287 loss: 0.05535 training acc: 0.98428 testing acc: 0.971\n",
      "epoch: 10 iter: 288 loss: 0.05606 training acc: 0.98376 testing acc: 0.97\n",
      "epoch: 10 iter: 289 loss: 0.06143 training acc: 0.98170 testing acc: 0.9678\n",
      "epoch: 10 iter: 290 loss: 0.06031 training acc: 0.98268 testing acc: 0.9698\n",
      "epoch: 10 iter: 291 loss: 0.06162 training acc: 0.98240 testing acc: 0.9688\n",
      "epoch: 10 iter: 292 loss: 0.05416 training acc: 0.98496 testing acc: 0.9712\n",
      "epoch: 10 iter: 293 loss: 0.05550 training acc: 0.98468 testing acc: 0.971\n",
      "epoch: 10 iter: 294 loss: 0.05504 training acc: 0.98466 testing acc: 0.9716\n",
      "epoch: 10 iter: 295 loss: 0.05346 training acc: 0.98512 testing acc: 0.9724\n",
      "epoch: 10 iter: 296 loss: 0.05417 training acc: 0.98486 testing acc: 0.9718\n",
      "epoch: 10 iter: 297 loss: 0.05862 training acc: 0.98288 testing acc: 0.968\n",
      "epoch: 10 iter: 298 loss: 0.06882 training acc: 0.97894 testing acc: 0.9646\n",
      "epoch: 10 iter: 299 loss: 0.05854 training acc: 0.98284 testing acc: 0.9694\n",
      "epoch: 10 iter: 300 loss: 0.05935 training acc: 0.98220 testing acc: 0.9704\n",
      "epoch: 10 iter: 301 loss: 0.05965 training acc: 0.98244 testing acc: 0.9682\n",
      "epoch: 10 iter: 302 loss: 0.05755 training acc: 0.98340 testing acc: 0.9694\n",
      "epoch: 10 iter: 303 loss: 0.05603 training acc: 0.98402 testing acc: 0.9708\n",
      "epoch: 10 iter: 304 loss: 0.07382 training acc: 0.97646 testing acc: 0.964\n",
      "epoch: 10 iter: 305 loss: 0.07034 training acc: 0.97822 testing acc: 0.9652\n",
      "epoch: 10 iter: 306 loss: 0.06839 training acc: 0.97892 testing acc: 0.9656\n",
      "epoch: 10 iter: 307 loss: 0.06085 training acc: 0.98158 testing acc: 0.9704\n",
      "epoch: 10 iter: 308 loss: 0.06405 training acc: 0.98054 testing acc: 0.9698\n",
      "epoch: 10 iter: 309 loss: 0.06307 training acc: 0.98076 testing acc: 0.9688\n",
      "epoch: 10 iter: 310 loss: 0.06590 training acc: 0.97966 testing acc: 0.9678\n",
      "epoch: 10 iter: 311 loss: 0.06648 training acc: 0.97954 testing acc: 0.9656\n",
      "epoch: 10 iter: 312 loss: 0.05699 training acc: 0.98302 testing acc: 0.9688\n",
      "epoch: 10 iter: 313 loss: 0.05894 training acc: 0.98254 testing acc: 0.9684\n",
      "epoch: 10 iter: 314 loss: 0.06600 training acc: 0.97990 testing acc: 0.9676\n",
      "epoch: 10 iter: 315 loss: 0.06381 training acc: 0.98110 testing acc: 0.9678\n",
      "epoch: 10 iter: 316 loss: 0.06220 training acc: 0.98164 testing acc: 0.967\n",
      "epoch: 10 iter: 317 loss: 0.05888 training acc: 0.98238 testing acc: 0.9682\n",
      "epoch: 10 iter: 318 loss: 0.05430 training acc: 0.98450 testing acc: 0.9696\n",
      "epoch: 10 iter: 319 loss: 0.05344 training acc: 0.98482 testing acc: 0.9706\n",
      "epoch: 10 iter: 320 loss: 0.05522 training acc: 0.98430 testing acc: 0.9702\n",
      "epoch: 10 iter: 321 loss: 0.05415 training acc: 0.98490 testing acc: 0.9698\n",
      "epoch: 10 iter: 322 loss: 0.05371 training acc: 0.98502 testing acc: 0.9704\n",
      "epoch: 10 iter: 323 loss: 0.05292 training acc: 0.98490 testing acc: 0.971\n",
      "epoch: 10 iter: 324 loss: 0.05320 training acc: 0.98480 testing acc: 0.97\n",
      "epoch: 10 iter: 325 loss: 0.05384 training acc: 0.98460 testing acc: 0.9704\n",
      "epoch: 10 iter: 326 loss: 0.05220 training acc: 0.98522 testing acc: 0.9712\n",
      "epoch: 10 iter: 327 loss: 0.05294 training acc: 0.98510 testing acc: 0.9706\n",
      "epoch: 10 iter: 328 loss: 0.05495 training acc: 0.98504 testing acc: 0.9706\n",
      "epoch: 10 iter: 329 loss: 0.05287 training acc: 0.98536 testing acc: 0.9712\n",
      "epoch: 10 iter: 330 loss: 0.05207 training acc: 0.98560 testing acc: 0.9712\n",
      "epoch: 10 iter: 331 loss: 0.05228 training acc: 0.98534 testing acc: 0.9706\n",
      "epoch: 10 iter: 332 loss: 0.05780 training acc: 0.98352 testing acc: 0.9696\n",
      "epoch: 10 iter: 333 loss: 0.05711 training acc: 0.98340 testing acc: 0.9714\n",
      "epoch: 10 iter: 334 loss: 0.05983 training acc: 0.98244 testing acc: 0.9704\n",
      "epoch: 10 iter: 335 loss: 0.06013 training acc: 0.98208 testing acc: 0.97\n",
      "epoch: 10 iter: 336 loss: 0.05868 training acc: 0.98270 testing acc: 0.97\n",
      "epoch: 10 iter: 337 loss: 0.05846 training acc: 0.98292 testing acc: 0.971\n",
      "epoch: 10 iter: 338 loss: 0.05677 training acc: 0.98366 testing acc: 0.9704\n",
      "epoch: 10 iter: 339 loss: 0.05640 training acc: 0.98368 testing acc: 0.9714\n",
      "epoch: 10 iter: 340 loss: 0.05455 training acc: 0.98458 testing acc: 0.9718\n",
      "epoch: 10 iter: 341 loss: 0.05713 training acc: 0.98344 testing acc: 0.9706\n",
      "epoch: 10 iter: 342 loss: 0.05347 training acc: 0.98506 testing acc: 0.9716\n",
      "epoch: 10 iter: 343 loss: 0.05515 training acc: 0.98402 testing acc: 0.9732\n",
      "epoch: 10 iter: 344 loss: 0.05527 training acc: 0.98390 testing acc: 0.9732\n",
      "epoch: 10 iter: 345 loss: 0.05504 training acc: 0.98442 testing acc: 0.9718\n",
      "epoch: 10 iter: 346 loss: 0.05594 training acc: 0.98446 testing acc: 0.9726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 iter: 347 loss: 0.05426 training acc: 0.98506 testing acc: 0.9738\n",
      "epoch: 10 iter: 348 loss: 0.05952 training acc: 0.98246 testing acc: 0.971\n",
      "epoch: 10 iter: 349 loss: 0.05723 training acc: 0.98352 testing acc: 0.9718\n",
      "epoch: 10 iter: 350 loss: 0.05618 training acc: 0.98370 testing acc: 0.9726\n",
      "epoch: 10 iter: 351 loss: 0.05262 training acc: 0.98528 testing acc: 0.9732\n",
      "epoch: 10 iter: 352 loss: 0.05259 training acc: 0.98544 testing acc: 0.9728\n",
      "epoch: 10 iter: 353 loss: 0.05221 training acc: 0.98542 testing acc: 0.9724\n",
      "epoch: 10 iter: 354 loss: 0.06188 training acc: 0.98168 testing acc: 0.968\n",
      "epoch: 10 iter: 355 loss: 0.05940 training acc: 0.98298 testing acc: 0.9694\n",
      "epoch: 10 iter: 356 loss: 0.05526 training acc: 0.98426 testing acc: 0.9722\n",
      "epoch: 10 iter: 357 loss: 0.05454 training acc: 0.98410 testing acc: 0.9722\n",
      "epoch: 10 iter: 358 loss: 0.05549 training acc: 0.98388 testing acc: 0.9724\n",
      "epoch: 10 iter: 359 loss: 0.06532 training acc: 0.98018 testing acc: 0.9678\n",
      "epoch: 10 iter: 360 loss: 0.06280 training acc: 0.98104 testing acc: 0.9696\n",
      "epoch: 10 iter: 361 loss: 0.06260 training acc: 0.98148 testing acc: 0.9692\n",
      "epoch: 10 iter: 362 loss: 0.05899 training acc: 0.98270 testing acc: 0.9716\n",
      "epoch: 10 iter: 363 loss: 0.05796 training acc: 0.98322 testing acc: 0.9722\n",
      "epoch: 10 iter: 364 loss: 0.05759 training acc: 0.98344 testing acc: 0.9712\n",
      "epoch: 10 iter: 365 loss: 0.05792 training acc: 0.98342 testing acc: 0.971\n",
      "epoch: 10 iter: 366 loss: 0.05901 training acc: 0.98266 testing acc: 0.9702\n",
      "epoch: 10 iter: 367 loss: 0.05751 training acc: 0.98318 testing acc: 0.9718\n",
      "epoch: 10 iter: 368 loss: 0.05712 training acc: 0.98346 testing acc: 0.9738\n",
      "epoch: 10 iter: 369 loss: 0.06761 training acc: 0.97964 testing acc: 0.9694\n",
      "epoch: 10 iter: 370 loss: 0.08663 training acc: 0.97296 testing acc: 0.9616\n",
      "epoch: 10 iter: 371 loss: 0.08156 training acc: 0.97512 testing acc: 0.9648\n",
      "epoch: 10 iter: 372 loss: 0.07867 training acc: 0.97600 testing acc: 0.9664\n",
      "epoch: 10 iter: 373 loss: 0.07462 training acc: 0.97694 testing acc: 0.9662\n",
      "epoch: 10 iter: 374 loss: 0.06459 training acc: 0.98050 testing acc: 0.9688\n",
      "epoch: 10 iter: 375 loss: 0.06385 training acc: 0.98104 testing acc: 0.9688\n",
      "epoch: 10 iter: 376 loss: 0.06283 training acc: 0.98156 testing acc: 0.9698\n",
      "epoch: 10 iter: 377 loss: 0.06619 training acc: 0.97998 testing acc: 0.9694\n",
      "epoch: 10 iter: 378 loss: 0.06473 training acc: 0.98046 testing acc: 0.97\n",
      "epoch: 10 iter: 379 loss: 0.05920 training acc: 0.98200 testing acc: 0.9712\n",
      "epoch: 10 iter: 380 loss: 0.05699 training acc: 0.98334 testing acc: 0.972\n",
      "epoch: 10 iter: 381 loss: 0.05908 training acc: 0.98228 testing acc: 0.9706\n",
      "epoch: 10 iter: 382 loss: 0.05849 training acc: 0.98266 testing acc: 0.9706\n",
      "epoch: 10 iter: 383 loss: 0.05855 training acc: 0.98260 testing acc: 0.9692\n",
      "epoch: 10 iter: 384 loss: 0.05638 training acc: 0.98356 testing acc: 0.9708\n",
      "epoch: 10 iter: 385 loss: 0.05942 training acc: 0.98270 testing acc: 0.968\n",
      "epoch: 10 iter: 386 loss: 0.05758 training acc: 0.98370 testing acc: 0.9728\n",
      "epoch: 10 iter: 387 loss: 0.05576 training acc: 0.98452 testing acc: 0.9742\n",
      "epoch: 10 iter: 388 loss: 0.05580 training acc: 0.98418 testing acc: 0.9726\n",
      "epoch: 10 iter: 389 loss: 0.05951 training acc: 0.98238 testing acc: 0.9694\n",
      "epoch: 10 iter: 390 loss: 0.05409 training acc: 0.98470 testing acc: 0.9738\n",
      "epoch: 10 iter: 391 loss: 0.05707 training acc: 0.98412 testing acc: 0.972\n",
      "epoch: 10 iter: 392 loss: 0.05787 training acc: 0.98370 testing acc: 0.9712\n",
      "epoch: 10 iter: 393 loss: 0.05643 training acc: 0.98432 testing acc: 0.9722\n",
      "epoch: 10 iter: 394 loss: 0.06344 training acc: 0.98142 testing acc: 0.9692\n",
      "epoch: 10 iter: 395 loss: 0.05725 training acc: 0.98372 testing acc: 0.9732\n",
      "epoch: 10 iter: 396 loss: 0.05619 training acc: 0.98406 testing acc: 0.9738\n",
      "epoch: 10 iter: 397 loss: 0.05480 training acc: 0.98450 testing acc: 0.9726\n",
      "epoch: 10 iter: 398 loss: 0.05583 training acc: 0.98376 testing acc: 0.9716\n",
      "epoch: 10 iter: 399 loss: 0.05549 training acc: 0.98410 testing acc: 0.9716\n",
      "epoch: 10 iter: 400 loss: 0.05595 training acc: 0.98376 testing acc: 0.9702\n",
      "epoch: 10 iter: 401 loss: 0.05253 training acc: 0.98526 testing acc: 0.9732\n",
      "epoch: 10 iter: 402 loss: 0.05323 training acc: 0.98510 testing acc: 0.9732\n",
      "epoch: 10 iter: 403 loss: 0.05091 training acc: 0.98612 testing acc: 0.9752\n",
      "epoch: 10 iter: 404 loss: 0.05020 training acc: 0.98628 testing acc: 0.9738\n",
      "epoch: 10 iter: 405 loss: 0.05996 training acc: 0.98268 testing acc: 0.9688\n",
      "epoch: 10 iter: 406 loss: 0.05943 training acc: 0.98274 testing acc: 0.9688\n",
      "epoch: 10 iter: 407 loss: 0.06414 training acc: 0.98114 testing acc: 0.9692\n",
      "epoch: 10 iter: 408 loss: 0.07115 training acc: 0.97876 testing acc: 0.9654\n",
      "epoch: 10 iter: 409 loss: 0.06291 training acc: 0.98166 testing acc: 0.9686\n",
      "epoch: 10 iter: 410 loss: 0.06161 training acc: 0.98220 testing acc: 0.9686\n",
      "epoch: 10 iter: 411 loss: 0.05861 training acc: 0.98316 testing acc: 0.9704\n",
      "epoch: 10 iter: 412 loss: 0.05853 training acc: 0.98330 testing acc: 0.97\n",
      "epoch: 10 iter: 413 loss: 0.05865 training acc: 0.98312 testing acc: 0.97\n",
      "epoch: 10 iter: 414 loss: 0.05890 training acc: 0.98294 testing acc: 0.9686\n",
      "epoch: 10 iter: 415 loss: 0.05690 training acc: 0.98366 testing acc: 0.9688\n",
      "epoch: 10 iter: 416 loss: 0.05932 training acc: 0.98272 testing acc: 0.9702\n",
      "epoch: 10 iter: 417 loss: 0.05451 training acc: 0.98432 testing acc: 0.9698\n",
      "epoch: 10 iter: 418 loss: 0.05527 training acc: 0.98402 testing acc: 0.9692\n",
      "epoch: 10 iter: 419 loss: 0.05548 training acc: 0.98386 testing acc: 0.969\n",
      "epoch: 10 iter: 420 loss: 0.05711 training acc: 0.98298 testing acc: 0.971\n",
      "epoch: 10 iter: 421 loss: 0.05578 training acc: 0.98352 testing acc: 0.9706\n",
      "epoch: 10 iter: 422 loss: 0.05505 training acc: 0.98388 testing acc: 0.9718\n",
      "epoch: 10 iter: 423 loss: 0.05440 training acc: 0.98416 testing acc: 0.9712\n",
      "epoch: 10 iter: 424 loss: 0.05134 training acc: 0.98520 testing acc: 0.9716\n",
      "epoch: 10 iter: 425 loss: 0.05651 training acc: 0.98360 testing acc: 0.971\n",
      "epoch: 10 iter: 426 loss: 0.05694 training acc: 0.98304 testing acc: 0.9714\n",
      "epoch: 10 iter: 427 loss: 0.05507 training acc: 0.98358 testing acc: 0.9718\n",
      "epoch: 10 iter: 428 loss: 0.05467 training acc: 0.98400 testing acc: 0.9734\n",
      "epoch: 10 iter: 429 loss: 0.05838 training acc: 0.98252 testing acc: 0.9708\n",
      "epoch: 10 iter: 430 loss: 0.06044 training acc: 0.98260 testing acc: 0.9668\n",
      "epoch: 10 iter: 431 loss: 0.05840 training acc: 0.98272 testing acc: 0.9672\n",
      "epoch: 10 iter: 432 loss: 0.05708 training acc: 0.98346 testing acc: 0.9682\n",
      "epoch: 10 iter: 433 loss: 0.06789 training acc: 0.97946 testing acc: 0.9632\n",
      "epoch: 10 iter: 434 loss: 0.06194 training acc: 0.98182 testing acc: 0.9652\n",
      "epoch: 10 iter: 435 loss: 0.05781 training acc: 0.98336 testing acc: 0.9686\n",
      "epoch: 10 iter: 436 loss: 0.05592 training acc: 0.98428 testing acc: 0.9702\n",
      "epoch: 10 iter: 437 loss: 0.05523 training acc: 0.98440 testing acc: 0.97\n",
      "epoch: 10 iter: 438 loss: 0.05441 training acc: 0.98488 testing acc: 0.9708\n",
      "epoch: 10 iter: 439 loss: 0.05385 training acc: 0.98474 testing acc: 0.9704\n",
      "epoch: 10 iter: 440 loss: 0.05364 training acc: 0.98496 testing acc: 0.9706\n",
      "epoch: 10 iter: 441 loss: 0.05410 training acc: 0.98468 testing acc: 0.9716\n",
      "epoch: 10 iter: 442 loss: 0.05612 training acc: 0.98366 testing acc: 0.9702\n",
      "epoch: 10 iter: 443 loss: 0.07269 training acc: 0.97784 testing acc: 0.9626\n",
      "epoch: 10 iter: 444 loss: 0.07320 training acc: 0.97740 testing acc: 0.9632\n",
      "epoch: 10 iter: 445 loss: 0.07243 training acc: 0.97774 testing acc: 0.9612\n",
      "epoch: 10 iter: 446 loss: 0.07850 training acc: 0.97556 testing acc: 0.9592\n",
      "epoch: 10 iter: 447 loss: 0.05311 training acc: 0.98500 testing acc: 0.972\n",
      "epoch: 10 iter: 448 loss: 0.05302 training acc: 0.98494 testing acc: 0.9722\n",
      "epoch: 10 iter: 449 loss: 0.05253 training acc: 0.98498 testing acc: 0.9722\n",
      "epoch: 10 iter: 450 loss: 0.05584 training acc: 0.98388 testing acc: 0.9708\n",
      "epoch: 10 iter: 451 loss: 0.05585 training acc: 0.98388 testing acc: 0.9706\n",
      "epoch: 10 iter: 452 loss: 0.05929 training acc: 0.98216 testing acc: 0.9676\n",
      "epoch: 10 iter: 453 loss: 0.05699 training acc: 0.98308 testing acc: 0.9666\n",
      "epoch: 10 iter: 454 loss: 0.05595 training acc: 0.98330 testing acc: 0.9662\n",
      "epoch: 10 iter: 455 loss: 0.05656 training acc: 0.98340 testing acc: 0.9674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 iter: 456 loss: 0.05833 training acc: 0.98262 testing acc: 0.9688\n",
      "epoch: 10 iter: 457 loss: 0.05600 training acc: 0.98340 testing acc: 0.9668\n",
      "epoch: 10 iter: 458 loss: 0.06525 training acc: 0.98066 testing acc: 0.9662\n",
      "epoch: 10 iter: 459 loss: 0.06332 training acc: 0.98114 testing acc: 0.9688\n",
      "epoch: 10 iter: 460 loss: 0.06114 training acc: 0.98192 testing acc: 0.9698\n",
      "epoch: 10 iter: 461 loss: 0.05925 training acc: 0.98272 testing acc: 0.9694\n",
      "epoch: 10 iter: 462 loss: 0.05574 training acc: 0.98388 testing acc: 0.9692\n",
      "epoch: 10 iter: 463 loss: 0.05630 training acc: 0.98364 testing acc: 0.968\n",
      "epoch: 10 iter: 464 loss: 0.05629 training acc: 0.98358 testing acc: 0.9678\n",
      "epoch: 10 iter: 465 loss: 0.05905 training acc: 0.98230 testing acc: 0.9672\n",
      "epoch: 10 iter: 466 loss: 0.06548 training acc: 0.97964 testing acc: 0.9646\n",
      "epoch: 10 iter: 467 loss: 0.06150 training acc: 0.98130 testing acc: 0.9662\n",
      "epoch: 10 iter: 468 loss: 0.06005 training acc: 0.98222 testing acc: 0.968\n",
      "epoch: 10 iter: 469 loss: 0.05768 training acc: 0.98312 testing acc: 0.9686\n",
      "epoch: 10 iter: 470 loss: 0.05847 training acc: 0.98284 testing acc: 0.9674\n",
      "epoch: 10 iter: 471 loss: 0.05753 training acc: 0.98276 testing acc: 0.9664\n",
      "epoch: 10 iter: 472 loss: 0.05512 training acc: 0.98422 testing acc: 0.97\n",
      "epoch: 10 iter: 473 loss: 0.05374 training acc: 0.98394 testing acc: 0.9684\n",
      "epoch: 10 iter: 474 loss: 0.06006 training acc: 0.98194 testing acc: 0.9676\n",
      "epoch: 10 iter: 475 loss: 0.05930 training acc: 0.98212 testing acc: 0.9678\n",
      "epoch: 10 iter: 476 loss: 0.05600 training acc: 0.98334 testing acc: 0.9684\n",
      "epoch: 10 iter: 477 loss: 0.05488 training acc: 0.98372 testing acc: 0.968\n",
      "epoch: 10 iter: 478 loss: 0.05507 training acc: 0.98374 testing acc: 0.9686\n",
      "epoch: 10 iter: 479 loss: 0.06052 training acc: 0.98236 testing acc: 0.969\n",
      "epoch: 10 iter: 480 loss: 0.05757 training acc: 0.98322 testing acc: 0.9688\n",
      "epoch: 10 iter: 481 loss: 0.06068 training acc: 0.98236 testing acc: 0.9662\n",
      "epoch: 10 iter: 482 loss: 0.06401 training acc: 0.98062 testing acc: 0.9662\n",
      "epoch: 10 iter: 483 loss: 0.05933 training acc: 0.98240 testing acc: 0.967\n",
      "epoch: 10 iter: 484 loss: 0.06279 training acc: 0.98090 testing acc: 0.9662\n",
      "epoch: 10 iter: 485 loss: 0.06893 training acc: 0.97846 testing acc: 0.9646\n",
      "epoch: 10 iter: 486 loss: 0.06418 training acc: 0.98052 testing acc: 0.9656\n",
      "epoch: 10 iter: 487 loss: 0.08431 training acc: 0.97280 testing acc: 0.9598\n",
      "epoch: 10 iter: 488 loss: 0.07220 training acc: 0.97742 testing acc: 0.9636\n",
      "epoch: 10 iter: 489 loss: 0.06900 training acc: 0.97854 testing acc: 0.9658\n",
      "epoch: 10 iter: 490 loss: 0.06661 training acc: 0.97920 testing acc: 0.9658\n",
      "epoch: 10 iter: 491 loss: 0.06587 training acc: 0.97976 testing acc: 0.9678\n",
      "epoch: 10 iter: 492 loss: 0.06392 training acc: 0.98050 testing acc: 0.9678\n",
      "epoch: 10 iter: 493 loss: 0.05899 training acc: 0.98280 testing acc: 0.9694\n",
      "epoch: 10 iter: 494 loss: 0.06324 training acc: 0.98110 testing acc: 0.968\n",
      "epoch: 10 iter: 495 loss: 0.06017 training acc: 0.98186 testing acc: 0.9686\n",
      "epoch: 10 iter: 496 loss: 0.05435 training acc: 0.98452 testing acc: 0.9666\n",
      "epoch: 10 iter: 497 loss: 0.05702 training acc: 0.98304 testing acc: 0.9656\n",
      "epoch: 10 iter: 498 loss: 0.05663 training acc: 0.98312 testing acc: 0.967\n",
      "epoch: 10 iter: 499 loss: 0.06424 training acc: 0.98040 testing acc: 0.9676\n",
      "epoch: 10 iter: 500 loss: 0.06367 training acc: 0.98074 testing acc: 0.9678\n",
      "epoch: 10 iter: 501 loss: 0.06017 training acc: 0.98186 testing acc: 0.9692\n",
      "epoch: 10 iter: 502 loss: 0.08236 training acc: 0.97428 testing acc: 0.9596\n",
      "epoch: 10 iter: 503 loss: 0.07754 training acc: 0.97608 testing acc: 0.9618\n",
      "epoch: 10 iter: 504 loss: 0.06224 training acc: 0.98090 testing acc: 0.967\n",
      "epoch: 10 iter: 505 loss: 0.05666 training acc: 0.98292 testing acc: 0.9692\n",
      "epoch: 10 iter: 506 loss: 0.05840 training acc: 0.98238 testing acc: 0.9682\n",
      "epoch: 10 iter: 507 loss: 0.06679 training acc: 0.97878 testing acc: 0.9666\n",
      "epoch: 10 iter: 508 loss: 0.06449 training acc: 0.97974 testing acc: 0.9664\n",
      "epoch: 10 iter: 509 loss: 0.06290 training acc: 0.97986 testing acc: 0.9674\n",
      "epoch: 10 iter: 510 loss: 0.06086 training acc: 0.98072 testing acc: 0.9676\n",
      "epoch: 10 iter: 511 loss: 0.05994 training acc: 0.98146 testing acc: 0.9674\n",
      "epoch: 10 iter: 512 loss: 0.06030 training acc: 0.98152 testing acc: 0.9674\n",
      "epoch: 10 iter: 513 loss: 0.05948 training acc: 0.98136 testing acc: 0.9676\n",
      "epoch: 10 iter: 514 loss: 0.05835 training acc: 0.98188 testing acc: 0.9676\n",
      "epoch: 10 iter: 515 loss: 0.05770 training acc: 0.98226 testing acc: 0.968\n",
      "epoch: 10 iter: 516 loss: 0.05808 training acc: 0.98228 testing acc: 0.9684\n",
      "epoch: 10 iter: 517 loss: 0.05829 training acc: 0.98200 testing acc: 0.9692\n",
      "epoch: 10 iter: 518 loss: 0.06378 training acc: 0.98006 testing acc: 0.9676\n",
      "epoch: 10 iter: 519 loss: 0.05910 training acc: 0.98202 testing acc: 0.9688\n",
      "epoch: 10 iter: 520 loss: 0.05877 training acc: 0.98196 testing acc: 0.9694\n",
      "epoch: 10 iter: 521 loss: 0.06055 training acc: 0.98188 testing acc: 0.9678\n",
      "epoch: 10 iter: 522 loss: 0.05434 training acc: 0.98360 testing acc: 0.9684\n",
      "epoch: 10 iter: 523 loss: 0.05604 training acc: 0.98304 testing acc: 0.9688\n",
      "epoch: 10 iter: 524 loss: 0.05622 training acc: 0.98276 testing acc: 0.968\n",
      "epoch: 10 iter: 525 loss: 0.05466 training acc: 0.98384 testing acc: 0.967\n",
      "epoch: 10 iter: 526 loss: 0.05416 training acc: 0.98404 testing acc: 0.9684\n",
      "epoch: 10 iter: 527 loss: 0.05713 training acc: 0.98292 testing acc: 0.967\n",
      "epoch: 10 iter: 528 loss: 0.05694 training acc: 0.98278 testing acc: 0.969\n",
      "epoch: 10 iter: 529 loss: 0.05669 training acc: 0.98280 testing acc: 0.9692\n",
      "epoch: 10 iter: 530 loss: 0.06378 training acc: 0.98088 testing acc: 0.9672\n",
      "epoch: 10 iter: 531 loss: 0.06050 training acc: 0.98162 testing acc: 0.9684\n",
      "epoch: 10 iter: 532 loss: 0.05679 training acc: 0.98284 testing acc: 0.9688\n",
      "epoch: 10 iter: 533 loss: 0.05600 training acc: 0.98298 testing acc: 0.969\n",
      "epoch: 10 iter: 534 loss: 0.05556 training acc: 0.98328 testing acc: 0.9692\n",
      "epoch: 10 iter: 535 loss: 0.06180 training acc: 0.98040 testing acc: 0.964\n",
      "epoch: 10 iter: 536 loss: 0.05570 training acc: 0.98314 testing acc: 0.9668\n",
      "epoch: 10 iter: 537 loss: 0.05378 training acc: 0.98404 testing acc: 0.9704\n",
      "epoch: 10 iter: 538 loss: 0.05357 training acc: 0.98424 testing acc: 0.9702\n",
      "epoch: 10 iter: 539 loss: 0.05540 training acc: 0.98364 testing acc: 0.9698\n",
      "epoch: 10 iter: 540 loss: 0.05523 training acc: 0.98346 testing acc: 0.9704\n",
      "epoch: 10 iter: 541 loss: 0.05582 training acc: 0.98362 testing acc: 0.9714\n",
      "epoch: 10 iter: 542 loss: 0.05409 training acc: 0.98386 testing acc: 0.9722\n",
      "epoch: 10 iter: 543 loss: 0.05480 training acc: 0.98390 testing acc: 0.9692\n",
      "epoch: 10 iter: 544 loss: 0.05609 training acc: 0.98302 testing acc: 0.9684\n",
      "epoch: 10 iter: 545 loss: 0.05508 training acc: 0.98374 testing acc: 0.9714\n",
      "epoch: 10 iter: 546 loss: 0.06162 training acc: 0.98104 testing acc: 0.9688\n",
      "epoch: 10 iter: 547 loss: 0.05841 training acc: 0.98202 testing acc: 0.97\n",
      "epoch: 10 iter: 548 loss: 0.05914 training acc: 0.98210 testing acc: 0.9684\n",
      "epoch: 10 iter: 549 loss: 0.05594 training acc: 0.98340 testing acc: 0.9688\n",
      "epoch: 10 iter: 550 loss: 0.05549 training acc: 0.98370 testing acc: 0.969\n",
      "epoch: 10 iter: 551 loss: 0.05231 training acc: 0.98460 testing acc: 0.9714\n",
      "epoch: 10 iter: 552 loss: 0.05507 training acc: 0.98380 testing acc: 0.9672\n",
      "epoch: 10 iter: 553 loss: 0.05935 training acc: 0.98258 testing acc: 0.968\n",
      "epoch: 10 iter: 554 loss: 0.06030 training acc: 0.98218 testing acc: 0.9664\n",
      "epoch: 10 iter: 555 loss: 0.07141 training acc: 0.97762 testing acc: 0.9608\n",
      "epoch: 10 iter: 556 loss: 0.05091 training acc: 0.98574 testing acc: 0.9716\n",
      "epoch: 10 iter: 557 loss: 0.05253 training acc: 0.98498 testing acc: 0.9714\n",
      "epoch: 10 iter: 558 loss: 0.05552 training acc: 0.98400 testing acc: 0.9704\n",
      "epoch: 10 iter: 559 loss: 0.05275 training acc: 0.98538 testing acc: 0.9706\n",
      "epoch: 10 iter: 560 loss: 0.07815 training acc: 0.97480 testing acc: 0.9618\n",
      "epoch: 10 iter: 561 loss: 0.07544 training acc: 0.97620 testing acc: 0.9628\n",
      "epoch: 10 iter: 562 loss: 0.07126 training acc: 0.97808 testing acc: 0.9666\n",
      "epoch: 10 iter: 563 loss: 0.06991 training acc: 0.97864 testing acc: 0.9676\n",
      "epoch: 10 iter: 564 loss: 0.06843 training acc: 0.97942 testing acc: 0.9672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 iter: 565 loss: 0.05868 training acc: 0.98270 testing acc: 0.97\n",
      "epoch: 10 iter: 566 loss: 0.05658 training acc: 0.98356 testing acc: 0.9708\n",
      "epoch: 10 iter: 567 loss: 0.05927 training acc: 0.98278 testing acc: 0.97\n",
      "epoch: 10 iter: 568 loss: 0.05609 training acc: 0.98386 testing acc: 0.9696\n",
      "epoch: 10 iter: 569 loss: 0.05881 training acc: 0.98324 testing acc: 0.9694\n",
      "epoch: 10 iter: 570 loss: 0.05790 training acc: 0.98348 testing acc: 0.969\n",
      "epoch: 10 iter: 571 loss: 0.06344 training acc: 0.98142 testing acc: 0.9684\n",
      "epoch: 10 iter: 572 loss: 0.05604 training acc: 0.98376 testing acc: 0.969\n",
      "epoch: 10 iter: 573 loss: 0.05256 training acc: 0.98512 testing acc: 0.9694\n",
      "epoch: 10 iter: 574 loss: 0.05553 training acc: 0.98382 testing acc: 0.9688\n",
      "epoch: 10 iter: 575 loss: 0.05779 training acc: 0.98284 testing acc: 0.9668\n",
      "epoch: 10 iter: 576 loss: 0.05443 training acc: 0.98424 testing acc: 0.9688\n",
      "epoch: 10 iter: 577 loss: 0.05440 training acc: 0.98448 testing acc: 0.9682\n",
      "epoch: 10 iter: 578 loss: 0.05245 training acc: 0.98528 testing acc: 0.9704\n",
      "epoch: 10 iter: 579 loss: 0.05385 training acc: 0.98492 testing acc: 0.967\n",
      "epoch: 10 iter: 580 loss: 0.05352 training acc: 0.98482 testing acc: 0.967\n",
      "epoch: 10 iter: 581 loss: 0.05352 training acc: 0.98458 testing acc: 0.9686\n",
      "epoch: 10 iter: 582 loss: 0.05954 training acc: 0.98206 testing acc: 0.965\n",
      "epoch: 10 iter: 583 loss: 0.06908 training acc: 0.97890 testing acc: 0.9648\n",
      "epoch: 10 iter: 584 loss: 0.05836 training acc: 0.98296 testing acc: 0.968\n",
      "epoch: 10 iter: 585 loss: 0.06144 training acc: 0.98182 testing acc: 0.9668\n",
      "epoch: 10 iter: 586 loss: 0.08330 training acc: 0.97278 testing acc: 0.9586\n",
      "epoch: 10 iter: 587 loss: 0.06187 training acc: 0.98118 testing acc: 0.9654\n",
      "epoch: 10 iter: 588 loss: 0.05709 training acc: 0.98276 testing acc: 0.967\n",
      "epoch: 10 iter: 589 loss: 0.05604 training acc: 0.98312 testing acc: 0.9676\n",
      "epoch: 10 iter: 590 loss: 0.05515 training acc: 0.98340 testing acc: 0.9684\n",
      "epoch: 10 iter: 591 loss: 0.05717 training acc: 0.98288 testing acc: 0.967\n",
      "epoch: 10 iter: 592 loss: 0.05535 training acc: 0.98330 testing acc: 0.9678\n",
      "epoch: 10 iter: 593 loss: 0.05484 training acc: 0.98358 testing acc: 0.9668\n",
      "epoch: 10 iter: 594 loss: 0.05561 training acc: 0.98342 testing acc: 0.9678\n",
      "epoch: 10 iter: 595 loss: 0.05416 training acc: 0.98426 testing acc: 0.9676\n",
      "epoch: 10 iter: 596 loss: 0.06040 training acc: 0.98184 testing acc: 0.9654\n",
      "epoch: 10 iter: 597 loss: 0.05636 training acc: 0.98354 testing acc: 0.9678\n",
      "epoch: 10 iter: 598 loss: 0.05428 training acc: 0.98440 testing acc: 0.9704\n",
      "epoch: 10 iter: 599 loss: 0.05399 training acc: 0.98436 testing acc: 0.9688\n",
      "epoch: 10 iter: 600 loss: 0.05332 training acc: 0.98492 testing acc: 0.971\n",
      "epoch: 10 iter: 601 loss: 0.06095 training acc: 0.98216 testing acc: 0.9668\n",
      "epoch: 10 iter: 602 loss: 0.05311 training acc: 0.98446 testing acc: 0.9698\n",
      "epoch: 10 iter: 603 loss: 0.05743 training acc: 0.98342 testing acc: 0.969\n",
      "epoch: 10 iter: 604 loss: 0.05577 training acc: 0.98400 testing acc: 0.9688\n",
      "epoch: 10 iter: 605 loss: 0.06067 training acc: 0.98200 testing acc: 0.9668\n",
      "epoch: 10 iter: 606 loss: 0.05346 training acc: 0.98490 testing acc: 0.9702\n",
      "epoch: 10 iter: 607 loss: 0.05486 training acc: 0.98450 testing acc: 0.9706\n",
      "epoch: 10 iter: 608 loss: 0.05159 training acc: 0.98532 testing acc: 0.9724\n",
      "epoch: 10 iter: 609 loss: 0.05599 training acc: 0.98396 testing acc: 0.9686\n",
      "epoch: 10 iter: 610 loss: 0.05522 training acc: 0.98376 testing acc: 0.9678\n",
      "epoch: 10 iter: 611 loss: 0.05372 training acc: 0.98442 testing acc: 0.9686\n",
      "epoch: 10 iter: 612 loss: 0.05062 training acc: 0.98530 testing acc: 0.9708\n",
      "epoch: 10 iter: 613 loss: 0.05106 training acc: 0.98516 testing acc: 0.9714\n",
      "epoch: 10 iter: 614 loss: 0.05575 training acc: 0.98314 testing acc: 0.969\n",
      "epoch: 10 iter: 615 loss: 0.05523 training acc: 0.98362 testing acc: 0.97\n",
      "epoch: 10 iter: 616 loss: 0.11268 training acc: 0.96330 testing acc: 0.9494\n",
      "epoch: 10 iter: 617 loss: 0.07213 training acc: 0.97762 testing acc: 0.962\n",
      "epoch: 10 iter: 618 loss: 0.06042 training acc: 0.98196 testing acc: 0.9666\n",
      "epoch: 10 iter: 619 loss: 0.05947 training acc: 0.98224 testing acc: 0.9674\n",
      "epoch: 10 iter: 620 loss: 0.05815 training acc: 0.98276 testing acc: 0.9678\n",
      "epoch: 10 iter: 621 loss: 0.07732 training acc: 0.97550 testing acc: 0.9602\n",
      "epoch: 10 iter: 622 loss: 0.06529 training acc: 0.98014 testing acc: 0.9648\n",
      "epoch: 10 iter: 623 loss: 0.07032 training acc: 0.97860 testing acc: 0.9636\n",
      "epoch: 10 iter: 624 loss: 0.07082 training acc: 0.97806 testing acc: 0.963\n",
      "epoch: 10 iter: 625 loss: 0.07521 training acc: 0.97630 testing acc: 0.966\n",
      "epoch: 10 iter: 626 loss: 0.07565 training acc: 0.97620 testing acc: 0.9644\n",
      "epoch: 10 iter: 627 loss: 0.07427 training acc: 0.97608 testing acc: 0.9642\n",
      "epoch: 10 iter: 628 loss: 0.06892 training acc: 0.97816 testing acc: 0.965\n",
      "epoch: 10 iter: 629 loss: 0.06508 training acc: 0.97968 testing acc: 0.9666\n",
      "epoch: 10 iter: 630 loss: 0.06005 training acc: 0.98168 testing acc: 0.9688\n",
      "epoch: 10 iter: 631 loss: 0.05794 training acc: 0.98226 testing acc: 0.9684\n",
      "epoch: 10 iter: 632 loss: 0.05761 training acc: 0.98248 testing acc: 0.967\n",
      "epoch: 10 iter: 633 loss: 0.05658 training acc: 0.98292 testing acc: 0.969\n",
      "epoch: 10 iter: 634 loss: 0.05430 training acc: 0.98380 testing acc: 0.9712\n",
      "epoch: 10 iter: 635 loss: 0.05643 training acc: 0.98314 testing acc: 0.9702\n",
      "epoch: 10 iter: 636 loss: 0.06281 training acc: 0.98118 testing acc: 0.969\n",
      "epoch: 10 iter: 637 loss: 0.05503 training acc: 0.98430 testing acc: 0.9702\n",
      "epoch: 10 iter: 638 loss: 0.05481 training acc: 0.98450 testing acc: 0.968\n",
      "epoch: 10 iter: 639 loss: 0.05499 training acc: 0.98446 testing acc: 0.9688\n",
      "epoch: 10 iter: 640 loss: 0.05652 training acc: 0.98396 testing acc: 0.969\n",
      "epoch: 10 iter: 641 loss: 0.06718 training acc: 0.97970 testing acc: 0.966\n",
      "epoch: 10 iter: 642 loss: 0.06467 training acc: 0.98106 testing acc: 0.9666\n",
      "epoch: 10 iter: 643 loss: 0.05738 training acc: 0.98336 testing acc: 0.971\n",
      "epoch: 10 iter: 644 loss: 0.05575 training acc: 0.98424 testing acc: 0.9724\n",
      "epoch: 10 iter: 645 loss: 0.05573 training acc: 0.98426 testing acc: 0.9724\n",
      "epoch: 10 iter: 646 loss: 0.05648 training acc: 0.98396 testing acc: 0.972\n",
      "epoch: 10 iter: 647 loss: 0.05622 training acc: 0.98370 testing acc: 0.972\n",
      "epoch: 10 iter: 648 loss: 0.05645 training acc: 0.98372 testing acc: 0.9724\n",
      "epoch: 10 iter: 649 loss: 0.05401 training acc: 0.98466 testing acc: 0.9722\n",
      "epoch: 10 iter: 650 loss: 0.05419 training acc: 0.98470 testing acc: 0.971\n",
      "epoch: 10 iter: 651 loss: 0.05599 training acc: 0.98392 testing acc: 0.972\n",
      "epoch: 10 iter: 652 loss: 0.06267 training acc: 0.98152 testing acc: 0.97\n",
      "epoch: 10 iter: 653 loss: 0.06048 training acc: 0.98236 testing acc: 0.9696\n",
      "epoch: 10 iter: 654 loss: 0.06009 training acc: 0.98256 testing acc: 0.9686\n",
      "epoch: 10 iter: 655 loss: 0.05455 training acc: 0.98442 testing acc: 0.971\n",
      "epoch: 10 iter: 656 loss: 0.05224 training acc: 0.98516 testing acc: 0.9716\n",
      "epoch: 10 iter: 657 loss: 0.05297 training acc: 0.98514 testing acc: 0.9704\n",
      "epoch: 10 iter: 658 loss: 0.05126 training acc: 0.98582 testing acc: 0.9708\n",
      "epoch: 10 iter: 659 loss: 0.05214 training acc: 0.98532 testing acc: 0.9702\n",
      "epoch: 10 iter: 660 loss: 0.05222 training acc: 0.98516 testing acc: 0.9706\n",
      "epoch: 10 iter: 661 loss: 0.05264 training acc: 0.98516 testing acc: 0.9696\n",
      "epoch: 10 iter: 662 loss: 0.05732 training acc: 0.98340 testing acc: 0.9684\n",
      "epoch: 10 iter: 663 loss: 0.05438 training acc: 0.98442 testing acc: 0.9704\n",
      "epoch: 10 iter: 664 loss: 0.05908 training acc: 0.98202 testing acc: 0.9702\n",
      "epoch: 10 iter: 665 loss: 0.05862 training acc: 0.98260 testing acc: 0.9716\n",
      "epoch: 10 iter: 666 loss: 0.06129 training acc: 0.98136 testing acc: 0.9704\n",
      "epoch: 10 iter: 667 loss: 0.05569 training acc: 0.98340 testing acc: 0.9712\n",
      "epoch: 10 iter: 668 loss: 0.05292 training acc: 0.98494 testing acc: 0.9716\n",
      "epoch: 10 iter: 669 loss: 0.05400 training acc: 0.98444 testing acc: 0.9714\n",
      "epoch: 10 iter: 670 loss: 0.05327 training acc: 0.98480 testing acc: 0.971\n",
      "epoch: 10 iter: 671 loss: 0.05550 training acc: 0.98412 testing acc: 0.9704\n",
      "epoch: 10 iter: 672 loss: 0.06483 training acc: 0.98074 testing acc: 0.9682\n",
      "epoch: 10 iter: 673 loss: 0.05680 training acc: 0.98322 testing acc: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 iter: 674 loss: 0.06137 training acc: 0.98246 testing acc: 0.9702\n",
      "epoch: 10 iter: 675 loss: 0.05844 training acc: 0.98286 testing acc: 0.9694\n",
      "epoch: 10 iter: 676 loss: 0.06025 training acc: 0.98194 testing acc: 0.968\n",
      "epoch: 10 iter: 677 loss: 0.06028 training acc: 0.98186 testing acc: 0.9668\n",
      "epoch: 10 iter: 678 loss: 0.06027 training acc: 0.98180 testing acc: 0.966\n",
      "epoch: 10 iter: 679 loss: 0.05963 training acc: 0.98206 testing acc: 0.9658\n",
      "epoch: 10 iter: 680 loss: 0.05727 training acc: 0.98292 testing acc: 0.9664\n",
      "epoch: 10 iter: 681 loss: 0.05694 training acc: 0.98286 testing acc: 0.967\n",
      "epoch: 10 iter: 682 loss: 0.05704 training acc: 0.98268 testing acc: 0.9662\n",
      "epoch: 10 iter: 683 loss: 0.06991 training acc: 0.97842 testing acc: 0.9668\n",
      "epoch: 10 iter: 684 loss: 0.06035 training acc: 0.98204 testing acc: 0.9704\n",
      "epoch: 10 iter: 685 loss: 0.05832 training acc: 0.98278 testing acc: 0.9702\n",
      "epoch: 10 iter: 686 loss: 0.06984 training acc: 0.97834 testing acc: 0.9652\n",
      "epoch: 10 iter: 687 loss: 0.06633 training acc: 0.97978 testing acc: 0.9658\n",
      "epoch: 10 iter: 688 loss: 0.05519 training acc: 0.98374 testing acc: 0.9704\n",
      "epoch: 10 iter: 689 loss: 0.05570 training acc: 0.98352 testing acc: 0.971\n",
      "epoch: 10 iter: 690 loss: 0.05531 training acc: 0.98372 testing acc: 0.9718\n",
      "epoch: 10 iter: 691 loss: 0.05240 training acc: 0.98454 testing acc: 0.9694\n",
      "epoch: 10 iter: 692 loss: 0.05238 training acc: 0.98456 testing acc: 0.9706\n",
      "epoch: 10 iter: 693 loss: 0.05177 training acc: 0.98484 testing acc: 0.9714\n",
      "epoch: 10 iter: 694 loss: 0.05897 training acc: 0.98192 testing acc: 0.9696\n",
      "epoch: 10 iter: 695 loss: 0.05761 training acc: 0.98266 testing acc: 0.9696\n",
      "epoch: 10 iter: 696 loss: 0.05548 training acc: 0.98366 testing acc: 0.9706\n",
      "epoch: 10 iter: 697 loss: 0.05478 training acc: 0.98358 testing acc: 0.971\n",
      "epoch: 10 iter: 698 loss: 0.05329 training acc: 0.98446 testing acc: 0.9714\n",
      "epoch: 10 iter: 699 loss: 0.05474 training acc: 0.98362 testing acc: 0.9696\n",
      "epoch: 10 iter: 700 loss: 0.05365 training acc: 0.98422 testing acc: 0.971\n",
      "epoch: 10 iter: 701 loss: 0.05617 training acc: 0.98330 testing acc: 0.9704\n",
      "epoch: 10 iter: 702 loss: 0.05510 training acc: 0.98380 testing acc: 0.9706\n",
      "epoch: 10 iter: 703 loss: 0.05696 training acc: 0.98366 testing acc: 0.9682\n",
      "epoch: 10 iter: 704 loss: 0.05529 training acc: 0.98426 testing acc: 0.969\n",
      "epoch: 10 iter: 705 loss: 0.05522 training acc: 0.98394 testing acc: 0.969\n",
      "epoch: 10 iter: 706 loss: 0.06867 training acc: 0.97926 testing acc: 0.9664\n",
      "epoch: 10 iter: 707 loss: 0.05446 training acc: 0.98394 testing acc: 0.9694\n",
      "epoch: 10 iter: 708 loss: 0.06045 training acc: 0.98168 testing acc: 0.968\n",
      "epoch: 10 iter: 709 loss: 0.05918 training acc: 0.98198 testing acc: 0.9682\n",
      "epoch: 10 iter: 710 loss: 0.05558 training acc: 0.98334 testing acc: 0.9706\n",
      "epoch: 10 iter: 711 loss: 0.05591 training acc: 0.98366 testing acc: 0.9704\n",
      "epoch: 10 iter: 712 loss: 0.05230 training acc: 0.98500 testing acc: 0.9716\n",
      "epoch: 10 iter: 713 loss: 0.06517 training acc: 0.98050 testing acc: 0.9698\n",
      "epoch: 10 iter: 714 loss: 0.07644 training acc: 0.97618 testing acc: 0.9656\n",
      "epoch: 10 iter: 715 loss: 0.07725 training acc: 0.97624 testing acc: 0.9638\n",
      "epoch: 10 iter: 716 loss: 0.06604 training acc: 0.97954 testing acc: 0.967\n",
      "epoch: 10 iter: 717 loss: 0.06371 training acc: 0.98100 testing acc: 0.968\n",
      "epoch: 10 iter: 718 loss: 0.06039 training acc: 0.98270 testing acc: 0.9682\n",
      "epoch: 10 iter: 719 loss: 0.06643 training acc: 0.98036 testing acc: 0.9676\n",
      "epoch: 10 iter: 720 loss: 0.05849 training acc: 0.98292 testing acc: 0.969\n",
      "epoch: 10 iter: 721 loss: 0.05823 training acc: 0.98360 testing acc: 0.9698\n",
      "epoch: 10 iter: 722 loss: 0.06245 training acc: 0.98134 testing acc: 0.9676\n",
      "epoch: 10 iter: 723 loss: 0.05824 training acc: 0.98312 testing acc: 0.9688\n",
      "epoch: 10 iter: 724 loss: 0.06221 training acc: 0.98118 testing acc: 0.9676\n",
      "epoch: 10 iter: 725 loss: 0.05901 training acc: 0.98274 testing acc: 0.9678\n",
      "epoch: 10 iter: 726 loss: 0.05702 training acc: 0.98356 testing acc: 0.9698\n",
      "epoch: 10 iter: 727 loss: 0.05457 training acc: 0.98416 testing acc: 0.97\n",
      "epoch: 10 iter: 728 loss: 0.05335 training acc: 0.98488 testing acc: 0.97\n",
      "epoch: 10 iter: 729 loss: 0.05324 training acc: 0.98462 testing acc: 0.9712\n",
      "epoch: 10 iter: 730 loss: 0.05254 training acc: 0.98462 testing acc: 0.9706\n",
      "epoch: 10 iter: 731 loss: 0.05201 training acc: 0.98498 testing acc: 0.971\n",
      "epoch: 10 iter: 732 loss: 0.05323 training acc: 0.98476 testing acc: 0.9718\n",
      "epoch: 10 iter: 733 loss: 0.05105 training acc: 0.98544 testing acc: 0.9724\n",
      "epoch: 10 iter: 734 loss: 0.05217 training acc: 0.98500 testing acc: 0.969\n",
      "epoch: 10 iter: 735 loss: 0.05170 training acc: 0.98522 testing acc: 0.9696\n",
      "epoch: 10 iter: 736 loss: 0.05246 training acc: 0.98474 testing acc: 0.9688\n",
      "epoch: 10 iter: 737 loss: 0.05574 training acc: 0.98374 testing acc: 0.9694\n",
      "epoch: 10 iter: 738 loss: 0.05801 training acc: 0.98258 testing acc: 0.9662\n",
      "epoch: 10 iter: 739 loss: 0.05854 training acc: 0.98238 testing acc: 0.9658\n",
      "epoch: 10 iter: 740 loss: 0.05463 training acc: 0.98430 testing acc: 0.9684\n",
      "epoch: 10 iter: 741 loss: 0.05296 training acc: 0.98500 testing acc: 0.9694\n",
      "epoch: 10 iter: 742 loss: 0.06196 training acc: 0.98224 testing acc: 0.9678\n",
      "epoch: 10 iter: 743 loss: 0.07169 training acc: 0.97904 testing acc: 0.9654\n",
      "epoch: 10 iter: 744 loss: 0.05273 training acc: 0.98480 testing acc: 0.97\n",
      "epoch: 10 iter: 745 loss: 0.05223 training acc: 0.98494 testing acc: 0.9698\n",
      "epoch: 10 iter: 746 loss: 0.05179 training acc: 0.98512 testing acc: 0.9702\n",
      "epoch: 10 iter: 747 loss: 0.05162 training acc: 0.98516 testing acc: 0.9712\n",
      "epoch: 10 iter: 748 loss: 0.05252 training acc: 0.98480 testing acc: 0.9718\n",
      "epoch: 10 iter: 749 loss: 0.05109 training acc: 0.98548 testing acc: 0.9704\n",
      "epoch: 10 iter: 750 loss: 0.05159 training acc: 0.98568 testing acc: 0.97\n",
      "epoch: 10 iter: 751 loss: 0.05095 training acc: 0.98576 testing acc: 0.9706\n",
      "epoch: 10 iter: 752 loss: 0.05317 training acc: 0.98520 testing acc: 0.9682\n",
      "epoch: 10 iter: 753 loss: 0.05254 training acc: 0.98550 testing acc: 0.968\n",
      "epoch: 10 iter: 754 loss: 0.05225 training acc: 0.98528 testing acc: 0.9694\n",
      "epoch: 10 iter: 755 loss: 0.04984 training acc: 0.98568 testing acc: 0.9708\n",
      "epoch: 10 iter: 756 loss: 0.05177 training acc: 0.98498 testing acc: 0.9692\n",
      "epoch: 10 iter: 757 loss: 0.05070 training acc: 0.98550 testing acc: 0.9696\n",
      "epoch: 10 iter: 758 loss: 0.05160 training acc: 0.98550 testing acc: 0.9706\n",
      "epoch: 10 iter: 759 loss: 0.04880 training acc: 0.98670 testing acc: 0.9718\n",
      "epoch: 10 iter: 760 loss: 0.04763 training acc: 0.98680 testing acc: 0.972\n",
      "epoch: 10 iter: 761 loss: 0.05497 training acc: 0.98426 testing acc: 0.9712\n",
      "epoch: 10 iter: 762 loss: 0.05087 training acc: 0.98564 testing acc: 0.9714\n",
      "epoch: 10 iter: 763 loss: 0.05260 training acc: 0.98452 testing acc: 0.9706\n",
      "epoch: 10 iter: 764 loss: 0.05177 training acc: 0.98484 testing acc: 0.9704\n",
      "epoch: 10 iter: 765 loss: 0.04986 training acc: 0.98556 testing acc: 0.9704\n",
      "epoch: 10 iter: 766 loss: 0.05206 training acc: 0.98464 testing acc: 0.9688\n",
      "epoch: 10 iter: 767 loss: 0.05193 training acc: 0.98476 testing acc: 0.9682\n",
      "epoch: 10 iter: 768 loss: 0.05044 training acc: 0.98530 testing acc: 0.9692\n",
      "epoch: 10 iter: 769 loss: 0.05436 training acc: 0.98390 testing acc: 0.9686\n",
      "epoch: 10 iter: 770 loss: 0.06164 training acc: 0.98182 testing acc: 0.9666\n",
      "epoch: 10 iter: 771 loss: 0.05679 training acc: 0.98330 testing acc: 0.9678\n",
      "epoch: 10 iter: 772 loss: 0.05773 training acc: 0.98310 testing acc: 0.9676\n",
      "epoch: 10 iter: 773 loss: 0.05339 training acc: 0.98440 testing acc: 0.9694\n",
      "epoch: 10 iter: 774 loss: 0.05395 training acc: 0.98450 testing acc: 0.9688\n",
      "epoch: 10 iter: 775 loss: 0.04881 training acc: 0.98642 testing acc: 0.9722\n",
      "epoch: 10 iter: 776 loss: 0.04872 training acc: 0.98636 testing acc: 0.971\n",
      "epoch: 10 iter: 777 loss: 0.05003 training acc: 0.98604 testing acc: 0.9718\n",
      "epoch: 10 iter: 778 loss: 0.05005 training acc: 0.98596 testing acc: 0.972\n",
      "epoch: 10 iter: 779 loss: 0.05074 training acc: 0.98578 testing acc: 0.971\n",
      "epoch: 10 iter: 780 loss: 0.05059 training acc: 0.98556 testing acc: 0.9706\n",
      "epoch: 10 iter: 781 loss: 0.05017 training acc: 0.98568 testing acc: 0.9708\n",
      "epoch: 10 iter: 782 loss: 0.05089 training acc: 0.98550 testing acc: 0.9712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 iter: 783 loss: 0.05135 training acc: 0.98532 testing acc: 0.9704\n",
      "epoch: 10 iter: 784 loss: 0.05690 training acc: 0.98260 testing acc: 0.9666\n",
      "epoch: 10 iter: 785 loss: 0.05829 training acc: 0.98246 testing acc: 0.9658\n",
      "epoch: 10 iter: 786 loss: 0.05883 training acc: 0.98182 testing acc: 0.9656\n",
      "epoch: 10 iter: 787 loss: 0.05748 training acc: 0.98220 testing acc: 0.9662\n",
      "epoch: 10 iter: 788 loss: 0.06421 training acc: 0.97980 testing acc: 0.9642\n",
      "epoch: 10 iter: 789 loss: 0.06283 training acc: 0.98036 testing acc: 0.9638\n",
      "epoch: 10 iter: 790 loss: 0.06228 training acc: 0.98064 testing acc: 0.964\n",
      "epoch: 10 iter: 791 loss: 0.05728 training acc: 0.98268 testing acc: 0.966\n",
      "epoch: 10 iter: 792 loss: 0.05646 training acc: 0.98326 testing acc: 0.966\n",
      "epoch: 10 iter: 793 loss: 0.05476 training acc: 0.98358 testing acc: 0.967\n",
      "epoch: 10 iter: 794 loss: 0.05470 training acc: 0.98388 testing acc: 0.9672\n",
      "epoch: 10 iter: 795 loss: 0.05360 training acc: 0.98430 testing acc: 0.9702\n",
      "epoch: 10 iter: 796 loss: 0.05157 training acc: 0.98538 testing acc: 0.971\n",
      "epoch: 10 iter: 797 loss: 0.06267 training acc: 0.98090 testing acc: 0.9648\n",
      "epoch: 10 iter: 798 loss: 0.06211 training acc: 0.98138 testing acc: 0.965\n",
      "epoch: 10 iter: 799 loss: 0.06088 training acc: 0.98174 testing acc: 0.966\n",
      "epoch: 10 iter: 800 loss: 0.05994 training acc: 0.98200 testing acc: 0.9666\n",
      "epoch: 10 iter: 801 loss: 0.05692 training acc: 0.98298 testing acc: 0.97\n",
      "epoch: 10 iter: 802 loss: 0.06014 training acc: 0.98196 testing acc: 0.9664\n",
      "epoch: 10 iter: 803 loss: 0.05386 training acc: 0.98412 testing acc: 0.97\n",
      "epoch: 10 iter: 804 loss: 0.05601 training acc: 0.98316 testing acc: 0.9688\n",
      "epoch: 10 iter: 805 loss: 0.05417 training acc: 0.98398 testing acc: 0.9708\n",
      "epoch: 10 iter: 806 loss: 0.05534 training acc: 0.98364 testing acc: 0.971\n",
      "epoch: 10 iter: 807 loss: 0.05506 training acc: 0.98322 testing acc: 0.969\n",
      "epoch: 10 iter: 808 loss: 0.05339 training acc: 0.98414 testing acc: 0.9702\n",
      "epoch: 10 iter: 809 loss: 0.06017 training acc: 0.98090 testing acc: 0.9652\n",
      "epoch: 10 iter: 810 loss: 0.05439 training acc: 0.98418 testing acc: 0.9682\n",
      "epoch: 10 iter: 811 loss: 0.06079 training acc: 0.98238 testing acc: 0.9666\n",
      "epoch: 10 iter: 812 loss: 0.05891 training acc: 0.98280 testing acc: 0.9666\n",
      "epoch: 10 iter: 813 loss: 0.05666 training acc: 0.98368 testing acc: 0.9692\n",
      "epoch: 10 iter: 814 loss: 0.05290 training acc: 0.98514 testing acc: 0.9698\n",
      "epoch: 10 iter: 815 loss: 0.05562 training acc: 0.98404 testing acc: 0.9706\n",
      "epoch: 10 iter: 816 loss: 0.05505 training acc: 0.98442 testing acc: 0.97\n",
      "epoch: 10 iter: 817 loss: 0.05157 training acc: 0.98488 testing acc: 0.9698\n",
      "epoch: 10 iter: 818 loss: 0.05196 training acc: 0.98442 testing acc: 0.9694\n",
      "epoch: 10 iter: 819 loss: 0.05167 training acc: 0.98478 testing acc: 0.9684\n",
      "epoch: 10 iter: 820 loss: 0.05250 training acc: 0.98486 testing acc: 0.9682\n",
      "epoch: 10 iter: 821 loss: 0.05562 training acc: 0.98330 testing acc: 0.9682\n",
      "epoch: 10 iter: 822 loss: 0.05147 training acc: 0.98538 testing acc: 0.9718\n",
      "epoch: 10 iter: 823 loss: 0.04945 training acc: 0.98596 testing acc: 0.9726\n",
      "epoch: 10 iter: 824 loss: 0.05033 training acc: 0.98572 testing acc: 0.9726\n",
      "epoch: 10 iter: 825 loss: 0.05166 training acc: 0.98538 testing acc: 0.9724\n",
      "epoch: 10 iter: 826 loss: 0.05316 training acc: 0.98438 testing acc: 0.9708\n",
      "epoch: 10 iter: 827 loss: 0.05318 training acc: 0.98432 testing acc: 0.9714\n",
      "epoch: 10 iter: 828 loss: 0.05277 training acc: 0.98488 testing acc: 0.972\n",
      "epoch: 10 iter: 829 loss: 0.05015 training acc: 0.98582 testing acc: 0.9714\n",
      "epoch: 10 iter: 830 loss: 0.05046 training acc: 0.98598 testing acc: 0.9722\n",
      "epoch: 10 iter: 831 loss: 0.04942 training acc: 0.98634 testing acc: 0.9726\n",
      "epoch: 10 iter: 832 loss: 0.04737 training acc: 0.98674 testing acc: 0.973\n",
      "epoch: 10 iter: 833 loss: 0.04904 training acc: 0.98624 testing acc: 0.9726\n",
      "epoch: 10 iter: 834 loss: 0.04881 training acc: 0.98624 testing acc: 0.9726\n",
      "epoch: 10 iter: 835 loss: 0.04906 training acc: 0.98586 testing acc: 0.9712\n",
      "epoch: 10 iter: 836 loss: 0.04895 training acc: 0.98600 testing acc: 0.9708\n",
      "epoch: 10 iter: 837 loss: 0.04826 training acc: 0.98614 testing acc: 0.9722\n",
      "epoch: 10 iter: 838 loss: 0.05161 training acc: 0.98454 testing acc: 0.9694\n",
      "epoch: 10 iter: 839 loss: 0.05045 training acc: 0.98504 testing acc: 0.9696\n",
      "epoch: 10 iter: 840 loss: 0.04905 training acc: 0.98596 testing acc: 0.972\n",
      "epoch: 10 iter: 841 loss: 0.05000 training acc: 0.98550 testing acc: 0.9722\n",
      "epoch: 10 iter: 842 loss: 0.05045 training acc: 0.98542 testing acc: 0.9704\n",
      "epoch: 10 iter: 843 loss: 0.05270 training acc: 0.98498 testing acc: 0.9718\n",
      "epoch: 10 iter: 844 loss: 0.05075 training acc: 0.98548 testing acc: 0.973\n",
      "epoch: 10 iter: 845 loss: 0.05747 training acc: 0.98286 testing acc: 0.9698\n",
      "epoch: 10 iter: 846 loss: 0.05783 training acc: 0.98288 testing acc: 0.966\n",
      "epoch: 10 iter: 847 loss: 0.05704 training acc: 0.98296 testing acc: 0.9672\n",
      "epoch: 10 iter: 848 loss: 0.05742 training acc: 0.98274 testing acc: 0.9666\n",
      "epoch: 10 iter: 849 loss: 0.05260 training acc: 0.98430 testing acc: 0.9692\n",
      "epoch: 10 iter: 850 loss: 0.05353 training acc: 0.98446 testing acc: 0.971\n",
      "epoch: 10 iter: 851 loss: 0.05178 training acc: 0.98522 testing acc: 0.9714\n",
      "epoch: 10 iter: 852 loss: 0.05217 training acc: 0.98516 testing acc: 0.9704\n",
      "epoch: 10 iter: 853 loss: 0.05603 training acc: 0.98332 testing acc: 0.9676\n",
      "epoch: 10 iter: 854 loss: 0.05413 training acc: 0.98410 testing acc: 0.9678\n",
      "epoch: 10 iter: 855 loss: 0.05424 training acc: 0.98394 testing acc: 0.967\n",
      "epoch: 10 iter: 856 loss: 0.05282 training acc: 0.98468 testing acc: 0.9684\n",
      "epoch: 10 iter: 857 loss: 0.05208 training acc: 0.98484 testing acc: 0.97\n",
      "epoch: 10 iter: 858 loss: 0.05114 training acc: 0.98522 testing acc: 0.9704\n",
      "epoch: 10 iter: 859 loss: 0.05266 training acc: 0.98482 testing acc: 0.971\n",
      "epoch: 10 iter: 860 loss: 0.05505 training acc: 0.98316 testing acc: 0.9694\n",
      "epoch: 10 iter: 861 loss: 0.05648 training acc: 0.98280 testing acc: 0.9708\n",
      "epoch: 10 iter: 862 loss: 0.05833 training acc: 0.98244 testing acc: 0.9692\n",
      "epoch: 10 iter: 863 loss: 0.05535 training acc: 0.98348 testing acc: 0.9696\n",
      "epoch: 10 iter: 864 loss: 0.05176 training acc: 0.98528 testing acc: 0.9726\n",
      "epoch: 10 iter: 865 loss: 0.05360 training acc: 0.98440 testing acc: 0.9704\n",
      "epoch: 10 iter: 866 loss: 0.05102 training acc: 0.98470 testing acc: 0.9718\n",
      "epoch: 10 iter: 867 loss: 0.05151 training acc: 0.98476 testing acc: 0.97\n",
      "epoch: 10 iter: 868 loss: 0.06180 training acc: 0.98088 testing acc: 0.9672\n",
      "epoch: 10 iter: 869 loss: 0.05758 training acc: 0.98286 testing acc: 0.969\n",
      "epoch: 10 iter: 870 loss: 0.05968 training acc: 0.98194 testing acc: 0.9692\n",
      "epoch: 10 iter: 871 loss: 0.05609 training acc: 0.98322 testing acc: 0.9692\n",
      "epoch: 10 iter: 872 loss: 0.06015 training acc: 0.98176 testing acc: 0.968\n",
      "epoch: 10 iter: 873 loss: 0.06344 training acc: 0.98004 testing acc: 0.966\n",
      "epoch: 10 iter: 874 loss: 0.06018 training acc: 0.98152 testing acc: 0.9664\n",
      "epoch: 10 iter: 875 loss: 0.05959 training acc: 0.98194 testing acc: 0.9682\n",
      "epoch: 10 iter: 876 loss: 0.05638 training acc: 0.98370 testing acc: 0.9692\n",
      "epoch: 10 iter: 877 loss: 0.05380 training acc: 0.98462 testing acc: 0.9694\n",
      "epoch: 10 iter: 878 loss: 0.05454 training acc: 0.98424 testing acc: 0.9694\n",
      "epoch: 10 iter: 879 loss: 0.05446 training acc: 0.98448 testing acc: 0.9704\n",
      "epoch: 10 iter: 880 loss: 0.05236 training acc: 0.98488 testing acc: 0.9694\n",
      "epoch: 10 iter: 881 loss: 0.05318 training acc: 0.98488 testing acc: 0.969\n",
      "epoch: 10 iter: 882 loss: 0.05367 training acc: 0.98456 testing acc: 0.9686\n",
      "epoch: 10 iter: 883 loss: 0.05096 training acc: 0.98580 testing acc: 0.97\n",
      "epoch: 10 iter: 884 loss: 0.05346 training acc: 0.98424 testing acc: 0.969\n",
      "epoch: 10 iter: 885 loss: 0.05131 training acc: 0.98474 testing acc: 0.9696\n",
      "epoch: 10 iter: 886 loss: 0.05225 training acc: 0.98462 testing acc: 0.9698\n",
      "epoch: 10 iter: 887 loss: 0.05170 training acc: 0.98460 testing acc: 0.9704\n",
      "epoch: 10 iter: 888 loss: 0.05174 training acc: 0.98464 testing acc: 0.97\n",
      "epoch: 10 iter: 889 loss: 0.05205 training acc: 0.98452 testing acc: 0.9716\n",
      "epoch: 10 iter: 890 loss: 0.05225 training acc: 0.98466 testing acc: 0.9714\n",
      "epoch: 10 iter: 891 loss: 0.05315 training acc: 0.98416 testing acc: 0.9716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 iter: 892 loss: 0.05252 training acc: 0.98454 testing acc: 0.9714\n",
      "epoch: 10 iter: 893 loss: 0.05325 training acc: 0.98412 testing acc: 0.9718\n",
      "epoch: 10 iter: 894 loss: 0.05679 training acc: 0.98298 testing acc: 0.9688\n",
      "epoch: 10 iter: 895 loss: 0.05356 training acc: 0.98456 testing acc: 0.9706\n",
      "epoch: 10 iter: 896 loss: 0.06121 training acc: 0.98162 testing acc: 0.9686\n",
      "epoch: 10 iter: 897 loss: 0.05392 training acc: 0.98510 testing acc: 0.9724\n",
      "epoch: 10 iter: 898 loss: 0.05251 training acc: 0.98490 testing acc: 0.9734\n",
      "epoch: 10 iter: 899 loss: 0.05314 training acc: 0.98448 testing acc: 0.973\n",
      "epoch: 10 iter: 900 loss: 0.05670 training acc: 0.98380 testing acc: 0.9712\n",
      "epoch: 10 iter: 901 loss: 0.05975 training acc: 0.98232 testing acc: 0.9682\n",
      "epoch: 10 iter: 902 loss: 0.05551 training acc: 0.98400 testing acc: 0.9702\n",
      "epoch: 10 iter: 903 loss: 0.06082 training acc: 0.98218 testing acc: 0.9698\n",
      "epoch: 10 iter: 904 loss: 0.05819 training acc: 0.98318 testing acc: 0.9712\n",
      "epoch: 10 iter: 905 loss: 0.05484 training acc: 0.98418 testing acc: 0.9696\n",
      "epoch: 10 iter: 906 loss: 0.05583 training acc: 0.98398 testing acc: 0.9686\n",
      "epoch: 10 iter: 907 loss: 0.05316 training acc: 0.98516 testing acc: 0.9724\n",
      "epoch: 10 iter: 908 loss: 0.05499 training acc: 0.98404 testing acc: 0.9708\n",
      "epoch: 10 iter: 909 loss: 0.05615 training acc: 0.98376 testing acc: 0.9694\n",
      "epoch: 10 iter: 910 loss: 0.05277 training acc: 0.98494 testing acc: 0.9708\n",
      "epoch: 10 iter: 911 loss: 0.05124 training acc: 0.98558 testing acc: 0.9728\n",
      "epoch: 10 iter: 912 loss: 0.05466 training acc: 0.98450 testing acc: 0.9704\n",
      "epoch: 10 iter: 913 loss: 0.05316 training acc: 0.98464 testing acc: 0.9698\n",
      "epoch: 10 iter: 914 loss: 0.05182 training acc: 0.98524 testing acc: 0.971\n",
      "epoch: 10 iter: 915 loss: 0.06380 training acc: 0.98148 testing acc: 0.9648\n",
      "epoch: 10 iter: 916 loss: 0.06141 training acc: 0.98220 testing acc: 0.9662\n",
      "epoch: 10 iter: 917 loss: 0.05188 training acc: 0.98456 testing acc: 0.9718\n",
      "epoch: 10 iter: 918 loss: 0.05333 training acc: 0.98464 testing acc: 0.969\n",
      "epoch: 10 iter: 919 loss: 0.05664 training acc: 0.98348 testing acc: 0.9676\n",
      "epoch: 10 iter: 920 loss: 0.05374 training acc: 0.98448 testing acc: 0.9704\n",
      "epoch: 10 iter: 921 loss: 0.05237 training acc: 0.98488 testing acc: 0.9708\n",
      "epoch: 10 iter: 922 loss: 0.04925 training acc: 0.98648 testing acc: 0.9716\n",
      "epoch: 10 iter: 923 loss: 0.04949 training acc: 0.98636 testing acc: 0.9718\n",
      "epoch: 10 iter: 924 loss: 0.05155 training acc: 0.98528 testing acc: 0.9738\n",
      "epoch: 10 iter: 925 loss: 0.05109 training acc: 0.98544 testing acc: 0.9736\n",
      "epoch: 10 iter: 926 loss: 0.05127 training acc: 0.98564 testing acc: 0.9726\n",
      "epoch: 10 iter: 927 loss: 0.05032 training acc: 0.98580 testing acc: 0.9728\n",
      "epoch: 10 iter: 928 loss: 0.04948 training acc: 0.98596 testing acc: 0.9734\n",
      "epoch: 10 iter: 929 loss: 0.04824 training acc: 0.98618 testing acc: 0.9742\n",
      "epoch: 10 iter: 930 loss: 0.04813 training acc: 0.98618 testing acc: 0.9732\n",
      "epoch: 10 iter: 931 loss: 0.04802 training acc: 0.98642 testing acc: 0.9744\n",
      "epoch: 10 iter: 932 loss: 0.05204 training acc: 0.98458 testing acc: 0.9716\n",
      "epoch: 10 iter: 933 loss: 0.05065 training acc: 0.98524 testing acc: 0.9722\n",
      "epoch: 10 iter: 934 loss: 0.05104 training acc: 0.98512 testing acc: 0.972\n",
      "epoch: 10 iter: 935 loss: 0.05119 training acc: 0.98496 testing acc: 0.9716\n",
      "epoch: 10 iter: 936 loss: 0.05002 training acc: 0.98532 testing acc: 0.9738\n",
      "epoch: 10 iter: 937 loss: 0.04854 training acc: 0.98582 testing acc: 0.9752\n",
      "epoch: 10 iter: 938 loss: 0.05594 training acc: 0.98396 testing acc: 0.9718\n",
      "epoch: 10 iter: 939 loss: 0.04972 training acc: 0.98612 testing acc: 0.9732\n",
      "epoch: 10 iter: 940 loss: 0.05452 training acc: 0.98444 testing acc: 0.9724\n",
      "epoch: 10 iter: 941 loss: 0.05056 training acc: 0.98590 testing acc: 0.9712\n",
      "epoch: 10 iter: 942 loss: 0.05306 training acc: 0.98480 testing acc: 0.9716\n",
      "epoch: 10 iter: 943 loss: 0.05098 training acc: 0.98556 testing acc: 0.973\n",
      "epoch: 10 iter: 944 loss: 0.05106 training acc: 0.98530 testing acc: 0.9728\n",
      "epoch: 10 iter: 945 loss: 0.05194 training acc: 0.98526 testing acc: 0.9702\n",
      "epoch: 10 iter: 946 loss: 0.05140 training acc: 0.98570 testing acc: 0.9728\n",
      "epoch: 10 iter: 947 loss: 0.04902 training acc: 0.98640 testing acc: 0.9726\n",
      "epoch: 10 iter: 948 loss: 0.05376 training acc: 0.98422 testing acc: 0.9718\n",
      "epoch: 10 iter: 949 loss: 0.05649 training acc: 0.98348 testing acc: 0.971\n",
      "epoch: 10 iter: 950 loss: 0.05497 training acc: 0.98424 testing acc: 0.9722\n",
      "epoch: 10 iter: 951 loss: 0.05125 training acc: 0.98560 testing acc: 0.9716\n",
      "epoch: 10 iter: 952 loss: 0.05071 training acc: 0.98556 testing acc: 0.9716\n",
      "epoch: 10 iter: 953 loss: 0.06416 training acc: 0.98120 testing acc: 0.9678\n",
      "epoch: 10 iter: 954 loss: 0.07983 training acc: 0.97486 testing acc: 0.9626\n",
      "epoch: 10 iter: 955 loss: 0.05876 training acc: 0.98294 testing acc: 0.9702\n",
      "epoch: 10 iter: 956 loss: 0.05771 training acc: 0.98346 testing acc: 0.9698\n",
      "epoch: 10 iter: 957 loss: 0.05896 training acc: 0.98248 testing acc: 0.969\n",
      "epoch: 10 iter: 958 loss: 0.05470 training acc: 0.98392 testing acc: 0.9704\n",
      "epoch: 10 iter: 959 loss: 0.05429 training acc: 0.98418 testing acc: 0.9706\n",
      "epoch: 10 iter: 960 loss: 0.05569 training acc: 0.98380 testing acc: 0.9696\n",
      "epoch: 10 iter: 961 loss: 0.05619 training acc: 0.98338 testing acc: 0.9706\n",
      "epoch: 10 iter: 962 loss: 0.05407 training acc: 0.98420 testing acc: 0.9712\n",
      "epoch: 10 iter: 963 loss: 0.05367 training acc: 0.98334 testing acc: 0.971\n",
      "epoch: 10 iter: 964 loss: 0.05619 training acc: 0.98260 testing acc: 0.9698\n",
      "epoch: 10 iter: 965 loss: 0.05723 training acc: 0.98222 testing acc: 0.9702\n",
      "epoch: 10 iter: 966 loss: 0.05489 training acc: 0.98312 testing acc: 0.97\n",
      "epoch: 10 iter: 967 loss: 0.05514 training acc: 0.98266 testing acc: 0.9698\n",
      "epoch: 10 iter: 968 loss: 0.05369 training acc: 0.98348 testing acc: 0.9704\n",
      "epoch: 10 iter: 969 loss: 0.05356 training acc: 0.98350 testing acc: 0.971\n",
      "epoch: 10 iter: 970 loss: 0.05248 training acc: 0.98400 testing acc: 0.9714\n",
      "epoch: 10 iter: 971 loss: 0.05256 training acc: 0.98450 testing acc: 0.9712\n",
      "epoch: 10 iter: 972 loss: 0.05583 training acc: 0.98292 testing acc: 0.9712\n",
      "epoch: 10 iter: 973 loss: 0.05228 training acc: 0.98394 testing acc: 0.9716\n",
      "epoch: 10 iter: 974 loss: 0.05833 training acc: 0.98176 testing acc: 0.967\n",
      "epoch: 10 iter: 975 loss: 0.05666 training acc: 0.98260 testing acc: 0.968\n",
      "epoch: 10 iter: 976 loss: 0.05080 training acc: 0.98488 testing acc: 0.9724\n",
      "epoch: 10 iter: 977 loss: 0.05277 training acc: 0.98428 testing acc: 0.9702\n",
      "epoch: 10 iter: 978 loss: 0.05959 training acc: 0.98144 testing acc: 0.9664\n",
      "epoch: 10 iter: 979 loss: 0.05564 training acc: 0.98318 testing acc: 0.9704\n",
      "epoch: 10 iter: 980 loss: 0.05614 training acc: 0.98314 testing acc: 0.969\n",
      "epoch: 10 iter: 981 loss: 0.05481 training acc: 0.98350 testing acc: 0.97\n",
      "epoch: 10 iter: 982 loss: 0.05573 training acc: 0.98306 testing acc: 0.9702\n",
      "epoch: 10 iter: 983 loss: 0.05830 training acc: 0.98190 testing acc: 0.966\n",
      "epoch: 10 iter: 984 loss: 0.06433 training acc: 0.98044 testing acc: 0.9676\n",
      "epoch: 10 iter: 985 loss: 0.06251 training acc: 0.98058 testing acc: 0.9664\n",
      "epoch: 10 iter: 986 loss: 0.05574 training acc: 0.98358 testing acc: 0.9684\n",
      "epoch: 10 iter: 987 loss: 0.05317 training acc: 0.98486 testing acc: 0.9694\n",
      "epoch: 10 iter: 988 loss: 0.05296 training acc: 0.98488 testing acc: 0.97\n",
      "epoch: 10 iter: 989 loss: 0.05488 training acc: 0.98348 testing acc: 0.968\n",
      "epoch: 10 iter: 990 loss: 0.05514 training acc: 0.98376 testing acc: 0.97\n",
      "epoch: 10 iter: 991 loss: 0.05567 training acc: 0.98352 testing acc: 0.971\n",
      "epoch: 10 iter: 992 loss: 0.05482 training acc: 0.98372 testing acc: 0.971\n",
      "epoch: 10 iter: 993 loss: 0.05636 training acc: 0.98330 testing acc: 0.9704\n",
      "epoch: 10 iter: 994 loss: 0.05611 training acc: 0.98334 testing acc: 0.9704\n",
      "epoch: 10 iter: 995 loss: 0.05471 training acc: 0.98384 testing acc: 0.9712\n",
      "epoch: 10 iter: 996 loss: 0.06559 training acc: 0.98010 testing acc: 0.9654\n",
      "epoch: 10 iter: 997 loss: 0.05607 training acc: 0.98372 testing acc: 0.9716\n",
      "epoch: 10 iter: 998 loss: 0.05678 training acc: 0.98358 testing acc: 0.9712\n",
      "epoch: 10 iter: 999 loss: 0.06664 training acc: 0.98008 testing acc: 0.9698\n"
     ]
    }
   ],
   "source": [
    "n_in = 784\n",
    "n_h1 = 100\n",
    "n_h2 = 100\n",
    "n_out = 10\n",
    "\n",
    "W1 = np.matrix(np.random.rand(n_in, n_h1)) * 0.001    # 784 * 100\n",
    "b1 = np.matrix(np.random.rand(n_h1, 1)) * 0           # 100 * 1\n",
    "W2 = np.matrix(np.random.rand(n_h1, n_h2)) * 0.001    # 100 * 100\n",
    "b2 = np.matrix(np.random.rand(n_h2, 1)) * 0           # 100 * 1\n",
    "W3 = np.matrix(np.random.rand(n_h2, n_out)) * 0.001   # 100 * 10\n",
    "b3 = np.matrix(np.random.rand(n_out, 1)) * 0          # 10 * 1\n",
    "\n",
    "batch_size = 50\n",
    "lr = 0.1\n",
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    perm = np.random.permutation(X_train.shape[0]).reshape(-1, batch_size)\n",
    "    for i in range(len(perm)):\n",
    "        ind = perm[i]\n",
    "        X_batch = X_train[ind].T # 784 * 50\n",
    "        Y_batch = Y_train[ind].T # 10 * 50\n",
    "\n",
    "        Z1 = W1.T * X_batch + b1\n",
    "        H1 = relu(Z1) # 100 * 50\n",
    "        Z2 = W2.T * H1 + b2\n",
    "        H2 = relu(Z2) # 100 * 50\n",
    "        Y_hat = softmax(W3.T * H2 + b3) # 10 * 50\n",
    "\n",
    "        dW3 = (1/batch_size) * H2 * (Y_hat - Y_batch).T # 100 * 10\n",
    "        db3 = (1/batch_size) * np.sum(Y_hat - Y_batch, axis = 1) # 10 * 1\n",
    "        dH2 = W3 * (Y_hat - Y_batch) # 100 * 50\n",
    "\n",
    "        dZ2 = np.multiply(dH2, relu_prime(H2))\n",
    "        dW2 = (1/batch_size) * H1 * dZ2.T # 100 * 100\n",
    "        db2 = (1/batch_size) * np.sum(dZ2, axis = 1) # 100 * 1\n",
    "        dH1 = W2 * dZ2 # 100 * 50\n",
    "\n",
    "        dZ1 = np.multiply(dH1, relu_prime(H1))\n",
    "        dW1 = (1/batch_size) * X_batch * dZ1.T # 784 * 100\n",
    "        db1 = (1/batch_size) * np.sum(dZ1, axis = 1) # 100 * 1\n",
    "\n",
    "        W1 -= lr * dW1\n",
    "        bs31 -= lr * db1\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "        W3 -= lr * dW3\n",
    "        b3 -= lr * db3\n",
    "        \n",
    "        Z1 = W1.T * X_train.T + b1\n",
    "        H1 = relu(Z1)\n",
    "        Z2 = W2.T * H1 + b2\n",
    "        H2 = relu(Z2) \n",
    "        Y_hat = softmax(W3.T * H2 + b3) \n",
    "\n",
    "        loss = cross_entropy_loss(Y_hat, Y_train.T)\n",
    "        losses.append(loss)\n",
    "\n",
    "        train_acc = np.sum(prediction(Y_hat) == prediction(Y_train.T)) / Y_train.shape[0]\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        Z1 = W1.T * X_test.T + b1\n",
    "        H1 = relu(Z1)\n",
    "        Z2 = W2.T * H1 + b2\n",
    "        H2 = relu(Z2) \n",
    "        Y_hat = softmax(W3.T * H2 + b3) \n",
    "\n",
    "        test_acc = np.sum(prediction(Y_hat) == prediction(Y_test.T)) / Y_test.shape[0]\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print('epoch: {} iter: {:3} loss: {:.5f} training acc: {:.5f} testing acc: {}'.format(epoch+1, i, loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Cross Entropy Loss')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a501402e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Cross Entropy Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Model Accuracy')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b203470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_accs)), train_accs, 'r')\n",
    "plt.plot(range(len(test_accs)), test_accs, 'b')\n",
    "plt.legend(['train','test'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Model Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[530   1   1   1   0   0   0   0   1   0]\n",
      " [  9 475   5   1   0   0   2   5   0   0]\n",
      " [  1   6 498   0   3   0   2   6   4   1]\n",
      " [  1   0   0 480   0   0   1   0   2   0]\n",
      " [  0   1   9   0 426   3   1  14   3   0]\n",
      " [  3   6   0   2   0 464   0   5   0   5]\n",
      " [  4   2   3   0   0   0 513   1   3   1]\n",
      " [  0   3   2   0   0   0   0 507   2   1]\n",
      " [  2   0   0   4   0   0   7   4 459   2]\n",
      " [  0   2   0   0   0   0   0   2   1 497]]\n",
      "Normalized confusion matrix\n",
      "[[0.99 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.02 0.96 0.01 0.   0.   0.   0.   0.01 0.   0.  ]\n",
      " [0.   0.01 0.96 0.   0.01 0.   0.   0.01 0.01 0.  ]\n",
      " [0.   0.   0.   0.99 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.02 0.   0.93 0.01 0.   0.03 0.01 0.  ]\n",
      " [0.01 0.01 0.   0.   0.   0.96 0.   0.01 0.   0.01]\n",
      " [0.01 0.   0.01 0.   0.   0.   0.97 0.   0.01 0.  ]\n",
      " [0.   0.01 0.   0.   0.   0.   0.   0.98 0.   0.  ]\n",
      " [0.   0.   0.   0.01 0.   0.   0.01 0.01 0.96 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.99]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEYCAYAAADFzZobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztnXd4FdXWh9+VBJBQpLcEpBM6hC7NC0gRBCwoiHQRFbtXP71WLNfeQLwKCmIDu1JEQJAqoaOCVCkSeic0gbC+P2YSD5Ekc3JmwhmyX555ODOzZ+11Zuas7L1n9vqJqmIwGAw5iYgL7YDBYDBkNybwGQyGHIcJfAaDIcdhAp/BYMhxmMBnMBhyHCbwGQyGHIevA5+I5BWRSSJyWES+CMFObxGZ7qZvFwoRaSki68KlPhEpLyIqIlHZ5ZNfEJEtItLO/vwfEXnPgzreEZHH3bbrdyQ73uMTkZuA+4E4IAlYCTynqvNDtNsHuAu4XFXPhOxomCMiClRR1Y0X2pf0EJEtwC2q+qO9Xh7YDORy+xqJyAdAoqo+5qbd7CLtuXLBXn/bXgs37F3MeN7iE5H7gTeA/wIlgXLA20A3F8xfBqzPCUHPCaZV5R3m3F5kqKpnC3ApcBTokUGZPFiBcYe9vAHksfddASQCDwB7gJ3AAHvfMOAUcNquYxDwFPBxgO3ygAJR9np/YBNWq3Mz0Dtg+/yA4y4HlgCH7f8vD9g3G3gGWGDbmQ4US+e7pfj/UID/3YGrgPXAAeA/AeUbAwuBQ3bZt4Dc9r659nc5Zn/fGwPs/x+wC/goZZt9TCW7jnh7vQywD7jCwbUbBzxgf46x677DXq9s25U09X0EnAVO2D4+FHAN+gF/2vU/6vD6n3Nd7G1q13+rfe1P2XVNSud7KHAbsAE4CIzk755OBPAYsNW+Ph8Cl6a5dwbZfs8N2DYA2Gbbuw1oBPxqX7e3AuquBMwC9tvf+xOgUMD+LUA7+/NT2Peufd2PBixngKfsfQ8Df2Dde78D19jbqwMngWT7mEP29g+AZwPqHAxstK/fRKCMk3N1sS1eB76O9kWLyqDM00ACUAIoDvwMPBMQOM7YZXJhBYzjQOG0N0s66yk3ahSQDzgCVLP3lQZqpv2BAUXsi97HPq6XvV7U3j/bvvGqAnnt9RfS+W4p/j9h+z8Y2At8ChQAato3a0W7fAOgqV1veWANcG/aH/157L+IFUDyEhCIAm70NUA0MA14xeG1G4gdTICb7O/8WcC+7wJ8CKxvC/aPOc01GG37Vxf4C6ju4PqnXpfznQPS/KjT+R4KTAYKYfU29gIdA77HRqAikB/4Gvgojd8fYt07eQO2vQNcArS3r9+3tv8xWAG0tW2jMnClfW2KYwXPN853rkhz7waUqWf7XN9e74H1BywC64/fMaB0Bucr9RwBbbACcLzt0whgrpNzdbEtXnd1iwL7NOOuaG/gaVXdo6p7sVpyfQL2n7b3n1bV77H+mlXLoj9ngVoikldVd6rq6vOU6QxsUNWPVPWMqo4H1gJXB5QZq6rrVfUE8DnWzZkep7HGM08DE4BiwJuqmmTXvxqoA6Cqy1Q1wa53C/Au0NrBd3pSVf+y/TkHVR2N9Rd8EVawfzQTeynMAVqKSATQCngJaG7va23vD4ZhqnpCVX8BfsEKgJD59XeDF1T1kKr+CfzE39erN/Caqm5S1aPAI0DPNN3ap1T1WJpz+4yqnlTV6ViBZ7zt/3ZgHlAfQFU3quoM+9rsBV4j8+uZiogUxwqqd6nqCtvmF6q6Q1XPqupnWNe2sUOTvYExqrpcVf+yv28zexw2hfTO1UWF14FvP1Ask/GRMlhdjRS22ttSbaQJnMex/joHhaoew/oLeRuwU0SmiEicA39SfIoJWN8VhD/7VTXZ/pzy49kdsP9EyvEiUlVEJovILhE5gjUuWiwD2wB7VfVkJmVGA7WAEfYNnymq+gfWH5l6QEuslsAOEalG1gJfeucss+vvBsHUHYU1Fp3CtvPYS3v90rueJURkgohst6/nx2R+PbGPzQV8CXyqqhMCtvcVkZUickhEDmFdV0c2SfN97WC/n6zf277F68C3EKsr0D2DMjuwHlKkUM7elhWOYXXpUigVuFNVp6nqlVgtn7VYASEzf1J82p5Fn4Lhf1h+VVHVgsB/sMbRMiLDx/Iikh9r3Ox94CkRKRKEP3OA67HGGbfb632BwlhP5oP25zxkdP3PuZ4ics71zEJdTuo+w7mBLJQ6nrePr2Nfz5vJ/HqmMAJrHC/1ibWIXIZ1z96JNfRSCFgVYDMzX8/5viKSD6tXlh33dljhaeBT1cNY41sjRaS7iESLSC4R6SQiL9nFxgOPiUhxESlml/84i1WuBFqJSDkRuRSrKQ+AiJQUka72xf4LqzWTfB4b3wNVReQmEYkSkRuBGlgtHq8pgDUOedRujd6eZv9urPGoYHgTWKaqtwBTsManABCRp0RkdgbHzsH6kc2112djvT40P6AVm5Zgfczo+v8C1BSReiJyCdY4WCh1na/u+0Skgv0H4r9Y45huvSVQAPtBg4jEAA86OUhEhmC1qm9S1bMBu/JhBbe9drkBWC2+FHYDsSKSOx3TnwID7POZB+v7LrKHVXIUnr/OoqqvYb3D9xjWBduG9WP61i7yLLAU66nYb8Bye1tW6poBfGbbWsa5wSoC6+nwDqwnWq2BO85jYz/QxS67H+vJZBdV3ZcVn4Lk31gPEpKw/rJ/lmb/U8A4u5tzQ2bGRKQb1gOm2+xN9wPxItLbXi+L9XQ6PeZg/XhTAt98rBbY3HSPsFo5j9k+/jszH8ng+qvqeqyHHz9ijWWlfe/zfaCGXde3BM8YrCfRc7Ge8p/ECuxuMQzrQcJhrD86Xzs8rhdWQN8hIkft5T+q+jvwKlZPajdQm3Ov3yysMeNdIvKP+1VVZwKPA19hvTVQCeiZlS/md7LlBWZDeCIiK4G2drA3GHIMJvAZDIYch6/n6hoMBkNWMIHPYDDkOEzgMxgMOY6wmngtUXlVchdw3W796uVct2kw+JmtW7ewb98+p+8UZkpkwctUz/xj4tB50RN7p6lqR7fqzgrhFfhyFyBPtUzf0giaBYvect2mweBnmjdp6Ko9PXPC8W/35MqRTmeaeEZYBT6DweBXBMQ/I2cm8BkMhtARICLyQnvhmLAN0WunDGPJ5/8hYcLDzP/kIQCeuKMziz97hIQJDzPp7aGULn5pavlXH7qeVd89yeLPHqFeXGxQdQ25ZSDlypSgQb1amRe+iO0CTJ/2A3VqVqNmXGVefukFY9dFu368bkEh4mwJA8I28AF0vPVNmvZ8gRa9rWm9r4+bSeMbn6dpzxeYOm8Vj9zaCYAOLWpQqVxxanUbxp3Pjmf4f4KbhdOnX3++m/yD6/77zW5ycjL33j2U7yZNZcWvv/PFhPGs+f13Y9clu367bsFhd3WdLGFAeHjhkKRjf2dfis6bJyV5Il1a1+HTyYsBWPzbFi4tkJdSxQo6ttuiZSuKFAkmacnFaXfJ4sVUqlSZChUrkjt3bnrc2JPJk74zdl2y67frFjSmxRc6qsqkt+9kwScPMfDa5qnbnxp6NRumPkPPTg155n9TAChTohCJuw6mltm++xBlShTKdp/9zo4d24mNLZu6HhMTy/btoWcsMna9JSz8FUyLD0BExojIHhFZlZXj2wx4nctvepHud77NkBtb0jy+EgBPjZxElU6PM2HqUm67sZVd1z+PN3OQg+d850xc+Att7HpLePjrsLUXJufRy/D7AVZKpCyxc+9hAPYePMrEWb/SqGb5c/Z/PnUJ3dtaWbG37z5EbKnCqftiShZKPd7gnJiYWBIT/044vH17ImXKhJ4M2dj1lrDxNyLS2RIGeBb4VHUuVt67oIm+JDf5o/Okfm7XLI7Vf+ygUrniqWU6t67D+i1Wotwpc37jpi6W7EDj2uU5cvQEu/YdCfEb5DwaNmrExo0b2LJ5M6dOneKLzybQuUtXY9clu14RHv66+3DDFlv/zU6zv9TeVkREZojIBvv/wvZ2EZHhIrJRRH4VkfjM7F/w9/hE5FYsqUDIZaX3L1G0AJ+9NhiAqMhIPpu6lBk/r2H8K7dQ5bISnD2r/LnzAHc/Z0kR/DB/NR1a1GT1xCc5fvI0Q54KLoFz35t7MW/ObPbt20el8rE8/sQw+g8cFPJ385vdqKgoXn/zLa7u3IHk5GT69R9IjZo1jV2X7PrtugWF4EU39l9pEgA/DMxU1RdE5GF7/f+ATkAVe2mCJeHQJEN3vRwLs9WbJquqoxeXIqJLqBdT1g4uMVPWDIZAmjdpyLJlS12LVBEFymie+rc6Knty3rBlqprhnDkR2QI0DAx8IrIOSxN6p4iUBmarajURedf+PD5tuXT9deSpwWAwZEhQXd1iIrI0YDlfxFRguogsC9hfMiWY2f+XsLfHcK4aXiLnKsf9gwve1TUYDBcJEY4bkPsya/EBzVV1h4iUAGaIyNoMyp6v4gy7sl6+zjIeSxSlmogkikjogxkGgyE8SZmr69JTXVXdYf+/B/gGSzR9t93Fxf5/j108EUs4K4VYMpGo9fKpbi9VLa2quVQ1VlXf96oug8FwoXHvqa6I5BORAimfgfZY+sETgX52sX5AyvSUiUBf++luU+BwRuN7YLq6BoPBLdx7qlsS+MZ+CTsK+FRVfxCRJcDndu/xT6CHXf574CpgI3AcGJBZBSbwGQwGd3BpOpqqbgLqnmf7fqDtebYrMDSYOkzgMxgMoRNG09GcYAKfwWBwhzBJQOAEE/gMBoMLSNjMw3VCWAW+etXLMffn4a7bLdxthOs2AfZ/c6cndiOcvw9lyAJnz3ozWynHXzfT1TUYDDmKlHx8PsEEPoPB4AJGZc1gMOREfNTV9U+Itnn7reE0jq9Do/q1GTnizaCPj4gQFg7vyVdPdgHgxxevI2FETxJG9GTThwP4/LHOALSsHcOuz29N3fdIr0ZZ8rd61Qo0iq9D00b1adEsazbS4ke1Li/sbtu2jQ7t/kW92tWJr1uTt4YHfz+khxfXDfx1foPGR6nnfdXi+331Kj4Y8x6z5yeQO3durrn6Kjp0uorKlas4tnFn17qs23aAAtG5AWj3f1+l7hv/n05MSticur5g9Q6uGzY5ZL+nTp9FsWLuicf36def2+64k1sG9nXNJvyt1jVl6gxiYmNp0bQRXbp0pXqNGmFpNyoqihdeepX68fEkJSVxeZMGtG13Zch2U3D7uvnt/AaF+OupbniEX4esW7uGRo2bEB0dTVRUFC1atmLSd986Pj6maD46NirP2Gn/lN7LnzcXrevGMmnhH2667Al+U+vyym7p0qWpH28l2y1QoABxcdXZsSN8RYH8dn6DxmhueEP1mrVYMH8e+/fv5/jx40ybNpXtidsyP9Dm5Vtb8ejYBZw9T/LVrs0qMXtlIkknTqduaxJXikUjevHtsK5UL5e1QCMIXTt3oHnThox5b1SWbGQXflYt27plCytXrqBR4wwT7zrGi+vm5/PrBBFxtIQDnnV1RaQs8CFQCjgLjFLVkAZh4uKqc98DD9Ktcwfy5ctP7dp1iIpy9hU6NSrPnsPHWbFxLy1r/zNH4Q2tq/LBtNWp6ys37qHagHEcO3maDg0v4/PHOlP71o+C9nnm7PmULlOGPXv2cPVV7alaLY4WLVsFbSc78Ktq2dGjR+l1w3W8/OobFCzoXE85I7y4bn49v06QC1BnKHjZ4jsDPKCq1YGmwFARCXnQod+AQcxPWMq0mbMpXLgIlRyO7zWrUZouTSqydkw/Pvy/DlxRJ5Yx/74SgCIFLqFh1RJMXbIltXzSidMcO2m1/qYt3UquqAiKFrwkaH9L22pXJUqUoGu37ixdsjhoG9mFH1XLTp8+Ta8bruPGXr3pfs21rtgEb66bH8+vYySIJQzwMh/fTlVdbn9OAtaQSTpoJ+zdY+Ue3Pbnn0z87huuv6Gno+OeGLeQyv3GEjdwHH1fnMbsXxMZ+MoMAK5tUZmpi7fw1+nk1PIlC0enfm5YtSQRIuw/cjIoX48dO0ZSUlLq55k/zqBGTfefxLqF31TLVJXbBg+iWlx17rnv/pDtpeDVdfPb+Q0OZ93ccGkVZstTXVt0qD6w6Dz7UlXWypYtl6mt3j17cODAfnLlysVrb4ygcOHCmR6TGT1aVeGVL5eds+2a5pUZfFUtziQrJ0+doe9LPwRtd8/u3fS8wWqFJJ85ww09e9G+Q5alhlPxm1qXV3Z/XrCATz/5iFq1atOkgaWxPOzZ/9Kx01Uh2fXquvnt/AZLRIR/Hhl4qrIGICL5gTnAc6r6dUZl4xs01Lk/u98VLH7tSNdtgpmr61fMXF33VdYii1TQ/B2edlT2yIS+maqseY2nLT4RyQV8BXySWdAzGAw+JozG75zg5VNdAd4H1qjqa17VYzAYLjxC+IzfOcHLTnlzoA/QRkRW2ktogy8GgyFsMQ83AFWdj68avwaDIRTCJag5wVdzdQ0GQ5giID56uGMCn8FgcAXT4jMYDDkKvz3cMIHPYDC4ggl8BoMh5+GfuBdegU+AqEj337DxaoZF0fbPemL34I+Pe2LXq1k6fvpL7yU5+vyKT/y0CavAZzAY/Iuf5uqawGcwGELGPNwwGAw5E//EPX+lnvdSXezQoUP07tmD+rWrE1+nBosSFgZ1fESEsHD0YL56/kYAWtcvz8+jbmHp2CGMfrgrkZHWXVEwXx6+/O+NLHrvVpaNvY0+HetmyV8vVLVOnjxJy8ub0KRBPRrUrcUzw550xS74T2Ut1PshPZKTk2naKJ5ru1/tij3w9nfhGPHXlDVfBb4+/frz3eTg8+I54cEH7uXK9h1Y8dsaEpaupFpc9aCOv/O6xqzbug+w9FTee6QrfZ/+moYD3uXP3Ye5uYMV4IZ0b8jaLftocssoOtz7IS/ccSW5ooK7DCmqWt9NmsqKX3/niwnjWfP7PwWUgiVPnjxMnT6TRctWkrB0BTOmT2PxooSQ7Xrlb4rK2srf1jBnfgLvvjPSFbsQ+v2QHiNHvEmcS7ZS8PJ3EQwm8HmEV+piR44cYcG8ufQbYCXzzJ07N4UKFXJ8fEzxAnRsWoWxU1YAULRgNH+dTmZj4gEAZi3dRPdWcQCoQn5b2jJf3twcTDrBmeSzQfnrlaqWiJA/f37ASul++vRpV1Sx/KayFur9kB6JiYn8MPV7V5LGBuLV7yJYTODzGZs3b6JY8eIMGTyQZo3jueO2Wzh27Jjj41++swOPvvtjqnrbvsPHyRUZQXy10gBc07o6sSUuBeCdb5YQd1kxNn11L0vHDuHfI6YR7FsQXqpqJScn06RhfS6LKUnbtu1o7IJqmd9U1kK9H9LjoQfu49nnX/TV089gkAhxtDi2JxIpIitEZLK9XkFEFonIBhH5TERy29vz2Osb7f3lM7Pt2RUQkUtEZLGI/CIiq0VkmFd1hUrymTOsXLGcwbfexsLFy4mOzserLzsbh+rUrAp7Dh5jxfpd52zv+/TXvDS0PfP+N5CkE6dSW3VXNq7Erxt3UfG6N2hyyyhev6djqri5U7xU1YqMjGTR0hVs2LyNpUuXsHrVqpBt+k1lLZT7IT2+nzKZ4iWKEx/fIGT/whGnrb0gr/s9WFo9KbwIvK6qVYCDQErTeRBwUFUrA6/b5TLEyz89fwFtVLUuUA/oKCJNPawvy5SJiSUmNja1tXDNtdezcsUKR8c2q1WWLs2rsnbCXXz4xLVcUb8CYx7tzqLft9Pu7nG0vH0M83/5kz/sbm+fjnX5bu5aADZtP8iWnYeoVq5YUP5mh6pWoUKFaNmqNTOmhz525DeVtVDuh/RI+HkBUyZPIq5KBfre3Is5P81iYL8+brgbNrgZ+EQkFugMvGevC9AG+NIuMg7obn/uZq9j728rmVTkpcqaqupRezWXvXgr8JFFSpUqRWxsWdavWwfA7J9mElfd2QD0E6NnUbnHm8T1HEHfp79m9orNDHzuW4oXslTacueK5IFelzN6oiVmtG3PYa5oUAGAEoXzUbVsUTbvPBiUv16pau3du5dDhw4BcOLECX6aNZOq1eJCtus3lbVQ7of0ePq559m4eRtrN2zmw4/H0/pfbRgzLnid5nAmiMBXTESWBiy3nsfcG8BDWJrcAEWBQ6p6xl5P5G/VxhhgG4C9/7BdPl281tyIBJYBlYGRqpqxylq5jFXWvFIXA3jl9eEM7H8zp06dokKFirwzekxI9u7r2YxOzaoSIcLoiUuZs2ILAC98OI9RD3dlyZghiMCjo2ax//CJoGx7paq1a+dOBg/qz9nkZM6ePcu11/fgqs5dQrbrN5U1cP9+8BIvfxdB4bwXuy8jsSER6QLsUdVlInJFBtbVwb7z1+G1yhqAiBQCvgHuUtV0B40aNGioCxYtdb1+r1S1zFxdi3B5UucUr+4Hr06DF+fXbZW1PCWraExvZ+9Rbn69c4YqayLyPJZsxRngEqAgVvzoAJRS1TMi0gx4SlU7iMg0+/NCEYkCdgHFNYMbPlseL6nqIWA2ELo4qcFgCDtErJf4nSyZoaqPqGqsqpYHegKzVLU38BNwvV2sH5DyTtREex17/6yMgh54+1S3uN3SQ0TyAu2AtV7VZzAYLiSePNVNy/8B94vIRqwxvPft7e8DRe3t9wMPZ2bIyzG+0sA4e5wvAvhcVSd7WJ/BYLiAeNHVV9XZWL1FVHUT0Pg8ZU4CPYKx66XK2q9Afa/sGwyG8MJPY70mO4vBYAgd8e7hjheYwGcwGEJGwNGDi3DBBD6DweAKJvAZDIachenqGgyGnIZgHm6EHV41wb2aYVH46jc8sXtw0r2e2PUbfuqS+YfwybXnhBwR+AwGg/f4KO6ZwGcwGNzBtPgMBkOOImWurl/wVQ5sL9WkvFABc8NuRISw8K2b+OopK3/dFfXK8vOIm0h4qzczX+lBxdJWSvvcuSL56OGrWPV+f+a+3pNyJbKWiThcz8P58Nv94KUqnFfXLRhEnC3hgK8Cn1dqUl6pgLlh985u9Vj354HU9eFD2zDgpak0vfMTPvtpHQ/3srIE929fk4NHT1Jr0AeM+HY5zw1scUH8zU67frsfvFKF88rfYDFiQx7hlZqUVypgodqNKZafjo0rMHba3ykMFaVgdB7A0ujdud9Kct2lWSU++dGSJ/h63gauqFf2nwY99je77frtfvBKFc4rf4PFtPh8hlcqYKHafXlIax59fz5nA9Qn73jjR755uhsbPxrETW3jeOULK3FrmaL5SNyXBEDyWeXI8b8oWvCSbPU3u+16hd9U4cLi/BpB8XNJKxEXjnilAhaK3U6NK7Dn0HFWbNxzzva7ronnmie+o3Kf9/lo+u+8OLhVunaDTbgcjufhQuA3VbhwOL/WC8z+afFlx1PdFIm40K+wR3ilAhaK3WY1ytClaUU6NqpAnlyRFIzOzdfDulGtbGGWrLOkLL+cu57vnrWEprbvO0pssQJs33eUyAihYHQeDiSdzDZ/L4Rdr/CbKlx4nF9n2ZXDBU9bfGkl4sIVr1TAQrH7xAcLqNznfeL6j6HvC1OZ/cs2egybSMHoPFSOKQRAm/rlUh98TEn4g97tLCWwa1tWYc4v29K17YW/F8KuV/hNFS5czq+furpet/hSJOIKpFcgHFTWvFIBc9tu8lll6PAfGf9oF86qcujoXwx5fToAH0xbzZgHO7Dq/f4cTDpJnxe+v+D+em3Xb/eDV6pwXvkbFGHUjXWCZyprtkTcVap6hy0R929VzVCr0CuVNb9h5uoavMZtlbUCZeO03r3OOnbz/90yQ5W17MDLFl9zoKuIXIUtESciH6vqzR7WaTAYLhDh0o11gmdjfOlIxJmgZzBcpJinugaDIWfhs7m62RL4AiXiDAbDxYeYfHwGgyEn4qO4ZwKfwWBwhwgfRT4T+AwGgyv4KO6ZwGcwGEJHxF+vs6Qb+EQkw7m1qnrEfXcMBoNfibxInuquBhQr8UIKKesKZDy/LAdwJvls5oWygFczLC677QtP7G753/We2PWqBXH6jDfXLSrSG3/90pLyiZtABoFPVYPPZGkwGHIkgvVKi19wNHNDRHqKyH/sz7Ei0sBbtwwGg9+IEGdLOJBp4BORt4B/AX3sTceBd7x0ymAw+AyHKanCpdvupMV3uaoOAU4CqOoBILenXmWA39S63n5rOI3j69Cofm1GjggfVa0IgR+faMfHdzW3/LylMQue7cicYe15o3/Dc8arLq9WnJlPXMmcYe355sErgq7r5MmTtLy8CU0a1KNB3Vo8M+zJoG2cDzev2x1DBlGxXCmaNKjzj33DX3+Vgnkj2b9vX0h1eHUe4OJSWRORS0RksYj8IiKrRWSYvb2CiCwSkQ0i8pmI5La357HXN9r7y2dWh5PAd1pEIrAeaCAiRQFvRoczwW9qXb+vXsUHY95j9vwEFi5ZwQ/fT2Hjxg0h23XjPAxuV4UNO5NS179a9CfNH/uB1k9O55JckfRuWQGAgnlz8ULvePq+NZ/WT05n8DsLg/Y3T548TJ0+k0XLVpKwdAUzpk9j8aKEoO2kxc3r1rtPP77+7p85DBO3bWPWrBmULRv6szyvzkM4qKwJ1lNdJ4sD/gLaqGpdoB7QUUSaAi8Cr6tqFeAgkJJ8cRBwUFUrA6/b5TLESeAbCXwFFLcj73wnhr3Ab2pd69auoVHjJkRHRxMVFUWLlq2Y9N23IdsN9TyULpyXK+uU5pN5m1K3zfxtV+rnFVsOUKZwNADXNinH98sT2X7gBAD7kv4K2l8RIX/+/ICVev306dOuPAJ087o1b9GKwuex9chD9/PMcy+60kXz6jyEj8qaO11dtThqr+ayFwXaAF/a28cB3e3P3ex17P1tJZOKMg18qvoh8BjwCnAA6KGqEzL13gPCQk0qCKrXrMWC+fPYv38/x48fZ9q0qWxPDD4lfFpCPQ/P3FiPp7/8lbPnyUEbFSlc3/QyZq2yAmGlkvm5NDo3Xz/YmumPt6NHs8uy5HNycjJNGtbnspiStG3bjsYuqIt5zfeTJ1K6TAy169R1zaYX5yEcfhdOu7lO47wtUrYS2APMAP4ADqnqGbtIIhBjf44BtgHY+w8DRTOy73TmRiRwGivqOs7hJyIgtxKtAAAgAElEQVRbgCQgGTgTatbVcFCTCoa4uOrc98CDdOvcgXz58lO7dh2iokKfLBPKebiyTmn2JZ3k162HuLxa8X/sf7F3PAnr97JogzWeFRkZQd3LCnP9q3O4JHckUx5pw7JN+9m0++g/js2IyMhIFi1dwaFDh+jZ41pWr1pFzVruj6m6xfHjx3n5xef51uUhEC/OQ7j8LoKYq1tMRAJTrY9S1VGBBVQ1GagnIoWAb4Dq57GT8sXPV3GGqeWdPNV9FBgPlAFigU9F5JHMjgvgX6paz41U0+GhJhUc/QYMYn7CUqbNnE3hwkWoVLlKyDZDOQ+NKxelQ90yLHnhKt69tSnN40ow8pbGADxwdQ2KFsjDE5//klp+58HjzFq9i+Onkjlw9BQJ6/dRM7ZQln0vVKgQLVu1ZsZ098dU3WTzpj/YunUzzRvXp1a1imzfnkjLZg3ZvWtX5gc7wM3zEC6/C3G4APtUtWHAMuq8BgFVPYSV0q4pUEhEUloOscAO+3MiUBbA3n8pVu80XZy03m4GGqnqY6r6KNAY6OvgONcJFzWpYNi7x9LF3fbnn0z87huuv6FnyDZDOQ/Pfb2K+g9NodHD3zNkVAIL1u5h6HuL6d2yAv+qWZLbRiWco8f7w8odNK1SjMgIIW/uSOIrFmHDzuBmK+7du5dDhw4BcOLECX6aNZOq1eKCspHd1KxVm01/7mLVuk2sWreJmJhY5i1cSslSpbJs06vzEC6/C7fG+ESkuN3SQ0TyAu2wJGp/AlKmCfUDUgYyJ9rr2PtnaSZiQk76XVvTlIsCNqVTNi0KTBcRBd49X2QPRmXNb2pdAL179uDAgf3kypWL194YQeHChUO26cV5eOnmeBL3H2fKI20BmLI8kdcmr2HDziRmrdrFT0+1R1X5ZN5m1u4ILvDt2rmTwYP6czY5mbNnz3Lt9T24qnOGulOOcPO6Deh7E/PnzWH/vn3EVSrHfx5/kr793bkHUvDqPISDypqI4ye2TigNjBORSKzG2eeqOllEfgcmiMizwArgfbv8+8BHIrIRq6WXaesiXZU1EXkdK3CVBxoB0+z19sB8Ve2dqXGRMqq6Q0RKYA1Q3qWqc9Mr7zeVNa/m6kZFeiOFYubqWpi5uu6rrBWtWFOveuZTR2U/vrleWKusrbL/Xw1MCdju+MUjVd1h/79HRL7B6ianG/gMBoN/CecHjWnJKEnB++ntc4KI5AMiVDXJ/tweeDoUmwaDITwRwmcerhMyHeMTkUrAc0ANLH1cAFS1aiaHlgS+sf8KRAGfqmp4P8ozGAxZ5qJo8QXwAfAs1gvMnYABOJiypqqbAPfe/DQYDGGNf8Kes9dZolV1GoCq/qGqj2FlazEYDAbAmpHh4lxdz3HS4vvLnvf2h4jcBmwHSnjrlsFg8BsXW1f3PiA/cDfWWN+lwEAvnTIYDP7DR3Ev88Cnqovsj0n8nYzUYDAYUhHk4tDVtd+7S3fah6pe64lHBoPBfwSReSUcyKjF91a2eWGjnD/TRMh23TcJeDfDwiu2vtPDE7uFe4z2xO7BLwZ7YterAXavxrjOni9/WIh48ZO4KMb4VHVmdjpiMBj8iwCRF0PgMxgMhmAIkzdVHGECn8FgcAU/Bb5gsinn8dIRJ3ipUnXo0CF69+xB/drVia9Tg0UJwYvqnA8v1K+2bdtGh3b/ol7t6sTXrclbw8NIvS1CWPjqNXz1aIfUbU/1bsivI29gxYjruaPzuemSGlQuxtEvB3FNswoXxN/0qF61Ao3i69C0UX1aNGvkml2/+esUK628f+QlnczVbYyV7+pSoJyI1AVuUdW7vHYuLSkqVfnz5+f06dO0vaIlHTp2onGTpiHbfvCBe7myfQc+mfAFp06d4vjx4yHbTFG/mjJ1BjGxsbRo2oguXbpSvUaNkOxGRUXxwkuvUj8+nqSkJC5v0oC27a4M2a4b/t7ZpRbrEg9RINpSIO3TpiqxRfNT987PUYXil6ZO9yYiQni2bxNmrEy8YP5mxNTpsyhWrJgrtsB//gbLxdbiGw50AfYDqOovXKApa16pVB05coQF8+bSb4CVeDJ37twUKpT19OopeKV+Vbp0aerHxwNQoEAB4uKqs2NH6OIyofobUzQfHRuUZeyP61K33dqxOv/9fHnqk/W9h0+m7rvjqpp8u3DzOduy09/sxm/+BoubYkNe4yTwRajq1jTbkr1wxgleqFRt3ryJYsWLM2TwQJo1jueO227h2LFjIdvNDvWrrVu2sHLlChqFgVrXywOb8ui4xee8flGhVEGub1GR+S9359vHO1KpdEEAyhSJpmvT8oyetuaC+ZsRgtC1cweaN23ImPfSlYQICr/5G1z9ECXiaAkHnAS+bXZ3V23Jt3uB9U6Mi0ghEflSRNaKyBoRaRaSt/ytUrVh8zaWLl3C6lWrMj8oE5LPnGHliuUMvvU2Fi5eTnR0Pl59OfTxF6/Vr44ePUqvG67j5VffoGDBgiHbC8XfTg3LsefwSVZs2nfO9jxRkfx1KpkWD37L2BlreffO1gC8PKgZj324OKR31Lw8vzNnz+fnRcv4ZuL3vPvO28yfF3r+XL/5Gyx+avE5eap7O1Z3txywG/jR3uaEN4EfVPV6EckNRGfJy/MQqFIVqjxfmZhYYmJjU1tN11x7Pa++HLpmupfqV6dPn6bXDddxY6/edL/GnUk0ofjbLK4kXRqVo2ODsuTJFUnB6NyMufcKtu8/xjcLNwPwXcKW1MAXX6k4Hz7QBoCiBS6hQ4OynEk+y6TFaTsX3vibGaVtOyVKlKBrt+4sXbKYFi1bhWTTb/4Gg4i/pqw5ERTfo6o9VbWYvfRU1X2ZHSciBYFW2IIgqnrKlorLMl6pVJUqVYrY2LKsX2eNTc3+aSZx1c8n4xkcXqlfqSq3DR5Etbjq3HPf/SHbSyEUf5/4eAmVB48nbsgE+r46i9m/7WDgG7OZtHgLV9SxfpQta5Zm447DAFS/bQJxQ6zlm4WbuffdBUEFvVD9zYhjx46RlJSU+nnmjzOoUTN0DWC/+RssF1WLT0RGc54ZLqp6ayaHVgT2AmPtJ8HLgHtU9ZzBs2BU1rxSqQJ45fXhDOx/M6dOnaJChYq8M3pMyDa9Ur/6ecECPv3kI2rVqk2TBvUAGPbsf+nY6aqw8/eVr35h7H3/4q6ra3Ps5Gluf9u9LphX53fP7t30vMFqRSefOcMNPXvRvkPHkO36zd9g8dNT3XRV1lILiNwYsHoJcA2wLbPXWUSkIZYwUXNVXSQibwJHVPXx9I6Jb9BQFyQscey8U7yaqxvhpyvtIX6bq+vF3Ffw7n7wwt8WzRqx3EWVtZiqtXXIyG8clX2yfZWwVlkDQFU/C1wXkY+wpCIzIxFIDEhr9SXwcNAeGgyG8EfATzk7suJqBeCyzAqp6i6sJ8LV7E1tgd+zUJ/BYPAB4vBfOOBkjO8gf4/xRWAplTttud0FfGI/0d2EJVRkMBguMi4qeUlba6Muls4GwFkNImGeqq4ELmhf3mAwZA9+CnwZdnXtIPeNqibbi0ePCQwGg9/xU5ICJ2N8i0Uk3nNPDAaDb0np6jpZwoGMNDeiVPUM0AIYLCJ/AMewvqOqqgmGBoPBQrxL6e8FGY3xLQbige7Z5IvBYPApF9PDDQFQ1T+yyReDweBjwmT4zhEZBb7iIpLuRFBVfc0DfzzBqzfqvXrWEy4DwE7xaoZF4ZbevO9+cJ57mY8D8ep+8OL+9cJiRJi8o+eEjAJfJJAfL86RwWC4qBAunhbfTlV9Ots8MRgM/sXFJ7YiUhb4ECgFnAVGqeqbIlIE+AwoD2wBblDVg/b7xm8CVwHHgf6qujyjOjJ6ncVH8dtgMFxIBOuprpPFAWeAB1S1OtAUGCoiNbBmjM1U1SrATP6eQdYJqGIvtwL/y6yCjAJfWyceXgiSk5Np2iiea7tf7Yo9r1TLvFSF80qtKxztrv36/1jy8b0kjLub+WPuBODaNrVZ9sl9HFvwX+LjYlLLNqwRS8K4u0kYdzeLPryHrq2DT/s05JaBlCtTggb13M1p59X94JW/wRJhJyPNbMkMVd2Z0mJT1SRgDRADdAPG2cXG8fcbJ92AD9UiASgkIqUz9DWDyg9k6uEFYuSIN4mLCz1RaAopqmUrf1vDnPkJvPvOSNb8Hno+hRRVuEXLVpKwdAUzpk9j8aKEkO2mqHV9N2kqK379nS8mjHfF33C223HoKJr2G06LgW8BsPqPXfR85CPmr9xyTrnVf+ym+cC3aNpvON3uG8OIh64hMsi0IX369ee7yT8EdYwTvLofvPI3WIJIRFpMRJYGLOnm9hSR8kB9YBFQUlV3ghUcgRJ2sRhgW8Bhifa2dPFRIhmLxMREfpj6Pf0HDnLNpleqZV6pwnml1uUnu+u27mXDn/9MBH7ir9MkJ58FIE/uKPSfOXQzpUXLVhQpUiQk/86HV/eDV/4Gg2AFEycLsE9VGwYs51VHEpH8wFfAvap6JJPq05Lhhfdd4Hvogft49vkXiYjwxnU3VcvAG1U4r9S6wtWuqjLpzUEsGHsnA7s1zrR8oxplWfbJfSz9+F7ufunb1EAYDnhxP4QFLguKi0gurKD3iap+bW/endKFtf/fY29PBMoGHB4L7MjIvmeBT0SqicjKgOWIrdCWZb6fMpniJYoTH9/ALTfPwW3VMvBGFc4rta5wtdtmyP+4vP8Iut8/liHXNaN5vQoZll/y+zYa9H6dFgPf4sG+V5AntxNNrezBi/shXBCHS6Z2rJvjfWBNmveFJwL97M/9gO8CtvcVi6bA4ZQucXp4FvhUdZ2q1lPVekADrMfMznJTp0PCzwuYMnkScVUq0PfmXsz5aRYD+/VxxV8vVMsCCVSFCxWv1LrC1e7OfZaQzt6Dx5g4ZzWNasQ6Om7d1r0cO3GKmhVLBudwNuDm/RAOCBAp4mhxQHOgD9AmoOF0FfACcKWIbACutNcBvsfK97kRGA3ckVkF2dXVbQv8cR5h8qB4+rnn2bh5G2s3bObDj8fT+l9tGDPuo5Cd80q1zCtVOK/UusLRbvQlucgfnTv1c7smVVi9aXe65S8rXTj1YUa5UoWoWq44W3ceDPk7uIFX90O44JbKmqrOV1VR1TopjSdV/V5V96tqW1WtYv9/wC6vqjpUVSupam1VXZpZHdnVB+gJjD/fjmBU1rzCK9Uyr1ThvFLrCke7JYoU4LMXrFZ9VGQEn01fyYyE9XRtXZPX7u9KsUL5+PrV/vy6fidd7xvD5XXL8+8+V3D6TDJnVbnnlW/Zf/h4UP72vbkX8+bMZt++fVQqH8vjTwxz5WGaV/eDV/4GR/jk2nNCpiprIVdgpZ3fAdRU1fT/VOOdyppXF8TM1fUWM1fXwov7oXmThixzUWWtUo26+t9PvndUtmd8bPirrLlAJ2B5ZkHPYDD4Gz/9wc6OwNeLdLq5BoPh4sE/Yc/jwCci0VhPX4Z4WY/BYLiwiOD0iW1Y4GngU9XjQFEv6zAYDOGB6eoaDIYch3/Cngl8BoPBJXzU4DOBz2AwhI6VpMA/kc8EPoPB4AqmxWcwGHIYzpKMhgthFfgswRL3T56f3qg3/I1XMywKX/GYJ3YPzn7WE7t+wHR1DQZDzsNhAoJwwQQ+g8HgCibwGQyGHIf4qKvru9TzXqiAeaV+5ZV6G4SnGprf7K794gGWjLuThLFDmf/e7QAULpCXya/357fx9zL59f4UKnAJAPf1akHC2KEkjB3K0g/v4uicpylcIG9Q9Xl1P4SDyppg6eo6WcIBXwU+r1TAvFK/8kq9LZzV0Pxmt+PdY2g6YCQtbrGkWP99cytmL9tE7V5vMHvZJv59cysAXh8/n6YDRtJ0wEieeHc681Zu4WDSiaDq8up+CBeVNbfkJbMDXwU+r1TAvFK/8kq9zU9qaH6z26VlHB9PXQ7Ax1OXc3XLf8qY3tCuDp//+GvQtr26H8JBZQ2srq6Tf+GArwKfVypg4L36lZvqbeGqhuY3u6ow6bX+LHj/dgZ2tfJiliicn137jwKwa/9RihfOf84xefPk4somVfh29uqQfHdbze9C47eurtdpqe4DbsHSuPwNGKCqJ7NqzysVMPhb/erQoUP07HEtq1etomYtd8ZM3FZvC1c1NL/ZbXP7KHbuT6J4oXxMfqM/67b+U6c3LZ2bV2Phb38G3c0NxAs1vwtP+LTmnOClvGQMcDfQUFVrAZFY2htZxisVsEDcVr/yQr0tXNXQ/GZ3535bve3QMSbOXUOjGjHsOXiUUkWtVl6povnZe/DoOcf0aFeHL7LQzU3BazW/C4ZDoaEwGeLzvKsbBeQVkSggmkxEfjPDKxUwr9SvvFJvC0c1NL/Zjb4kF/nzBqi3NarM6k17mDJ/LTd3ssbhbu4Uz+R5a1OPKZgvDy3qlWfSvDVZ8ter+yFccEtXNzvwrKurqttF5BXgT+AEMF1Vp6ctF4zKmlcqYF6pX3ml3haOamh+s1uiSH4+++9Nlp3ICD6b8SszFm1g2ZpEPn66J/06x7Nt92F6Pz4h9ZiurWowc/FGjp88nSV/vbofwkFlLUVX1y94prImIoWBr4AbgUPAF8CXqvpxesc0aNBQFyzKVBIzaMxcXUMgZq6u+ypr1WvX17Hf/uSobLPKhS+4ypqXXd12wGZV3auqp4Gvgcs9rM9gMFxA/PQ6i5dPdf8EmtqCQyeAtoD7zTmDwRAW+KkD5FmLT1UXAV8Cy7FeZYkARnlVn8FguLCYhxs2qvok4M7EV4PBEN6ES1RzgMnOYjAYQkaEsJmH6wQT+AwGgyv4J+yZwGcwGNzCR5HPBD6DweAC4fOqihN8lZ3FYDCEL27O1RWRMSKyR0RWBWwrIiIzRGSD/X9he7uIyHAR2Sgiv4pIfGb2c0SLz28zLMxME4vks96chwM/PeOJ3SI9x3hi98CEgZ7YdRMPXlX5AHgL+DBg28PATFV9QUQettf/D+gEVLGXJsD/7P/TxbT4DAaDK4iIo8UJqjoXOJBmczdgnP15HNA9YPuHapEAFBKR0hnZN4HPYDC4QhBd3WIisjRgudVhFSVVdSeA/X8Je3sMsC2gXKK9LV1yRFfXYDB4TxBd3X0uJyk4X9UZjpP4qsVnVMvOJTk5maaN4rm2+9Wu2fTTeVi/bh3NGtVPXUoXu5SRw99wxXao5zYiQlj4cje+eqQdAO8ObcnvI3uQ8HI3El7uRp3ylkZGoXy5mfBgWxa92p25z19NjbKFgq7Ly9+FY5zOVwttIHB3ShfW/n+PvT0RKBtQLpZMcn/6qsWXolJVPz6epKQkLm/SgLbtrqR6jRoh2U1R65oydQYxsbG0aNqILl26hq3dFEaOeJO4uOocSTriij2/nYeq1aqxcMmK1DqqVIjl6m7XhGQzhVDP7dCrarA28RAFo3OlbvvPR0v4NmHLOeUevLYuv27ZT8+XZ1K1zKW8PrgZnYcFl/3bq99FsGTD6ywTgX7AC/b/3wVsv1NEJmA91Dic0iVOD1+1+Ixq2d8kJibyw9TvXU046cfzkMLsWTOpWLES5S67LGRboZ7bmCLRdGxQlg9mrs+0bPXYQvz0m/UbXb/jMJcVz0+JSy8Jqj6vfhfBILj+Ost4YCFQTUQSRWQQVsC7UkQ2AFfa6wDfA5uAjcBo4I7M7Psq8AWSk1XLAB564D6eff5FIiLcu4R+PA8pfPnFBK6/ISRJl1RCPbcvDWjCYx8t4Wya15Ke6tWARa9258X+jckdZdn+besBujWxgnXDysUoVzw/MUXzZdn3C6ne5mbgU9VeqlpaVXOpaqyqvq+q+1W1rapWsf8/YJdVVR2qqpVUtbaqZpr+ztPAJyL3iMgqEVktIve6ZTenq5Z9P2UyxUsUJz6+Qci2AvHbeUjh1KlTTJk8iWuu6xGyrVDPbacGZdl7+CQrNu0/Z/uTnyyl3j1f0fL/JlI4fx4e6F4HgFe++ZXC+XKT8HI3butUg1827+dMctbeX7zQ6m0mESkgIrWAwUBj4BTwg4hMUdUNodg1qmWQ8PMCpkyexLQfpnLy5EmSjhxhYL8+jBn3UUh2/XYeUpj+w1Tq1YunZMmSIdsK9dw2rVaCzo3K0SE+lktyRVIgOjfv392KQcPnAnDqzFk++mkD93a1pEuTTpxmyNvzU49f83YPtuxJCtrvcFBv89P78V62+KoDCap6XFXPAHOAkEaejWqZxdPPPc/GzdtYu2EzH348ntb/ahNy0PPSX6/spvDF5xPocaM73dxQz+2Tny6jypDPqH7HF/R9YzZzVu1g0PC5lCqUN7XM1Y0uY/WflqrfpdG5yWV3ewe0q8r8NbtJOhGcmFG4qLeZRKQWq4DnRKQoVur5qzhP6vlgVNaMapm3+PE8HD9+nJ9mzmD4yHdcsecVY+5pTbGClyAi/LplP3eP+hmAarGX8t5drUg+q6xNPMTtAa0/p3j1uwiacIlqDvBMZQ3AfhIzFDgK/A6cUNX70ivvlcqa3zBzdS28mqsb4dFpKNprrCd2vZir67bKWu268fr19AWOylYtFX1Rq6xhP4mJV9VWWPPuQhrfMxgMYYpYf1CcLOGApy8wi0gJVd0jIuWAa4FmXtZnMBguIGES1Jzg9cyNr+wxvtPAUFU96HF9BoPhghA+r6o4wWuVtZZe2jcYDOGDn4aQfTVX12AwhCfh9KqKE0zgMxgM7uCjyGcCn8FgcAWjq2swGHIc/gl7JvAZDAY3CCLzSjhgAl8Y4rcZFl4RGS5vuzrEKzW0wu3cV4X7a32GeTqziH+ulwl8BoMhZFISkfoFE/gMBoMr+CjumcBnMBjcwU9PdX2Xet4LtS6j3mbsBuLV/TDkloGUK1OCBvVqZdlGRISwcPRgvnr+RgBa1y/Pz6NuYenYIYx+uCuRkVbwue/GZiS8N5iE9wazdOwQjs58lMIFgtPyCBofJeTzVeBLUev6btJUVvz6O19MGM+a338P2W6KStXK39YwZ34C774z0hW7Xvlr7Hpr16v7oU+//nw3OTgFtbTceV1j1m3dB1hjau890pW+T39NwwHv8ufuw9zcoS4Ar3+2kKa3jKbpLaN5YtQs5v2ylYNJJ0P+Dhnho7jnr8DnlVqXUW8zdgPx6n5o0bIVRYoUyfLxMcUL0LFpFcZOsSQ1ixaM5q/TyWxMPADArKWb6N4q7h/H3dC2Fp/PXJ3lep3gVGgoXHrDvgp82aHWlZPV24zdf3IhVcvS8vKdHXj03R9T1dv2HT5OrsgI4quVBuCa1tWJLXHpOcfkzRPFlY0r8e3cNZ775yexIa9V1jqKyDoR2SgiD4dqz2u1rpyu3mbsnsuFVi0LpFOzKuw5eIwV63eds73v01/z0tD2zPvfQJJOnOJM8tlz9ne+vCoLV23zvJsL+Kqv66XKWiQwEkv4NxFYIiITVTXLgyVeqnUZ9TZjN5BwUC0LpFmtsnRpXpWOTSuTJ3cUBaPzMObR7gx87lva3T0OgLYNK1Iltug5x/VoU5MvPO7mpuCn9829bPE1Bjaq6iZVPQVMALqFYtArtS6j3mbsBhIuqmWBPDF6FpV7vElczxH0ffprZq/YzMDnvqV4oWgAcueK5IFelzN64rLUYwrmy0OLupcxacG6bPDQaUc3PKKjl+/xxQDbAtYTgX8MlASjsuaVWpdRbzN2A/Hqfuh7cy/mzZnNvn37qFQ+lsefGEb/gYNCsnlfz2Z0alaVCBFGT1zKnBVbUvd1bVmNmUs3cfxkcHKVWcFvMzc8U1kTkR5AB1W9xV7vAzRW1bvSO8aorBkM/8STubrL3uFs0nbXQlX9+IY6a/4iR2WL5Iu64CprXrb4EoGyAeuxwA4P6zMYDBcQP7X4vBzjWwJUEZEKIpIb6AlM9LA+g8FwATFjfICqnhGRO4FpQCQwRlWz5/GSwWDIViSMNHOd4LXK2vfA917WYTAYwgQT+AwGQ04jXLqxTjCBz2AwuIJ5uGEwGHIcbs5Yc3u6a1pM4DMYDO7gUuQLmO7aCagB9BKRGm66agKfwWAIGcHKwOxkcYDr013TElZjfMuXL9uXN5dsdVi8GLDPZRe8sGnsGrvhaPcyNytevnzZtLy5pJjD4peISOAUrVGqOipg3dF011AIq8CnqsWdlhWRpW5Pe/HCprFr7PrdrhNUtaOL5s7XLHR1bq3p6hoMhnDD8+muJvAZDIZww/PprmHV1Q2SUZkXCQubxq6x63e72Up2THf1LC2VwWAwhCumq2swGHIcJvAZDIYch68Cn4iMEZE9IrLKZbtlReQnEVkjIqtF5B6X7F4iIotF5Bfb7jA37AbYjxSRFSIy2UWbW0TkNxFZmeZdq1DtFhKRL0VkrX2em7lgs5rtZ8pyRETudcnf++xrtkpExovIJS7Zvce2udotX227nk7xuuhQVd8sQCsgHljlst3SQLz9uQCwHqjhgl0B8tufcwGLgKYu+n0/8Ckw2UWbW4BiHly7ccAt9ufcQCGX7UcCu4DLXLAVA2wG8trrnwP9XbBbC1gFRGM9WPwRqOLSd/8DqGif21/cuH8v5sVXLT5VnQsc8MDuTlVdbn9OAtZg3fyh2lVVPWqv5rIXV54miUgs0Bl4zw17XiIiBbH+aL0PoKqnVPWQy9W0Bf5QVaczfzIjCsgrIlFYgcqN98iqAwmqelxVzwBzgGtcsOv5FK+LDV8FvuxARMoD9bFaZ27YixSRlcAeYIaqumIXeAN4CDibWcEgUWC6iCyzFfDcoCKwFxhrd83fE5F8LtlOoScw3g1DqrodeAX4E9gJHFbV6S6YXgW0EpGiIhINXMW5L+pmlfNN8Qr5D/fFjAl8AYhIfuAr4F5VPeKGTVVNVtV6WG+fNxaRWqHaFJEuwB5VXZZp4eBprqrxWJkxhopIKxdsRmENUfxPVesDxwDXxqHsl1y7Al+4ZK8wVoupAlAGyCciN4dqV1XXAC8CM4AfsLqkZ0K1SzZM8brYMIHPRkRyYQW9T1T1a7ft26uGOBIAAAR6SURBVF272YAbcxqbA11FZAtWt6aNiHzsgl1UdYf9/x7gG6xuVKgkAokBrd0vsQKhW3QClqvqbpfstQM2q+peVT0NfA1c7oZhVX1fVeNVtRXWsM0GF8waRcMgMYEPEBHBGn9ao6qvuWi3uIgUsj/nxfpBrQ3Vrqo+oqqxqloeq4s3S1VDbpGISD4RKZDyGWiP1T0LCVXdBWwTkWr2prbA76HaDaAXLnVzbf4EmopItH1vtMUa9w0ZESlh/18OuBZ3/DaKhkHiqylrIjIeuAIoJiKJwJOq+r4LppsDfYDf7PE4gP+oJZYUCqWBcXZixQjgc1V17dUTDygJfGP91okCPlXVH1yyfRfwif3D3AQMcMOoPVZ2JTDEDXsAqrpIRL4ElmN1RVfg3nSwr0SkKHAaGKqqB0M1qEbRMGjMlDWDwZDjMF1dg8GQ4zCBz2Aw5DhM4DMYDDkOE/gMBkOOwwQ+g8GQ4zCBz2eISLKdiWSViHxhv86RVVtXpGR2EZGuGWX1sLOr3JGFOp4SkX873Z6mzAcicn0QdZV3O3OP4eLEBD7/cUJV66lqLeAUcFvgTrEI+rqq6kRVfSGDIoWAoAOfwRCOmMDnb+YBle2WzhoReRvrpduyItJeRBaKyHK7ZZgfUvO2rRWR+VgzB7C39xeRt+zPJUXkGzuP4C8icjnwAlDJbm2+bJd7UESWiMivEpBrUEQetXPD/QhUIxNEZLBt5xcR+SpNK7adiMwTkfX2HOWUxA8vB9Tt2svLhpyBCXw+xU6X1An4zd5UDfgwIAnAY0A7O+HAUuB+O5nmaOBqoCVQKh3zw4E5qloXa07taqykAn/Yrc0HRaQ9UAVrLm89oIGItBKRBlhTpupjBdZGDr7O16rayK5vDTAoYF95oDVWCq537O8wCCtjSiPb/mARqeCgHoMB8NmUNQNg5YhLmVY3D2uOcRlgq6om2NubAjWABfb0s9zAQiAOa/L9BgA7scH5Uk+1AfqClV0GOGxnLAmkvb2ssNfzYwXCAsA3qnrcrsPJnNFaIvIsVnc6P9bUqxQ+V9WzwAYR2WR/h/ZAnYDxv0vtutc7qMtgMIHPh5yw01ylYge3Y4GbsHL/9UpTrh7upSsS4HlVfTdNHfdmoY4PgO6q+ouI9Meaj51CWltq132XqgYGyJRcigZDppiu7sVJAtBcRCqDNZFfRKpiZYapICKV7HK90jl+JnC7fWykWBmUk7BacylMAwYGjB3G2JlH5gLXiEheO9PL1Q78LQDstFOD9U6zr4eIRNg+VwTW2XXfbpdHRKqK+4lNDRcxpsV3EaKqe+2W03gRyWNvfkxV14uVVXmKiOwD5mPpQKTlHmCUiAwCkoHbVXWhiCywXxeZao/zVQcW2i3Oo8DNqrpcRD4DVgJbsbrjmfE4VsbrrVhjloEBdh1WivaSwG2qelJE3sMa+1suVuV7ge7Ozo7BYLKzGAyGHIjp6hoMhhyHCXwGgyHHYQKfwWDIcZjAZzAYchwm8BkMhhyHCXwGgyHHYQKfwWDIcfw/hLU2kIoIjnkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b86f668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a6eac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z1 = W1.T * X_test.T + b1\n",
    "H1 = relu(Z1)\n",
    "Z2 = W2.T * H1 + b2\n",
    "H2 = relu(Z2) \n",
    "Y_hat = softmax(W3.T * H2 + b3) \n",
    "\n",
    "cnf_matrix = confusion_matrix(np.array(prediction(Y_test.T))[0].tolist(), np.array(prediction(Y_hat))[0].tolist())\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[1,2,3,4,5,6,7,8,9,0],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[1,2,3,4,5,6,7,8,9,0], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [W1,b1,W2,b2,W3,b3]\n",
    "filehandler = open(\"nn_parameters.txt\",\"wb\")\n",
    "pickle.dump(theta, filehandler, protocol = 2)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
